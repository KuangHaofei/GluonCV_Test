Namespace(aux=False, aux_weight=0.5, backbone='resnet50', base_size=768, batch_size=16, checkname='psp_resnet50_mhp', clip_grad=0, crop_size=768, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='mhp', dtype='float32', epochs=30, eval=False, kvstore='device', log_interval=1, logging_file='train.log', lr=0.0005, mode=None, model='psp', model_zoo=None, momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, resume=None, save_dir='runs/mhp/psp/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
Starting Epoch: 0
Total Epochs: 30
Epoch 0 iteration 0001/0187: training loss 1.007; learning rate 0.000500
Epoch 0 iteration 0002/0187: training loss 1.025; learning rate 0.000500
Epoch 0 iteration 0003/0187: training loss 0.998; learning rate 0.000500
Epoch 0 iteration 0004/0187: training loss 1.003; learning rate 0.000500
Epoch 0 iteration 0005/0187: training loss 0.997; learning rate 0.000500
Epoch 0 iteration 0006/0187: training loss 0.977; learning rate 0.000499
Epoch 0 iteration 0007/0187: training loss 0.953; learning rate 0.000499
Epoch 0 iteration 0008/0187: training loss 0.924; learning rate 0.000499
Epoch 0 iteration 0009/0187: training loss 0.928; learning rate 0.000499
Epoch 0 iteration 0010/0187: training loss 0.907; learning rate 0.000499
Epoch 0 iteration 0011/0187: training loss 0.896; learning rate 0.000499
Epoch 0 iteration 0012/0187: training loss 0.905; learning rate 0.000499
Epoch 0 iteration 0013/0187: training loss 0.905; learning rate 0.000499
Epoch 0 iteration 0014/0187: training loss 0.902; learning rate 0.000499
Epoch 0 iteration 0015/0187: training loss 0.890; learning rate 0.000499
Epoch 0 iteration 0016/0187: training loss 0.886; learning rate 0.000499
Epoch 0 iteration 0017/0187: training loss 0.866; learning rate 0.000499
Epoch 0 iteration 0018/0187: training loss 0.860; learning rate 0.000498
Epoch 0 iteration 0019/0187: training loss 0.863; learning rate 0.000498
Epoch 0 iteration 0020/0187: training loss 0.845; learning rate 0.000498
Epoch 0 iteration 0021/0187: training loss 0.844; learning rate 0.000498
Epoch 0 iteration 0022/0187: training loss 0.839; learning rate 0.000498
Epoch 0 iteration 0023/0187: training loss 0.836; learning rate 0.000498
Epoch 0 iteration 0024/0187: training loss 0.829; learning rate 0.000498
Epoch 0 iteration 0025/0187: training loss 0.822; learning rate 0.000498
Epoch 0 iteration 0026/0187: training loss 0.825; learning rate 0.000498
Epoch 0 iteration 0027/0187: training loss 0.824; learning rate 0.000498
Epoch 0 iteration 0028/0187: training loss 0.820; learning rate 0.000498
Epoch 0 iteration 0029/0187: training loss 0.810; learning rate 0.000498
Epoch 0 iteration 0030/0187: training loss 0.803; learning rate 0.000498
Epoch 0 iteration 0031/0187: training loss 0.803; learning rate 0.000497
Epoch 0 iteration 0032/0187: training loss 0.800; learning rate 0.000497
Epoch 0 iteration 0033/0187: training loss 0.804; learning rate 0.000497
Epoch 0 iteration 0034/0187: training loss 0.799; learning rate 0.000497
Epoch 0 iteration 0035/0187: training loss 0.798; learning rate 0.000497
Epoch 0 iteration 0036/0187: training loss 0.795; learning rate 0.000497
Epoch 0 iteration 0037/0187: training loss 0.791; learning rate 0.000497
Epoch 0 iteration 0038/0187: training loss 0.791; learning rate 0.000497
Epoch 0 iteration 0039/0187: training loss 0.784; learning rate 0.000497
Epoch 0 iteration 0040/0187: training loss 0.779; learning rate 0.000497
Epoch 0 iteration 0041/0187: training loss 0.780; learning rate 0.000497
Epoch 0 iteration 0042/0187: training loss 0.770; learning rate 0.000497
Epoch 0 iteration 0043/0187: training loss 0.766; learning rate 0.000496
Epoch 0 iteration 0044/0187: training loss 0.759; learning rate 0.000496
Epoch 0 iteration 0045/0187: training loss 0.753; learning rate 0.000496
Epoch 0 iteration 0046/0187: training loss 0.753; learning rate 0.000496
Epoch 0 iteration 0047/0187: training loss 0.753; learning rate 0.000496
Epoch 0 iteration 0048/0187: training loss 0.747; learning rate 0.000496
Epoch 0 iteration 0049/0187: training loss 0.744; learning rate 0.000496
Epoch 0 iteration 0050/0187: training loss 0.743; learning rate 0.000496
Epoch 0 iteration 0051/0187: training loss 0.741; learning rate 0.000496
Epoch 0 iteration 0052/0187: training loss 0.736; learning rate 0.000496
Epoch 0 iteration 0053/0187: training loss 0.734; learning rate 0.000496
Epoch 0 iteration 0054/0187: training loss 0.731; learning rate 0.000496
Epoch 0 iteration 0055/0187: training loss 0.726; learning rate 0.000496
Epoch 0 iteration 0056/0187: training loss 0.722; learning rate 0.000495
Epoch 0 iteration 0057/0187: training loss 0.719; learning rate 0.000495
Epoch 0 iteration 0058/0187: training loss 0.713; learning rate 0.000495
Epoch 0 iteration 0059/0187: training loss 0.713; learning rate 0.000495
Epoch 0 iteration 0060/0187: training loss 0.711; learning rate 0.000495
Epoch 0 iteration 0061/0187: training loss 0.709; learning rate 0.000495
Epoch 0 iteration 0062/0187: training loss 0.705; learning rate 0.000495
Epoch 0 iteration 0063/0187: training loss 0.703; learning rate 0.000495
Epoch 0 iteration 0064/0187: training loss 0.703; learning rate 0.000495
Epoch 0 iteration 0065/0187: training loss 0.699; learning rate 0.000495
Epoch 0 iteration 0066/0187: training loss 0.695; learning rate 0.000495
Epoch 0 iteration 0067/0187: training loss 0.692; learning rate 0.000495
Epoch 0 iteration 0068/0187: training loss 0.689; learning rate 0.000494
Epoch 0 iteration 0069/0187: training loss 0.686; learning rate 0.000494
Epoch 0 iteration 0070/0187: training loss 0.682; learning rate 0.000494
Epoch 0 iteration 0071/0187: training loss 0.679; learning rate 0.000494
Epoch 0 iteration 0072/0187: training loss 0.678; learning rate 0.000494
Epoch 0 iteration 0073/0187: training loss 0.677; learning rate 0.000494
Epoch 0 iteration 0074/0187: training loss 0.675; learning rate 0.000494
Epoch 0 iteration 0075/0187: training loss 0.674; learning rate 0.000494
Epoch 0 iteration 0076/0187: training loss 0.673; learning rate 0.000494
Epoch 0 iteration 0077/0187: training loss 0.670; learning rate 0.000494
Epoch 0 iteration 0078/0187: training loss 0.668; learning rate 0.000494
Epoch 0 iteration 0079/0187: training loss 0.665; learning rate 0.000494
Epoch 0 iteration 0080/0187: training loss 0.664; learning rate 0.000493
Epoch 0 iteration 0081/0187: training loss 0.662; learning rate 0.000493
Epoch 0 iteration 0082/0187: training loss 0.659; learning rate 0.000493
Epoch 0 iteration 0083/0187: training loss 0.657; learning rate 0.000493
Epoch 0 iteration 0084/0187: training loss 0.655; learning rate 0.000493
Epoch 0 iteration 0085/0187: training loss 0.653; learning rate 0.000493
Epoch 0 iteration 0086/0187: training loss 0.652; learning rate 0.000493
Epoch 0 iteration 0087/0187: training loss 0.650; learning rate 0.000493
Epoch 0 iteration 0088/0187: training loss 0.648; learning rate 0.000493
Epoch 0 iteration 0089/0187: training loss 0.645; learning rate 0.000493
Epoch 0 iteration 0090/0187: training loss 0.644; learning rate 0.000493
Epoch 0 iteration 0091/0188: training loss 0.642; learning rate 0.000493
Epoch 0 iteration 0092/0188: training loss 0.640; learning rate 0.000493
Epoch 0 iteration 0093/0188: training loss 0.638; learning rate 0.000492
Epoch 0 iteration 0094/0188: training loss 0.638; learning rate 0.000492
Epoch 0 iteration 0095/0188: training loss 0.636; learning rate 0.000492
Epoch 0 iteration 0096/0188: training loss 0.635; learning rate 0.000492
Epoch 0 iteration 0097/0188: training loss 0.633; learning rate 0.000492
Epoch 0 iteration 0098/0188: training loss 0.631; learning rate 0.000492
Epoch 0 iteration 0099/0188: training loss 0.630; learning rate 0.000492
Epoch 0 iteration 0100/0188: training loss 0.629; learning rate 0.000492
Epoch 0 iteration 0101/0188: training loss 0.628; learning rate 0.000492
Epoch 0 iteration 0102/0188: training loss 0.625; learning rate 0.000492
Epoch 0 iteration 0103/0188: training loss 0.624; learning rate 0.000492
Epoch 0 iteration 0104/0188: training loss 0.621; learning rate 0.000492
Epoch 0 iteration 0105/0188: training loss 0.619; learning rate 0.000491
Epoch 0 iteration 0106/0188: training loss 0.618; learning rate 0.000491
Epoch 0 iteration 0107/0188: training loss 0.616; learning rate 0.000491
Epoch 0 iteration 0108/0188: training loss 0.614; learning rate 0.000491
Epoch 0 iteration 0109/0188: training loss 0.614; learning rate 0.000491
Epoch 0 iteration 0110/0188: training loss 0.613; learning rate 0.000491
Epoch 0 iteration 0111/0188: training loss 0.611; learning rate 0.000491
Epoch 0 iteration 0112/0188: training loss 0.610; learning rate 0.000491
Epoch 0 iteration 0113/0188: training loss 0.609; learning rate 0.000491
Epoch 0 iteration 0114/0188: training loss 0.607; learning rate 0.000491
Epoch 0 iteration 0115/0188: training loss 0.605; learning rate 0.000491
Epoch 0 iteration 0116/0188: training loss 0.604; learning rate 0.000491
Epoch 0 iteration 0117/0188: training loss 0.604; learning rate 0.000491
Epoch 0 iteration 0118/0188: training loss 0.602; learning rate 0.000490
Epoch 0 iteration 0119/0188: training loss 0.601; learning rate 0.000490
Epoch 0 iteration 0120/0188: training loss 0.601; learning rate 0.000490
Epoch 0 iteration 0121/0188: training loss 0.601; learning rate 0.000490
Epoch 0 iteration 0122/0188: training loss 0.600; learning rate 0.000490
Epoch 0 iteration 0123/0188: training loss 0.599; learning rate 0.000490
Epoch 0 iteration 0124/0188: training loss 0.598; learning rate 0.000490
Epoch 0 iteration 0125/0188: training loss 0.596; learning rate 0.000490
Epoch 0 iteration 0126/0188: training loss 0.595; learning rate 0.000490
Epoch 0 iteration 0127/0188: training loss 0.593; learning rate 0.000490
Epoch 0 iteration 0128/0188: training loss 0.592; learning rate 0.000490
Epoch 0 iteration 0129/0188: training loss 0.591; learning rate 0.000490
Epoch 0 iteration 0130/0188: training loss 0.588; learning rate 0.000489
Epoch 0 iteration 0131/0188: training loss 0.588; learning rate 0.000489
Epoch 0 iteration 0132/0188: training loss 0.587; learning rate 0.000489
Epoch 0 iteration 0133/0188: training loss 0.586; learning rate 0.000489
Epoch 0 iteration 0134/0188: training loss 0.585; learning rate 0.000489
Epoch 0 iteration 0135/0188: training loss 0.584; learning rate 0.000489
Epoch 0 iteration 0136/0188: training loss 0.583; learning rate 0.000489
Epoch 0 iteration 0137/0188: training loss 0.581; learning rate 0.000489
Epoch 0 iteration 0138/0188: training loss 0.580; learning rate 0.000489
Epoch 0 iteration 0139/0188: training loss 0.581; learning rate 0.000489
Epoch 0 iteration 0140/0188: training loss 0.579; learning rate 0.000489
Epoch 0 iteration 0141/0188: training loss 0.578; learning rate 0.000489
Epoch 0 iteration 0142/0188: training loss 0.577; learning rate 0.000489
Epoch 0 iteration 0143/0188: training loss 0.576; learning rate 0.000488
Epoch 0 iteration 0144/0188: training loss 0.575; learning rate 0.000488
Epoch 0 iteration 0145/0188: training loss 0.574; learning rate 0.000488
Epoch 0 iteration 0146/0188: training loss 0.573; learning rate 0.000488
Epoch 0 iteration 0147/0188: training loss 0.572; learning rate 0.000488
Epoch 0 iteration 0148/0188: training loss 0.571; learning rate 0.000488
Epoch 0 iteration 0149/0188: training loss 0.569; learning rate 0.000488
Epoch 0 iteration 0150/0188: training loss 0.568; learning rate 0.000488
Epoch 0 iteration 0151/0188: training loss 0.567; learning rate 0.000488
Epoch 0 iteration 0152/0188: training loss 0.566; learning rate 0.000488
Epoch 0 iteration 0153/0188: training loss 0.566; learning rate 0.000488
Epoch 0 iteration 0154/0188: training loss 0.566; learning rate 0.000488
Epoch 0 iteration 0155/0188: training loss 0.565; learning rate 0.000487
Epoch 0 iteration 0156/0188: training loss 0.564; learning rate 0.000487
Epoch 0 iteration 0157/0188: training loss 0.564; learning rate 0.000487
Epoch 0 iteration 0158/0188: training loss 0.562; learning rate 0.000487
Epoch 0 iteration 0159/0188: training loss 0.561; learning rate 0.000487
Epoch 0 iteration 0160/0188: training loss 0.560; learning rate 0.000487
Epoch 0 iteration 0161/0188: training loss 0.560; learning rate 0.000487
Epoch 0 iteration 0162/0188: training loss 0.558; learning rate 0.000487
Epoch 0 iteration 0163/0188: training loss 0.557; learning rate 0.000487
Epoch 0 iteration 0164/0188: training loss 0.556; learning rate 0.000487
Epoch 0 iteration 0165/0188: training loss 0.555; learning rate 0.000487
Epoch 0 iteration 0166/0188: training loss 0.554; learning rate 0.000487
Epoch 0 iteration 0167/0188: training loss 0.554; learning rate 0.000487
Epoch 0 iteration 0168/0188: training loss 0.554; learning rate 0.000486
Epoch 0 iteration 0169/0188: training loss 0.554; learning rate 0.000486
Epoch 0 iteration 0170/0188: training loss 0.553; learning rate 0.000486
Epoch 0 iteration 0171/0188: training loss 0.552; learning rate 0.000486
Epoch 0 iteration 0172/0188: training loss 0.552; learning rate 0.000486
Epoch 0 iteration 0173/0188: training loss 0.551; learning rate 0.000486
Epoch 0 iteration 0174/0188: training loss 0.551; learning rate 0.000486
Epoch 0 iteration 0175/0188: training loss 0.551; learning rate 0.000486
Epoch 0 iteration 0176/0188: training loss 0.549; learning rate 0.000486
Epoch 0 iteration 0177/0188: training loss 0.548; learning rate 0.000486
Epoch 0 iteration 0178/0188: training loss 0.547; learning rate 0.000486
Epoch 0 iteration 0179/0188: training loss 0.546; learning rate 0.000486
Epoch 0 iteration 0180/0188: training loss 0.545; learning rate 0.000485
Epoch 0 iteration 0181/0188: training loss 0.544; learning rate 0.000485
Epoch 0 iteration 0182/0188: training loss 0.544; learning rate 0.000485
Epoch 0 iteration 0183/0188: training loss 0.543; learning rate 0.000485
Epoch 0 iteration 0184/0188: training loss 0.543; learning rate 0.000485
Epoch 0 iteration 0185/0188: training loss 0.543; learning rate 0.000485
Epoch 0 iteration 0186/0188: training loss 0.543; learning rate 0.000485
Epoch 0 validation pixAcc: 0.280, mIoU: 0.113
Epoch 1 iteration 0001/0187: training loss 0.420; learning rate 0.000485
Epoch 1 iteration 0002/0187: training loss 0.378; learning rate 0.000485
Epoch 1 iteration 0003/0187: training loss 0.356; learning rate 0.000485
Epoch 1 iteration 0004/0187: training loss 0.373; learning rate 0.000485
Epoch 1 iteration 0005/0187: training loss 0.368; learning rate 0.000484
Epoch 1 iteration 0006/0187: training loss 0.388; learning rate 0.000484
Epoch 1 iteration 0007/0187: training loss 0.392; learning rate 0.000484
Epoch 1 iteration 0008/0187: training loss 0.392; learning rate 0.000484
Epoch 1 iteration 0009/0187: training loss 0.393; learning rate 0.000484
Epoch 1 iteration 0010/0187: training loss 0.396; learning rate 0.000484
Epoch 1 iteration 0011/0187: training loss 0.391; learning rate 0.000484
Epoch 1 iteration 0012/0187: training loss 0.389; learning rate 0.000484
Epoch 1 iteration 0013/0187: training loss 0.387; learning rate 0.000484
Epoch 1 iteration 0014/0187: training loss 0.394; learning rate 0.000484
Epoch 1 iteration 0015/0187: training loss 0.406; learning rate 0.000484
Epoch 1 iteration 0016/0187: training loss 0.401; learning rate 0.000484
Epoch 1 iteration 0017/0187: training loss 0.392; learning rate 0.000484
Epoch 1 iteration 0018/0187: training loss 0.390; learning rate 0.000483
Epoch 1 iteration 0019/0187: training loss 0.392; learning rate 0.000483
Epoch 1 iteration 0020/0187: training loss 0.395; learning rate 0.000483
Epoch 1 iteration 0021/0187: training loss 0.393; learning rate 0.000483
Epoch 1 iteration 0022/0187: training loss 0.390; learning rate 0.000483
Epoch 1 iteration 0023/0187: training loss 0.389; learning rate 0.000483
Epoch 1 iteration 0024/0187: training loss 0.385; learning rate 0.000483
Epoch 1 iteration 0025/0187: training loss 0.385; learning rate 0.000483
Epoch 1 iteration 0026/0187: training loss 0.396; learning rate 0.000483
Epoch 1 iteration 0027/0187: training loss 0.393; learning rate 0.000483
Epoch 1 iteration 0028/0187: training loss 0.393; learning rate 0.000483
Epoch 1 iteration 0029/0187: training loss 0.392; learning rate 0.000483
Epoch 1 iteration 0030/0187: training loss 0.396; learning rate 0.000482
Epoch 1 iteration 0031/0187: training loss 0.396; learning rate 0.000482
Epoch 1 iteration 0032/0187: training loss 0.397; learning rate 0.000482
Epoch 1 iteration 0033/0187: training loss 0.398; learning rate 0.000482
Epoch 1 iteration 0034/0187: training loss 0.396; learning rate 0.000482
Epoch 1 iteration 0035/0187: training loss 0.394; learning rate 0.000482
Epoch 1 iteration 0036/0187: training loss 0.393; learning rate 0.000482
Epoch 1 iteration 0037/0187: training loss 0.393; learning rate 0.000482
Epoch 1 iteration 0038/0187: training loss 0.394; learning rate 0.000482
Epoch 1 iteration 0039/0187: training loss 0.394; learning rate 0.000482
Epoch 1 iteration 0040/0187: training loss 0.393; learning rate 0.000482
Epoch 1 iteration 0041/0187: training loss 0.395; learning rate 0.000482
Epoch 1 iteration 0042/0187: training loss 0.396; learning rate 0.000482
Epoch 1 iteration 0043/0187: training loss 0.397; learning rate 0.000481
Epoch 1 iteration 0044/0187: training loss 0.395; learning rate 0.000481
Epoch 1 iteration 0045/0187: training loss 0.394; learning rate 0.000481
Epoch 1 iteration 0046/0187: training loss 0.393; learning rate 0.000481
Epoch 1 iteration 0047/0187: training loss 0.395; learning rate 0.000481
Epoch 1 iteration 0048/0187: training loss 0.395; learning rate 0.000481
Epoch 1 iteration 0049/0187: training loss 0.393; learning rate 0.000481
Epoch 1 iteration 0050/0187: training loss 0.394; learning rate 0.000481
Epoch 1 iteration 0051/0187: training loss 0.393; learning rate 0.000481
Epoch 1 iteration 0052/0187: training loss 0.393; learning rate 0.000481
Epoch 1 iteration 0053/0187: training loss 0.394; learning rate 0.000481
Epoch 1 iteration 0054/0187: training loss 0.394; learning rate 0.000481
Epoch 1 iteration 0055/0187: training loss 0.395; learning rate 0.000480
Epoch 1 iteration 0056/0187: training loss 0.395; learning rate 0.000480
Epoch 1 iteration 0057/0187: training loss 0.396; learning rate 0.000480
Epoch 1 iteration 0058/0187: training loss 0.396; learning rate 0.000480
Epoch 1 iteration 0059/0187: training loss 0.396; learning rate 0.000480
Epoch 1 iteration 0060/0187: training loss 0.395; learning rate 0.000480
Epoch 1 iteration 0061/0187: training loss 0.395; learning rate 0.000480
Epoch 1 iteration 0062/0187: training loss 0.394; learning rate 0.000480
Epoch 1 iteration 0063/0187: training loss 0.393; learning rate 0.000480
Epoch 1 iteration 0064/0187: training loss 0.394; learning rate 0.000480
Epoch 1 iteration 0065/0187: training loss 0.397; learning rate 0.000480
Epoch 1 iteration 0066/0187: training loss 0.397; learning rate 0.000480
Epoch 1 iteration 0067/0187: training loss 0.398; learning rate 0.000479
Epoch 1 iteration 0068/0187: training loss 0.397; learning rate 0.000479
Epoch 1 iteration 0069/0187: training loss 0.398; learning rate 0.000479
Epoch 1 iteration 0070/0187: training loss 0.399; learning rate 0.000479
Epoch 1 iteration 0071/0187: training loss 0.399; learning rate 0.000479
Epoch 1 iteration 0072/0187: training loss 0.400; learning rate 0.000479
Epoch 1 iteration 0073/0187: training loss 0.399; learning rate 0.000479
Epoch 1 iteration 0074/0187: training loss 0.398; learning rate 0.000479
Epoch 1 iteration 0075/0187: training loss 0.397; learning rate 0.000479
Epoch 1 iteration 0076/0187: training loss 0.397; learning rate 0.000479
Epoch 1 iteration 0077/0187: training loss 0.396; learning rate 0.000479
Epoch 1 iteration 0078/0187: training loss 0.395; learning rate 0.000479
Epoch 1 iteration 0079/0187: training loss 0.394; learning rate 0.000479
Epoch 1 iteration 0080/0187: training loss 0.394; learning rate 0.000478
Epoch 1 iteration 0081/0187: training loss 0.394; learning rate 0.000478
Epoch 1 iteration 0082/0187: training loss 0.394; learning rate 0.000478
Epoch 1 iteration 0083/0187: training loss 0.394; learning rate 0.000478
Epoch 1 iteration 0084/0187: training loss 0.393; learning rate 0.000478
Epoch 1 iteration 0085/0187: training loss 0.392; learning rate 0.000478
Epoch 1 iteration 0086/0187: training loss 0.391; learning rate 0.000478
Epoch 1 iteration 0087/0187: training loss 0.392; learning rate 0.000478
Epoch 1 iteration 0088/0187: training loss 0.391; learning rate 0.000478
Epoch 1 iteration 0089/0187: training loss 0.391; learning rate 0.000478
Epoch 1 iteration 0090/0187: training loss 0.390; learning rate 0.000478
Epoch 1 iteration 0091/0187: training loss 0.390; learning rate 0.000478
Epoch 1 iteration 0092/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0093/0187: training loss 0.390; learning rate 0.000477
Epoch 1 iteration 0094/0187: training loss 0.390; learning rate 0.000477
Epoch 1 iteration 0095/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0096/0187: training loss 0.393; learning rate 0.000477
Epoch 1 iteration 0097/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0098/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0099/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0100/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0101/0187: training loss 0.392; learning rate 0.000477
Epoch 1 iteration 0102/0187: training loss 0.391; learning rate 0.000477
Epoch 1 iteration 0103/0187: training loss 0.390; learning rate 0.000477
Epoch 1 iteration 0104/0187: training loss 0.390; learning rate 0.000477
Epoch 1 iteration 0105/0187: training loss 0.390; learning rate 0.000476
Epoch 1 iteration 0106/0187: training loss 0.389; learning rate 0.000476
Epoch 1 iteration 0107/0187: training loss 0.389; learning rate 0.000476
Epoch 1 iteration 0108/0187: training loss 0.388; learning rate 0.000476
Epoch 1 iteration 0109/0187: training loss 0.387; learning rate 0.000476
Epoch 1 iteration 0110/0187: training loss 0.386; learning rate 0.000476
Epoch 1 iteration 0111/0187: training loss 0.386; learning rate 0.000476
Epoch 1 iteration 0112/0187: training loss 0.386; learning rate 0.000476
Epoch 1 iteration 0113/0187: training loss 0.385; learning rate 0.000476
Epoch 1 iteration 0114/0187: training loss 0.385; learning rate 0.000476
Epoch 1 iteration 0115/0187: training loss 0.385; learning rate 0.000476
Epoch 1 iteration 0116/0187: training loss 0.384; learning rate 0.000476
Epoch 1 iteration 0117/0187: training loss 0.383; learning rate 0.000475
Epoch 1 iteration 0118/0187: training loss 0.384; learning rate 0.000475
Epoch 1 iteration 0119/0187: training loss 0.384; learning rate 0.000475
Epoch 1 iteration 0120/0187: training loss 0.384; learning rate 0.000475
Epoch 1 iteration 0121/0187: training loss 0.383; learning rate 0.000475
Epoch 1 iteration 0122/0187: training loss 0.382; learning rate 0.000475
Epoch 1 iteration 0123/0187: training loss 0.383; learning rate 0.000475
Epoch 1 iteration 0124/0187: training loss 0.382; learning rate 0.000475
Epoch 1 iteration 0125/0187: training loss 0.382; learning rate 0.000475
Epoch 1 iteration 0126/0187: training loss 0.381; learning rate 0.000475
Epoch 1 iteration 0127/0187: training loss 0.381; learning rate 0.000475
Epoch 1 iteration 0128/0187: training loss 0.381; learning rate 0.000475
Epoch 1 iteration 0129/0187: training loss 0.380; learning rate 0.000474
Epoch 1 iteration 0130/0187: training loss 0.381; learning rate 0.000474
Epoch 1 iteration 0131/0187: training loss 0.380; learning rate 0.000474
Epoch 1 iteration 0132/0187: training loss 0.379; learning rate 0.000474
Epoch 1 iteration 0133/0187: training loss 0.379; learning rate 0.000474
Epoch 1 iteration 0134/0187: training loss 0.378; learning rate 0.000474
Epoch 1 iteration 0135/0187: training loss 0.378; learning rate 0.000474
Epoch 1 iteration 0136/0187: training loss 0.378; learning rate 0.000474
Epoch 1 iteration 0137/0187: training loss 0.377; learning rate 0.000474
Epoch 1 iteration 0138/0187: training loss 0.377; learning rate 0.000474
Epoch 1 iteration 0139/0187: training loss 0.376; learning rate 0.000474
Epoch 1 iteration 0140/0187: training loss 0.376; learning rate 0.000474
Epoch 1 iteration 0141/0187: training loss 0.375; learning rate 0.000474
Epoch 1 iteration 0142/0187: training loss 0.374; learning rate 0.000473
Epoch 1 iteration 0143/0187: training loss 0.374; learning rate 0.000473
Epoch 1 iteration 0144/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0145/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0146/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0147/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0148/0187: training loss 0.372; learning rate 0.000473
Epoch 1 iteration 0149/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0150/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0151/0187: training loss 0.374; learning rate 0.000473
Epoch 1 iteration 0152/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0153/0187: training loss 0.373; learning rate 0.000473
Epoch 1 iteration 0154/0187: training loss 0.372; learning rate 0.000472
Epoch 1 iteration 0155/0187: training loss 0.372; learning rate 0.000472
Epoch 1 iteration 0156/0187: training loss 0.372; learning rate 0.000472
Epoch 1 iteration 0157/0187: training loss 0.371; learning rate 0.000472
Epoch 1 iteration 0158/0187: training loss 0.372; learning rate 0.000472
Epoch 1 iteration 0159/0187: training loss 0.371; learning rate 0.000472
Epoch 1 iteration 0160/0187: training loss 0.371; learning rate 0.000472
Epoch 1 iteration 0161/0187: training loss 0.371; learning rate 0.000472
Epoch 1 iteration 0162/0187: training loss 0.370; learning rate 0.000472
Epoch 1 iteration 0163/0187: training loss 0.370; learning rate 0.000472
Epoch 1 iteration 0164/0187: training loss 0.370; learning rate 0.000472
Epoch 1 iteration 0165/0187: training loss 0.370; learning rate 0.000472
Epoch 1 iteration 0166/0187: training loss 0.369; learning rate 0.000472
Epoch 1 iteration 0167/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0168/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0169/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0170/0187: training loss 0.368; learning rate 0.000471
Epoch 1 iteration 0171/0187: training loss 0.368; learning rate 0.000471
Epoch 1 iteration 0172/0187: training loss 0.368; learning rate 0.000471
Epoch 1 iteration 0173/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0174/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0175/0187: training loss 0.369; learning rate 0.000471
Epoch 1 iteration 0176/0187: training loss 0.368; learning rate 0.000471
Epoch 1 iteration 0177/0187: training loss 0.368; learning rate 0.000471
Epoch 1 iteration 0178/0187: training loss 0.367; learning rate 0.000471
Epoch 1 iteration 0179/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0180/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0181/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0182/0187: training loss 0.365; learning rate 0.000470
Epoch 1 iteration 0183/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0184/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0185/0187: training loss 0.366; learning rate 0.000470
Epoch 1 iteration 0186/0187: training loss 0.365; learning rate 0.000470
Epoch 1 iteration 0187/0187: training loss 0.366; learning rate 0.000470
Epoch 1 validation pixAcc: 0.303, mIoU: 0.152
Epoch 2 iteration 0001/0187: training loss 0.382; learning rate 0.000470
Epoch 2 iteration 0002/0187: training loss 0.364; learning rate 0.000470
Epoch 2 iteration 0003/0187: training loss 0.353; learning rate 0.000469
Epoch 2 iteration 0004/0187: training loss 0.348; learning rate 0.000469
Epoch 2 iteration 0005/0187: training loss 0.348; learning rate 0.000469
Epoch 2 iteration 0006/0187: training loss 0.353; learning rate 0.000469
Epoch 2 iteration 0007/0187: training loss 0.350; learning rate 0.000469
Epoch 2 iteration 0008/0187: training loss 0.348; learning rate 0.000469
Epoch 2 iteration 0009/0187: training loss 0.366; learning rate 0.000469
Epoch 2 iteration 0010/0187: training loss 0.362; learning rate 0.000469
Epoch 2 iteration 0011/0187: training loss 0.355; learning rate 0.000469
Epoch 2 iteration 0012/0187: training loss 0.361; learning rate 0.000469
Epoch 2 iteration 0013/0187: training loss 0.355; learning rate 0.000469
Epoch 2 iteration 0014/0187: training loss 0.353; learning rate 0.000469
Epoch 2 iteration 0015/0187: training loss 0.348; learning rate 0.000469
Epoch 2 iteration 0016/0187: training loss 0.349; learning rate 0.000468
Epoch 2 iteration 0017/0187: training loss 0.347; learning rate 0.000468
Epoch 2 iteration 0018/0187: training loss 0.350; learning rate 0.000468
Epoch 2 iteration 0019/0187: training loss 0.346; learning rate 0.000468
Epoch 2 iteration 0020/0187: training loss 0.347; learning rate 0.000468
Epoch 2 iteration 0021/0187: training loss 0.349; learning rate 0.000468
Epoch 2 iteration 0022/0187: training loss 0.348; learning rate 0.000468
Epoch 2 iteration 0023/0187: training loss 0.344; learning rate 0.000468
Epoch 2 iteration 0024/0187: training loss 0.345; learning rate 0.000468
Epoch 2 iteration 0025/0187: training loss 0.344; learning rate 0.000468
Epoch 2 iteration 0026/0187: training loss 0.343; learning rate 0.000468
Epoch 2 iteration 0027/0187: training loss 0.344; learning rate 0.000468
Epoch 2 iteration 0028/0187: training loss 0.345; learning rate 0.000467
Epoch 2 iteration 0029/0187: training loss 0.345; learning rate 0.000467
Epoch 2 iteration 0030/0187: training loss 0.341; learning rate 0.000467
Epoch 2 iteration 0031/0187: training loss 0.343; learning rate 0.000467
Epoch 2 iteration 0032/0187: training loss 0.340; learning rate 0.000467
Epoch 2 iteration 0033/0187: training loss 0.341; learning rate 0.000467
Epoch 2 iteration 0034/0187: training loss 0.339; learning rate 0.000467
Epoch 2 iteration 0035/0187: training loss 0.339; learning rate 0.000467
Epoch 2 iteration 0036/0187: training loss 0.341; learning rate 0.000467
Epoch 2 iteration 0037/0187: training loss 0.339; learning rate 0.000467
Epoch 2 iteration 0038/0187: training loss 0.337; learning rate 0.000467
Epoch 2 iteration 0039/0187: training loss 0.339; learning rate 0.000467
Epoch 2 iteration 0040/0187: training loss 0.338; learning rate 0.000466
Epoch 2 iteration 0041/0187: training loss 0.336; learning rate 0.000466
Epoch 2 iteration 0042/0187: training loss 0.336; learning rate 0.000466
Epoch 2 iteration 0043/0187: training loss 0.334; learning rate 0.000466
Epoch 2 iteration 0044/0187: training loss 0.333; learning rate 0.000466
Epoch 2 iteration 0045/0187: training loss 0.334; learning rate 0.000466
Epoch 2 iteration 0046/0187: training loss 0.334; learning rate 0.000466
Epoch 2 iteration 0047/0187: training loss 0.334; learning rate 0.000466
Epoch 2 iteration 0048/0187: training loss 0.333; learning rate 0.000466
Epoch 2 iteration 0049/0187: training loss 0.333; learning rate 0.000466
Epoch 2 iteration 0050/0187: training loss 0.331; learning rate 0.000466
Epoch 2 iteration 0051/0187: training loss 0.333; learning rate 0.000466
Epoch 2 iteration 0052/0187: training loss 0.332; learning rate 0.000466
Epoch 2 iteration 0053/0187: training loss 0.331; learning rate 0.000465
Epoch 2 iteration 0054/0187: training loss 0.331; learning rate 0.000465
Epoch 2 iteration 0055/0187: training loss 0.332; learning rate 0.000465
Epoch 2 iteration 0056/0187: training loss 0.332; learning rate 0.000465
Epoch 2 iteration 0057/0187: training loss 0.332; learning rate 0.000465
Epoch 2 iteration 0058/0187: training loss 0.331; learning rate 0.000465
Epoch 2 iteration 0059/0187: training loss 0.330; learning rate 0.000465
Epoch 2 iteration 0060/0187: training loss 0.331; learning rate 0.000465
Epoch 2 iteration 0061/0187: training loss 0.331; learning rate 0.000465
Epoch 2 iteration 0062/0187: training loss 0.329; learning rate 0.000465
Epoch 2 iteration 0063/0187: training loss 0.328; learning rate 0.000465
Epoch 2 iteration 0064/0187: training loss 0.328; learning rate 0.000465
Epoch 2 iteration 0065/0187: training loss 0.328; learning rate 0.000464
Epoch 2 iteration 0066/0187: training loss 0.328; learning rate 0.000464
Epoch 2 iteration 0067/0187: training loss 0.329; learning rate 0.000464
Epoch 2 iteration 0068/0187: training loss 0.327; learning rate 0.000464
Epoch 2 iteration 0069/0187: training loss 0.327; learning rate 0.000464
Epoch 2 iteration 0070/0187: training loss 0.329; learning rate 0.000464
Epoch 2 iteration 0071/0187: training loss 0.330; learning rate 0.000464
Epoch 2 iteration 0072/0187: training loss 0.330; learning rate 0.000464
Epoch 2 iteration 0073/0187: training loss 0.329; learning rate 0.000464
Epoch 2 iteration 0074/0187: training loss 0.330; learning rate 0.000464
Epoch 2 iteration 0075/0187: training loss 0.330; learning rate 0.000464
Epoch 2 iteration 0076/0187: training loss 0.329; learning rate 0.000464
Epoch 2 iteration 0077/0187: training loss 0.329; learning rate 0.000464
Epoch 2 iteration 0078/0187: training loss 0.330; learning rate 0.000463
Epoch 2 iteration 0079/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0080/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0081/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0082/0187: training loss 0.328; learning rate 0.000463
Epoch 2 iteration 0083/0187: training loss 0.328; learning rate 0.000463
Epoch 2 iteration 0084/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0085/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0086/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0087/0187: training loss 0.329; learning rate 0.000463
Epoch 2 iteration 0088/0187: training loss 0.328; learning rate 0.000463
Epoch 2 iteration 0089/0187: training loss 0.328; learning rate 0.000463
Epoch 2 iteration 0090/0187: training loss 0.328; learning rate 0.000462
Epoch 2 iteration 0091/0188: training loss 0.329; learning rate 0.000462
Epoch 2 iteration 0092/0188: training loss 0.331; learning rate 0.000462
Epoch 2 iteration 0093/0188: training loss 0.331; learning rate 0.000462
Epoch 2 iteration 0094/0188: training loss 0.330; learning rate 0.000462
Epoch 2 iteration 0095/0188: training loss 0.330; learning rate 0.000462
Epoch 2 iteration 0096/0188: training loss 0.330; learning rate 0.000462
Epoch 2 iteration 0097/0188: training loss 0.330; learning rate 0.000462
Epoch 2 iteration 0098/0188: training loss 0.329; learning rate 0.000462
Epoch 2 iteration 0099/0188: training loss 0.328; learning rate 0.000462
Epoch 2 iteration 0100/0188: training loss 0.327; learning rate 0.000462
Epoch 2 iteration 0101/0188: training loss 0.327; learning rate 0.000462
Epoch 2 iteration 0102/0188: training loss 0.328; learning rate 0.000461
Epoch 2 iteration 0103/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0104/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0105/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0106/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0107/0188: training loss 0.325; learning rate 0.000461
Epoch 2 iteration 0108/0188: training loss 0.325; learning rate 0.000461
Epoch 2 iteration 0109/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0110/0188: training loss 0.326; learning rate 0.000461
Epoch 2 iteration 0111/0188: training loss 0.327; learning rate 0.000461
Epoch 2 iteration 0112/0188: training loss 0.327; learning rate 0.000461
Epoch 2 iteration 0113/0188: training loss 0.328; learning rate 0.000461
Epoch 2 iteration 0114/0188: training loss 0.328; learning rate 0.000461
Epoch 2 iteration 0115/0188: training loss 0.328; learning rate 0.000460
Epoch 2 iteration 0116/0188: training loss 0.328; learning rate 0.000460
Epoch 2 iteration 0117/0188: training loss 0.328; learning rate 0.000460
Epoch 2 iteration 0118/0188: training loss 0.327; learning rate 0.000460
Epoch 2 iteration 0119/0188: training loss 0.327; learning rate 0.000460
Epoch 2 iteration 0120/0188: training loss 0.326; learning rate 0.000460
Epoch 2 iteration 0121/0188: training loss 0.326; learning rate 0.000460
Epoch 2 iteration 0122/0188: training loss 0.327; learning rate 0.000460
Epoch 2 iteration 0123/0188: training loss 0.326; learning rate 0.000460
Epoch 2 iteration 0124/0188: training loss 0.326; learning rate 0.000460
Epoch 2 iteration 0125/0188: training loss 0.326; learning rate 0.000460
Epoch 2 iteration 0126/0188: training loss 0.327; learning rate 0.000460
Epoch 2 iteration 0127/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0128/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0129/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0130/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0131/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0132/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0133/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0134/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0135/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0136/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0137/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0138/0188: training loss 0.326; learning rate 0.000459
Epoch 2 iteration 0139/0188: training loss 0.326; learning rate 0.000458
Epoch 2 iteration 0140/0188: training loss 0.326; learning rate 0.000458
Epoch 2 iteration 0141/0188: training loss 0.326; learning rate 0.000458
Epoch 2 iteration 0142/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0143/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0144/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0145/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0146/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0147/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0148/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0149/0188: training loss 0.325; learning rate 0.000458
Epoch 2 iteration 0150/0188: training loss 0.324; learning rate 0.000458
Epoch 2 iteration 0151/0188: training loss 0.324; learning rate 0.000458
Epoch 2 iteration 0152/0188: training loss 0.324; learning rate 0.000457
Epoch 2 iteration 0153/0188: training loss 0.324; learning rate 0.000457
Epoch 2 iteration 0154/0188: training loss 0.324; learning rate 0.000457
Epoch 2 iteration 0155/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0156/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0157/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0158/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0159/0188: training loss 0.322; learning rate 0.000457
Epoch 2 iteration 0160/0188: training loss 0.322; learning rate 0.000457
Epoch 2 iteration 0161/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0162/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0163/0188: training loss 0.323; learning rate 0.000457
Epoch 2 iteration 0164/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0165/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0166/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0167/0188: training loss 0.324; learning rate 0.000456
Epoch 2 iteration 0168/0188: training loss 0.324; learning rate 0.000456
Epoch 2 iteration 0169/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0170/0188: training loss 0.324; learning rate 0.000456
Epoch 2 iteration 0171/0188: training loss 0.324; learning rate 0.000456
Epoch 2 iteration 0172/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0173/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0174/0188: training loss 0.323; learning rate 0.000456
Epoch 2 iteration 0175/0188: training loss 0.322; learning rate 0.000456
Epoch 2 iteration 0176/0188: training loss 0.322; learning rate 0.000455
Epoch 2 iteration 0177/0188: training loss 0.322; learning rate 0.000455
Epoch 2 iteration 0178/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0179/0188: training loss 0.322; learning rate 0.000455
Epoch 2 iteration 0180/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0181/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0182/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0183/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0184/0188: training loss 0.321; learning rate 0.000455
Epoch 2 iteration 0185/0188: training loss 0.320; learning rate 0.000455
Epoch 2 iteration 0186/0188: training loss 0.320; learning rate 0.000455
Epoch 2 validation pixAcc: 0.313, mIoU: 0.176
Epoch 3 iteration 0001/0187: training loss 0.385; learning rate 0.000455
Epoch 3 iteration 0002/0187: training loss 0.403; learning rate 0.000454
Epoch 3 iteration 0003/0187: training loss 0.357; learning rate 0.000454
Epoch 3 iteration 0004/0187: training loss 0.364; learning rate 0.000454
Epoch 3 iteration 0005/0187: training loss 0.382; learning rate 0.000454
Epoch 3 iteration 0006/0187: training loss 0.373; learning rate 0.000454
Epoch 3 iteration 0007/0187: training loss 0.359; learning rate 0.000454
Epoch 3 iteration 0008/0187: training loss 0.339; learning rate 0.000454
Epoch 3 iteration 0009/0187: training loss 0.325; learning rate 0.000454
Epoch 3 iteration 0010/0187: training loss 0.331; learning rate 0.000454
Epoch 3 iteration 0011/0187: training loss 0.332; learning rate 0.000454
Epoch 3 iteration 0012/0187: training loss 0.324; learning rate 0.000454
Epoch 3 iteration 0013/0187: training loss 0.318; learning rate 0.000454
Epoch 3 iteration 0014/0187: training loss 0.310; learning rate 0.000453
Epoch 3 iteration 0015/0187: training loss 0.322; learning rate 0.000453
Epoch 3 iteration 0016/0187: training loss 0.317; learning rate 0.000453
Epoch 3 iteration 0017/0187: training loss 0.317; learning rate 0.000453
Epoch 3 iteration 0018/0187: training loss 0.315; learning rate 0.000453
Epoch 3 iteration 0019/0187: training loss 0.310; learning rate 0.000453
Epoch 3 iteration 0020/0187: training loss 0.307; learning rate 0.000453
Epoch 3 iteration 0021/0187: training loss 0.305; learning rate 0.000453
Epoch 3 iteration 0022/0187: training loss 0.302; learning rate 0.000453
Epoch 3 iteration 0023/0187: training loss 0.304; learning rate 0.000453
Epoch 3 iteration 0024/0187: training loss 0.305; learning rate 0.000453
Epoch 3 iteration 0025/0187: training loss 0.303; learning rate 0.000453
Epoch 3 iteration 0026/0187: training loss 0.302; learning rate 0.000452
Epoch 3 iteration 0027/0187: training loss 0.302; learning rate 0.000452
Epoch 3 iteration 0028/0187: training loss 0.306; learning rate 0.000452
Epoch 3 iteration 0029/0187: training loss 0.304; learning rate 0.000452
Epoch 3 iteration 0030/0187: training loss 0.302; learning rate 0.000452
Epoch 3 iteration 0031/0187: training loss 0.302; learning rate 0.000452
Epoch 3 iteration 0032/0187: training loss 0.305; learning rate 0.000452
Epoch 3 iteration 0033/0187: training loss 0.305; learning rate 0.000452
Epoch 3 iteration 0034/0187: training loss 0.307; learning rate 0.000452
Epoch 3 iteration 0035/0187: training loss 0.307; learning rate 0.000452
Epoch 3 iteration 0036/0187: training loss 0.305; learning rate 0.000452
Epoch 3 iteration 0037/0187: training loss 0.306; learning rate 0.000452
Epoch 3 iteration 0038/0187: training loss 0.305; learning rate 0.000452
Epoch 3 iteration 0039/0187: training loss 0.305; learning rate 0.000451
Epoch 3 iteration 0040/0187: training loss 0.303; learning rate 0.000451
Epoch 3 iteration 0041/0187: training loss 0.303; learning rate 0.000451
Epoch 3 iteration 0042/0187: training loss 0.301; learning rate 0.000451
Epoch 3 iteration 0043/0187: training loss 0.301; learning rate 0.000451
Epoch 3 iteration 0044/0187: training loss 0.300; learning rate 0.000451
Epoch 3 iteration 0045/0187: training loss 0.299; learning rate 0.000451
Epoch 3 iteration 0046/0187: training loss 0.301; learning rate 0.000451
Epoch 3 iteration 0047/0187: training loss 0.302; learning rate 0.000451
Epoch 3 iteration 0048/0187: training loss 0.303; learning rate 0.000451
Epoch 3 iteration 0049/0187: training loss 0.303; learning rate 0.000451
Epoch 3 iteration 0050/0187: training loss 0.303; learning rate 0.000451
Epoch 3 iteration 0051/0187: training loss 0.302; learning rate 0.000450
Epoch 3 iteration 0052/0187: training loss 0.301; learning rate 0.000450
Epoch 3 iteration 0053/0187: training loss 0.299; learning rate 0.000450
Epoch 3 iteration 0054/0187: training loss 0.300; learning rate 0.000450
Epoch 3 iteration 0055/0187: training loss 0.301; learning rate 0.000450
Epoch 3 iteration 0056/0187: training loss 0.300; learning rate 0.000450
Epoch 3 iteration 0057/0187: training loss 0.300; learning rate 0.000450
Epoch 3 iteration 0058/0187: training loss 0.300; learning rate 0.000450
Epoch 3 iteration 0059/0187: training loss 0.298; learning rate 0.000450
Epoch 3 iteration 0060/0187: training loss 0.298; learning rate 0.000450
Epoch 3 iteration 0061/0187: training loss 0.298; learning rate 0.000450
Epoch 3 iteration 0062/0187: training loss 0.296; learning rate 0.000450
Epoch 3 iteration 0063/0187: training loss 0.296; learning rate 0.000449
Epoch 3 iteration 0064/0187: training loss 0.296; learning rate 0.000449
Epoch 3 iteration 0065/0187: training loss 0.295; learning rate 0.000449
Epoch 3 iteration 0066/0187: training loss 0.295; learning rate 0.000449
Epoch 3 iteration 0067/0187: training loss 0.295; learning rate 0.000449
Epoch 3 iteration 0068/0187: training loss 0.295; learning rate 0.000449
Epoch 3 iteration 0069/0187: training loss 0.297; learning rate 0.000449
Epoch 3 iteration 0070/0187: training loss 0.297; learning rate 0.000449
Epoch 3 iteration 0071/0187: training loss 0.297; learning rate 0.000449
Epoch 3 iteration 0072/0187: training loss 0.296; learning rate 0.000449
Epoch 3 iteration 0073/0187: training loss 0.298; learning rate 0.000449
Epoch 3 iteration 0074/0187: training loss 0.297; learning rate 0.000449
Epoch 3 iteration 0075/0187: training loss 0.297; learning rate 0.000449
Epoch 3 iteration 0076/0187: training loss 0.297; learning rate 0.000448
Epoch 3 iteration 0077/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0078/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0079/0187: training loss 0.299; learning rate 0.000448
Epoch 3 iteration 0080/0187: training loss 0.300; learning rate 0.000448
Epoch 3 iteration 0081/0187: training loss 0.299; learning rate 0.000448
Epoch 3 iteration 0082/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0083/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0084/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0085/0187: training loss 0.297; learning rate 0.000448
Epoch 3 iteration 0086/0187: training loss 0.297; learning rate 0.000448
Epoch 3 iteration 0087/0187: training loss 0.298; learning rate 0.000448
Epoch 3 iteration 0088/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0089/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0090/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0091/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0092/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0093/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0094/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0095/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0096/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0097/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0098/0187: training loss 0.298; learning rate 0.000447
Epoch 3 iteration 0099/0187: training loss 0.297; learning rate 0.000447
Epoch 3 iteration 0100/0187: training loss 0.297; learning rate 0.000446
Epoch 3 iteration 0101/0187: training loss 0.297; learning rate 0.000446
Epoch 3 iteration 0102/0187: training loss 0.297; learning rate 0.000446
Epoch 3 iteration 0103/0187: training loss 0.296; learning rate 0.000446
Epoch 3 iteration 0104/0187: training loss 0.296; learning rate 0.000446
Epoch 3 iteration 0105/0187: training loss 0.296; learning rate 0.000446
Epoch 3 iteration 0106/0187: training loss 0.296; learning rate 0.000446
Epoch 3 iteration 0107/0187: training loss 0.295; learning rate 0.000446
Epoch 3 iteration 0108/0187: training loss 0.295; learning rate 0.000446
Epoch 3 iteration 0109/0187: training loss 0.296; learning rate 0.000446
Epoch 3 iteration 0110/0187: training loss 0.295; learning rate 0.000446
Epoch 3 iteration 0111/0187: training loss 0.295; learning rate 0.000446
Epoch 3 iteration 0112/0187: training loss 0.295; learning rate 0.000446
Epoch 3 iteration 0113/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0114/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0115/0187: training loss 0.295; learning rate 0.000445
Epoch 3 iteration 0116/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0117/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0118/0187: training loss 0.297; learning rate 0.000445
Epoch 3 iteration 0119/0187: training loss 0.297; learning rate 0.000445
Epoch 3 iteration 0120/0187: training loss 0.297; learning rate 0.000445
Epoch 3 iteration 0121/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0122/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0123/0187: training loss 0.296; learning rate 0.000445
Epoch 3 iteration 0124/0187: training loss 0.295; learning rate 0.000445
Epoch 3 iteration 0125/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0126/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0127/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0128/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0129/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0130/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0131/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0132/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0133/0187: training loss 0.296; learning rate 0.000444
Epoch 3 iteration 0134/0187: training loss 0.295; learning rate 0.000444
Epoch 3 iteration 0135/0187: training loss 0.295; learning rate 0.000444
Epoch 3 iteration 0136/0187: training loss 0.295; learning rate 0.000444
Epoch 3 iteration 0137/0187: training loss 0.294; learning rate 0.000443
Epoch 3 iteration 0138/0187: training loss 0.294; learning rate 0.000443
Epoch 3 iteration 0139/0187: training loss 0.296; learning rate 0.000443
Epoch 3 iteration 0140/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0141/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0142/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0143/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0144/0187: training loss 0.296; learning rate 0.000443
Epoch 3 iteration 0145/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0146/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0147/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0148/0187: training loss 0.297; learning rate 0.000443
Epoch 3 iteration 0149/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0150/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0151/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0152/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0153/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0154/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0155/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0156/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0157/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0158/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0159/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0160/0187: training loss 0.297; learning rate 0.000442
Epoch 3 iteration 0161/0187: training loss 0.296; learning rate 0.000442
Epoch 3 iteration 0162/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0163/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0164/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0165/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0166/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0167/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0168/0187: training loss 0.296; learning rate 0.000441
Epoch 3 iteration 0169/0187: training loss 0.296; learning rate 0.000441
Epoch 3 iteration 0170/0187: training loss 0.296; learning rate 0.000441
Epoch 3 iteration 0171/0187: training loss 0.296; learning rate 0.000441
Epoch 3 iteration 0172/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0173/0187: training loss 0.297; learning rate 0.000441
Epoch 3 iteration 0174/0187: training loss 0.297; learning rate 0.000440
Epoch 3 iteration 0175/0187: training loss 0.297; learning rate 0.000440
Epoch 3 iteration 0176/0187: training loss 0.297; learning rate 0.000440
Epoch 3 iteration 0177/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0178/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0179/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0180/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0181/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0182/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0183/0187: training loss 0.297; learning rate 0.000440
Epoch 3 iteration 0184/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0185/0187: training loss 0.296; learning rate 0.000440
Epoch 3 iteration 0186/0187: training loss 0.296; learning rate 0.000439
Epoch 3 iteration 0187/0187: training loss 0.295; learning rate 0.000439
Epoch 3 validation pixAcc: 0.319, mIoU: 0.182
Epoch 4 iteration 0001/0187: training loss 0.319; learning rate 0.000439
Epoch 4 iteration 0002/0187: training loss 0.302; learning rate 0.000439
Epoch 4 iteration 0003/0187: training loss 0.288; learning rate 0.000439
Epoch 4 iteration 0004/0187: training loss 0.280; learning rate 0.000439
Epoch 4 iteration 0005/0187: training loss 0.296; learning rate 0.000439
Epoch 4 iteration 0006/0187: training loss 0.289; learning rate 0.000439
Epoch 4 iteration 0007/0187: training loss 0.301; learning rate 0.000439
Epoch 4 iteration 0008/0187: training loss 0.296; learning rate 0.000439
Epoch 4 iteration 0009/0187: training loss 0.299; learning rate 0.000439
Epoch 4 iteration 0010/0187: training loss 0.304; learning rate 0.000439
Epoch 4 iteration 0011/0187: training loss 0.305; learning rate 0.000438
Epoch 4 iteration 0012/0187: training loss 0.301; learning rate 0.000438
Epoch 4 iteration 0013/0187: training loss 0.304; learning rate 0.000438
Epoch 4 iteration 0014/0187: training loss 0.297; learning rate 0.000438
Epoch 4 iteration 0015/0187: training loss 0.294; learning rate 0.000438
Epoch 4 iteration 0016/0187: training loss 0.291; learning rate 0.000438
Epoch 4 iteration 0017/0187: training loss 0.291; learning rate 0.000438
Epoch 4 iteration 0018/0187: training loss 0.287; learning rate 0.000438
Epoch 4 iteration 0019/0187: training loss 0.281; learning rate 0.000438
Epoch 4 iteration 0020/0187: training loss 0.277; learning rate 0.000438
Epoch 4 iteration 0021/0187: training loss 0.273; learning rate 0.000438
Epoch 4 iteration 0022/0187: training loss 0.275; learning rate 0.000438
Epoch 4 iteration 0023/0187: training loss 0.278; learning rate 0.000437
Epoch 4 iteration 0024/0187: training loss 0.274; learning rate 0.000437
Epoch 4 iteration 0025/0187: training loss 0.273; learning rate 0.000437
Epoch 4 iteration 0026/0187: training loss 0.274; learning rate 0.000437
Epoch 4 iteration 0027/0187: training loss 0.276; learning rate 0.000437
Epoch 4 iteration 0028/0187: training loss 0.274; learning rate 0.000437
Epoch 4 iteration 0029/0187: training loss 0.277; learning rate 0.000437
Epoch 4 iteration 0030/0187: training loss 0.275; learning rate 0.000437
Epoch 4 iteration 0031/0187: training loss 0.273; learning rate 0.000437
Epoch 4 iteration 0032/0187: training loss 0.271; learning rate 0.000437
Epoch 4 iteration 0033/0187: training loss 0.269; learning rate 0.000437
Epoch 4 iteration 0034/0187: training loss 0.268; learning rate 0.000437
Epoch 4 iteration 0035/0187: training loss 0.267; learning rate 0.000436
Epoch 4 iteration 0036/0187: training loss 0.271; learning rate 0.000436
Epoch 4 iteration 0037/0187: training loss 0.270; learning rate 0.000436
Epoch 4 iteration 0038/0187: training loss 0.274; learning rate 0.000436
Epoch 4 iteration 0039/0187: training loss 0.273; learning rate 0.000436
Epoch 4 iteration 0040/0187: training loss 0.272; learning rate 0.000436
Epoch 4 iteration 0041/0187: training loss 0.273; learning rate 0.000436
Epoch 4 iteration 0042/0187: training loss 0.272; learning rate 0.000436
Epoch 4 iteration 0043/0187: training loss 0.273; learning rate 0.000436
Epoch 4 iteration 0044/0187: training loss 0.271; learning rate 0.000436
Epoch 4 iteration 0045/0187: training loss 0.271; learning rate 0.000436
Epoch 4 iteration 0046/0187: training loss 0.273; learning rate 0.000436
Epoch 4 iteration 0047/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0048/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0049/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0050/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0051/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0052/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0053/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0054/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0055/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0056/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0057/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0058/0187: training loss 0.275; learning rate 0.000435
Epoch 4 iteration 0059/0187: training loss 0.274; learning rate 0.000435
Epoch 4 iteration 0060/0187: training loss 0.274; learning rate 0.000434
Epoch 4 iteration 0061/0187: training loss 0.273; learning rate 0.000434
Epoch 4 iteration 0062/0187: training loss 0.273; learning rate 0.000434
Epoch 4 iteration 0063/0187: training loss 0.273; learning rate 0.000434
Epoch 4 iteration 0064/0187: training loss 0.274; learning rate 0.000434
Epoch 4 iteration 0065/0187: training loss 0.275; learning rate 0.000434
Epoch 4 iteration 0066/0187: training loss 0.277; learning rate 0.000434
Epoch 4 iteration 0067/0187: training loss 0.277; learning rate 0.000434
Epoch 4 iteration 0068/0187: training loss 0.276; learning rate 0.000434
Epoch 4 iteration 0069/0187: training loss 0.276; learning rate 0.000434
Epoch 4 iteration 0070/0187: training loss 0.277; learning rate 0.000434
Epoch 4 iteration 0071/0187: training loss 0.278; learning rate 0.000434
Epoch 4 iteration 0072/0187: training loss 0.278; learning rate 0.000433
Epoch 4 iteration 0073/0187: training loss 0.278; learning rate 0.000433
Epoch 4 iteration 0074/0187: training loss 0.278; learning rate 0.000433
Epoch 4 iteration 0075/0187: training loss 0.279; learning rate 0.000433
Epoch 4 iteration 0076/0187: training loss 0.280; learning rate 0.000433
Epoch 4 iteration 0077/0187: training loss 0.280; learning rate 0.000433
Epoch 4 iteration 0078/0187: training loss 0.280; learning rate 0.000433
Epoch 4 iteration 0079/0187: training loss 0.279; learning rate 0.000433
Epoch 4 iteration 0080/0187: training loss 0.279; learning rate 0.000433
Epoch 4 iteration 0081/0187: training loss 0.278; learning rate 0.000433
Epoch 4 iteration 0082/0187: training loss 0.278; learning rate 0.000433
Epoch 4 iteration 0083/0187: training loss 0.277; learning rate 0.000433
Epoch 4 iteration 0084/0187: training loss 0.276; learning rate 0.000432
Epoch 4 iteration 0085/0187: training loss 0.276; learning rate 0.000432
Epoch 4 iteration 0086/0187: training loss 0.276; learning rate 0.000432
Epoch 4 iteration 0087/0187: training loss 0.277; learning rate 0.000432
Epoch 4 iteration 0088/0187: training loss 0.276; learning rate 0.000432
Epoch 4 iteration 0089/0187: training loss 0.277; learning rate 0.000432
Epoch 4 iteration 0090/0187: training loss 0.277; learning rate 0.000432
Epoch 4 iteration 0091/0188: training loss 0.276; learning rate 0.000432
Epoch 4 iteration 0092/0188: training loss 0.275; learning rate 0.000432
Epoch 4 iteration 0093/0188: training loss 0.274; learning rate 0.000432
Epoch 4 iteration 0094/0188: training loss 0.275; learning rate 0.000432
Epoch 4 iteration 0095/0188: training loss 0.275; learning rate 0.000432
Epoch 4 iteration 0096/0188: training loss 0.274; learning rate 0.000432
Epoch 4 iteration 0097/0188: training loss 0.274; learning rate 0.000431
Epoch 4 iteration 0098/0188: training loss 0.274; learning rate 0.000431
Epoch 4 iteration 0099/0188: training loss 0.274; learning rate 0.000431
Epoch 4 iteration 0100/0188: training loss 0.275; learning rate 0.000431
Epoch 4 iteration 0101/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0102/0188: training loss 0.275; learning rate 0.000431
Epoch 4 iteration 0103/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0104/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0105/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0106/0188: training loss 0.275; learning rate 0.000431
Epoch 4 iteration 0107/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0108/0188: training loss 0.276; learning rate 0.000431
Epoch 4 iteration 0109/0188: training loss 0.276; learning rate 0.000430
Epoch 4 iteration 0110/0188: training loss 0.277; learning rate 0.000430
Epoch 4 iteration 0111/0188: training loss 0.277; learning rate 0.000430
Epoch 4 iteration 0112/0188: training loss 0.276; learning rate 0.000430
Epoch 4 iteration 0113/0188: training loss 0.277; learning rate 0.000430
Epoch 4 iteration 0114/0188: training loss 0.276; learning rate 0.000430
Epoch 4 iteration 0115/0188: training loss 0.276; learning rate 0.000430
Epoch 4 iteration 0116/0188: training loss 0.278; learning rate 0.000430
Epoch 4 iteration 0117/0188: training loss 0.278; learning rate 0.000430
Epoch 4 iteration 0118/0188: training loss 0.278; learning rate 0.000430
Epoch 4 iteration 0119/0188: training loss 0.278; learning rate 0.000430
Epoch 4 iteration 0120/0188: training loss 0.278; learning rate 0.000430
Epoch 4 iteration 0121/0188: training loss 0.278; learning rate 0.000429
Epoch 4 iteration 0122/0188: training loss 0.278; learning rate 0.000429
Epoch 4 iteration 0123/0188: training loss 0.278; learning rate 0.000429
Epoch 4 iteration 0124/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0125/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0126/0188: training loss 0.278; learning rate 0.000429
Epoch 4 iteration 0127/0188: training loss 0.278; learning rate 0.000429
Epoch 4 iteration 0128/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0129/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0130/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0131/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0132/0188: training loss 0.277; learning rate 0.000429
Epoch 4 iteration 0133/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0134/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0135/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0136/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0137/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0138/0188: training loss 0.278; learning rate 0.000428
Epoch 4 iteration 0139/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0140/0188: training loss 0.277; learning rate 0.000428
Epoch 4 iteration 0141/0188: training loss 0.278; learning rate 0.000428
Epoch 4 iteration 0142/0188: training loss 0.279; learning rate 0.000428
Epoch 4 iteration 0143/0188: training loss 0.279; learning rate 0.000428
Epoch 4 iteration 0144/0188: training loss 0.279; learning rate 0.000428
Epoch 4 iteration 0145/0188: training loss 0.278; learning rate 0.000428
Epoch 4 iteration 0146/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0147/0188: training loss 0.278; learning rate 0.000427
Epoch 4 iteration 0148/0188: training loss 0.278; learning rate 0.000427
Epoch 4 iteration 0149/0188: training loss 0.278; learning rate 0.000427
Epoch 4 iteration 0150/0188: training loss 0.278; learning rate 0.000427
Epoch 4 iteration 0151/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0152/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0153/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0154/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0155/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0156/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0157/0188: training loss 0.279; learning rate 0.000427
Epoch 4 iteration 0158/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0159/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0160/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0161/0188: training loss 0.277; learning rate 0.000426
Epoch 4 iteration 0162/0188: training loss 0.277; learning rate 0.000426
Epoch 4 iteration 0163/0188: training loss 0.277; learning rate 0.000426
Epoch 4 iteration 0164/0188: training loss 0.277; learning rate 0.000426
Epoch 4 iteration 0165/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0166/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0167/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0168/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0169/0188: training loss 0.278; learning rate 0.000426
Epoch 4 iteration 0170/0188: training loss 0.278; learning rate 0.000425
Epoch 4 iteration 0171/0188: training loss 0.278; learning rate 0.000425
Epoch 4 iteration 0172/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0173/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0174/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0175/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0176/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0177/0188: training loss 0.277; learning rate 0.000425
Epoch 4 iteration 0178/0188: training loss 0.276; learning rate 0.000425
Epoch 4 iteration 0179/0188: training loss 0.276; learning rate 0.000425
Epoch 4 iteration 0180/0188: training loss 0.276; learning rate 0.000425
Epoch 4 iteration 0181/0188: training loss 0.276; learning rate 0.000425
Epoch 4 iteration 0182/0188: training loss 0.277; learning rate 0.000424
Epoch 4 iteration 0183/0188: training loss 0.276; learning rate 0.000424
Epoch 4 iteration 0184/0188: training loss 0.277; learning rate 0.000424
Epoch 4 iteration 0185/0188: training loss 0.277; learning rate 0.000424
Epoch 4 iteration 0186/0188: training loss 0.277; learning rate 0.000424
Epoch 4 validation pixAcc: 0.326, mIoU: 0.192
Epoch 5 iteration 0001/0187: training loss 0.263; learning rate 0.000424
Epoch 5 iteration 0002/0187: training loss 0.251; learning rate 0.000424
Epoch 5 iteration 0003/0187: training loss 0.252; learning rate 0.000424
Epoch 5 iteration 0004/0187: training loss 0.264; learning rate 0.000424
Epoch 5 iteration 0005/0187: training loss 0.262; learning rate 0.000424
Epoch 5 iteration 0006/0187: training loss 0.258; learning rate 0.000424
Epoch 5 iteration 0007/0187: training loss 0.263; learning rate 0.000424
Epoch 5 iteration 0008/0187: training loss 0.261; learning rate 0.000423
Epoch 5 iteration 0009/0187: training loss 0.264; learning rate 0.000423
Epoch 5 iteration 0010/0187: training loss 0.272; learning rate 0.000423
Epoch 5 iteration 0011/0187: training loss 0.269; learning rate 0.000423
Epoch 5 iteration 0012/0187: training loss 0.267; learning rate 0.000423
Epoch 5 iteration 0013/0187: training loss 0.277; learning rate 0.000423
Epoch 5 iteration 0014/0187: training loss 0.277; learning rate 0.000423
Epoch 5 iteration 0015/0187: training loss 0.275; learning rate 0.000423
Epoch 5 iteration 0016/0187: training loss 0.270; learning rate 0.000423
Epoch 5 iteration 0017/0187: training loss 0.269; learning rate 0.000423
Epoch 5 iteration 0018/0187: training loss 0.271; learning rate 0.000423
Epoch 5 iteration 0019/0187: training loss 0.272; learning rate 0.000423
Epoch 5 iteration 0020/0187: training loss 0.270; learning rate 0.000422
Epoch 5 iteration 0021/0187: training loss 0.273; learning rate 0.000422
Epoch 5 iteration 0022/0187: training loss 0.271; learning rate 0.000422
Epoch 5 iteration 0023/0187: training loss 0.270; learning rate 0.000422
Epoch 5 iteration 0024/0187: training loss 0.272; learning rate 0.000422
Epoch 5 iteration 0025/0187: training loss 0.268; learning rate 0.000422
Epoch 5 iteration 0026/0187: training loss 0.266; learning rate 0.000422
Epoch 5 iteration 0027/0187: training loss 0.266; learning rate 0.000422
Epoch 5 iteration 0028/0187: training loss 0.265; learning rate 0.000422
Epoch 5 iteration 0029/0187: training loss 0.265; learning rate 0.000422
Epoch 5 iteration 0030/0187: training loss 0.270; learning rate 0.000422
Epoch 5 iteration 0031/0187: training loss 0.269; learning rate 0.000422
Epoch 5 iteration 0032/0187: training loss 0.269; learning rate 0.000421
Epoch 5 iteration 0033/0187: training loss 0.267; learning rate 0.000421
Epoch 5 iteration 0034/0187: training loss 0.269; learning rate 0.000421
Epoch 5 iteration 0035/0187: training loss 0.271; learning rate 0.000421
Epoch 5 iteration 0036/0187: training loss 0.269; learning rate 0.000421
Epoch 5 iteration 0037/0187: training loss 0.269; learning rate 0.000421
Epoch 5 iteration 0038/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0039/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0040/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0041/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0042/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0043/0187: training loss 0.268; learning rate 0.000421
Epoch 5 iteration 0044/0187: training loss 0.269; learning rate 0.000420
Epoch 5 iteration 0045/0187: training loss 0.270; learning rate 0.000420
Epoch 5 iteration 0046/0187: training loss 0.269; learning rate 0.000420
Epoch 5 iteration 0047/0187: training loss 0.268; learning rate 0.000420
Epoch 5 iteration 0048/0187: training loss 0.269; learning rate 0.000420
Epoch 5 iteration 0049/0187: training loss 0.268; learning rate 0.000420
Epoch 5 iteration 0050/0187: training loss 0.267; learning rate 0.000420
Epoch 5 iteration 0051/0187: training loss 0.268; learning rate 0.000420
Epoch 5 iteration 0052/0187: training loss 0.266; learning rate 0.000420
Epoch 5 iteration 0053/0187: training loss 0.267; learning rate 0.000420
Epoch 5 iteration 0054/0187: training loss 0.267; learning rate 0.000420
Epoch 5 iteration 0055/0187: training loss 0.267; learning rate 0.000420
Epoch 5 iteration 0056/0187: training loss 0.267; learning rate 0.000419
Epoch 5 iteration 0057/0187: training loss 0.269; learning rate 0.000419
Epoch 5 iteration 0058/0187: training loss 0.269; learning rate 0.000419
Epoch 5 iteration 0059/0187: training loss 0.270; learning rate 0.000419
Epoch 5 iteration 0060/0187: training loss 0.270; learning rate 0.000419
Epoch 5 iteration 0061/0187: training loss 0.269; learning rate 0.000419
Epoch 5 iteration 0062/0187: training loss 0.268; learning rate 0.000419
Epoch 5 iteration 0063/0187: training loss 0.269; learning rate 0.000419
Epoch 5 iteration 0064/0187: training loss 0.270; learning rate 0.000419
Epoch 5 iteration 0065/0187: training loss 0.269; learning rate 0.000419
Epoch 5 iteration 0066/0187: training loss 0.268; learning rate 0.000419
Epoch 5 iteration 0067/0187: training loss 0.268; learning rate 0.000419
Epoch 5 iteration 0068/0187: training loss 0.268; learning rate 0.000419
Epoch 5 iteration 0069/0187: training loss 0.269; learning rate 0.000418
Epoch 5 iteration 0070/0187: training loss 0.267; learning rate 0.000418
Epoch 5 iteration 0071/0187: training loss 0.268; learning rate 0.000418
Epoch 5 iteration 0072/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0073/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0074/0187: training loss 0.267; learning rate 0.000418
Epoch 5 iteration 0075/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0076/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0077/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0078/0187: training loss 0.265; learning rate 0.000418
Epoch 5 iteration 0079/0187: training loss 0.266; learning rate 0.000418
Epoch 5 iteration 0080/0187: training loss 0.265; learning rate 0.000418
Epoch 5 iteration 0081/0187: training loss 0.265; learning rate 0.000417
Epoch 5 iteration 0082/0187: training loss 0.265; learning rate 0.000417
Epoch 5 iteration 0083/0187: training loss 0.265; learning rate 0.000417
Epoch 5 iteration 0084/0187: training loss 0.266; learning rate 0.000417
Epoch 5 iteration 0085/0187: training loss 0.266; learning rate 0.000417
Epoch 5 iteration 0086/0187: training loss 0.267; learning rate 0.000417
Epoch 5 iteration 0087/0187: training loss 0.267; learning rate 0.000417
Epoch 5 iteration 0088/0187: training loss 0.267; learning rate 0.000417
Epoch 5 iteration 0089/0187: training loss 0.266; learning rate 0.000417
Epoch 5 iteration 0090/0187: training loss 0.266; learning rate 0.000417
Epoch 5 iteration 0091/0187: training loss 0.267; learning rate 0.000417
Epoch 5 iteration 0092/0187: training loss 0.267; learning rate 0.000417
Epoch 5 iteration 0093/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0094/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0095/0187: training loss 0.268; learning rate 0.000416
Epoch 5 iteration 0096/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0097/0187: training loss 0.268; learning rate 0.000416
Epoch 5 iteration 0098/0187: training loss 0.269; learning rate 0.000416
Epoch 5 iteration 0099/0187: training loss 0.268; learning rate 0.000416
Epoch 5 iteration 0100/0187: training loss 0.268; learning rate 0.000416
Epoch 5 iteration 0101/0187: training loss 0.268; learning rate 0.000416
Epoch 5 iteration 0102/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0103/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0104/0187: training loss 0.267; learning rate 0.000416
Epoch 5 iteration 0105/0187: training loss 0.267; learning rate 0.000415
Epoch 5 iteration 0106/0187: training loss 0.267; learning rate 0.000415
Epoch 5 iteration 0107/0187: training loss 0.268; learning rate 0.000415
Epoch 5 iteration 0108/0187: training loss 0.267; learning rate 0.000415
Epoch 5 iteration 0109/0187: training loss 0.267; learning rate 0.000415
Epoch 5 iteration 0110/0187: training loss 0.268; learning rate 0.000415
Epoch 5 iteration 0111/0187: training loss 0.268; learning rate 0.000415
Epoch 5 iteration 0112/0187: training loss 0.269; learning rate 0.000415
Epoch 5 iteration 0113/0187: training loss 0.269; learning rate 0.000415
Epoch 5 iteration 0114/0187: training loss 0.269; learning rate 0.000415
Epoch 5 iteration 0115/0187: training loss 0.269; learning rate 0.000415
Epoch 5 iteration 0116/0187: training loss 0.268; learning rate 0.000415
Epoch 5 iteration 0117/0187: training loss 0.268; learning rate 0.000415
Epoch 5 iteration 0118/0187: training loss 0.268; learning rate 0.000414
Epoch 5 iteration 0119/0187: training loss 0.268; learning rate 0.000414
Epoch 5 iteration 0120/0187: training loss 0.268; learning rate 0.000414
Epoch 5 iteration 0121/0187: training loss 0.268; learning rate 0.000414
Epoch 5 iteration 0122/0187: training loss 0.268; learning rate 0.000414
Epoch 5 iteration 0123/0187: training loss 0.267; learning rate 0.000414
Epoch 5 iteration 0124/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0125/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0126/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0127/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0128/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0129/0187: training loss 0.266; learning rate 0.000414
Epoch 5 iteration 0130/0187: training loss 0.266; learning rate 0.000413
Epoch 5 iteration 0131/0187: training loss 0.266; learning rate 0.000413
Epoch 5 iteration 0132/0187: training loss 0.266; learning rate 0.000413
Epoch 5 iteration 0133/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0134/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0135/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0136/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0137/0187: training loss 0.264; learning rate 0.000413
Epoch 5 iteration 0138/0187: training loss 0.264; learning rate 0.000413
Epoch 5 iteration 0139/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0140/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0141/0187: training loss 0.265; learning rate 0.000413
Epoch 5 iteration 0142/0187: training loss 0.265; learning rate 0.000412
Epoch 5 iteration 0143/0187: training loss 0.265; learning rate 0.000412
Epoch 5 iteration 0144/0187: training loss 0.265; learning rate 0.000412
Epoch 5 iteration 0145/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0146/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0147/0187: training loss 0.265; learning rate 0.000412
Epoch 5 iteration 0148/0187: training loss 0.265; learning rate 0.000412
Epoch 5 iteration 0149/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0150/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0151/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0152/0187: training loss 0.264; learning rate 0.000412
Epoch 5 iteration 0153/0187: training loss 0.263; learning rate 0.000412
Epoch 5 iteration 0154/0187: training loss 0.264; learning rate 0.000411
Epoch 5 iteration 0155/0187: training loss 0.264; learning rate 0.000411
Epoch 5 iteration 0156/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0157/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0158/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0159/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0160/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0161/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0162/0187: training loss 0.263; learning rate 0.000411
Epoch 5 iteration 0163/0187: training loss 0.264; learning rate 0.000411
Epoch 5 iteration 0164/0187: training loss 0.264; learning rate 0.000411
Epoch 5 iteration 0165/0187: training loss 0.264; learning rate 0.000411
Epoch 5 iteration 0166/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0167/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0168/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0169/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0170/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0171/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0172/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0173/0187: training loss 0.263; learning rate 0.000410
Epoch 5 iteration 0174/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0175/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0176/0187: training loss 0.264; learning rate 0.000410
Epoch 5 iteration 0177/0187: training loss 0.265; learning rate 0.000410
Epoch 5 iteration 0178/0187: training loss 0.265; learning rate 0.000410
Epoch 5 iteration 0179/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0180/0187: training loss 0.266; learning rate 0.000409
Epoch 5 iteration 0181/0187: training loss 0.266; learning rate 0.000409
Epoch 5 iteration 0182/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0183/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0184/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0185/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0186/0187: training loss 0.265; learning rate 0.000409
Epoch 5 iteration 0187/0187: training loss 0.265; learning rate 0.000409
Epoch 5 validation pixAcc: 0.328, mIoU: 0.199
Epoch 6 iteration 0001/0187: training loss 0.304; learning rate 0.000409
Epoch 6 iteration 0002/0187: training loss 0.294; learning rate 0.000409
Epoch 6 iteration 0003/0187: training loss 0.294; learning rate 0.000408
Epoch 6 iteration 0004/0187: training loss 0.270; learning rate 0.000408
Epoch 6 iteration 0005/0187: training loss 0.263; learning rate 0.000408
Epoch 6 iteration 0006/0187: training loss 0.275; learning rate 0.000408
Epoch 6 iteration 0007/0187: training loss 0.273; learning rate 0.000408
Epoch 6 iteration 0008/0187: training loss 0.277; learning rate 0.000408
Epoch 6 iteration 0009/0187: training loss 0.282; learning rate 0.000408
Epoch 6 iteration 0010/0187: training loss 0.276; learning rate 0.000408
Epoch 6 iteration 0011/0187: training loss 0.274; learning rate 0.000408
Epoch 6 iteration 0012/0187: training loss 0.276; learning rate 0.000408
Epoch 6 iteration 0013/0187: training loss 0.280; learning rate 0.000408
Epoch 6 iteration 0014/0187: training loss 0.276; learning rate 0.000408
Epoch 6 iteration 0015/0187: training loss 0.271; learning rate 0.000407
Epoch 6 iteration 0016/0187: training loss 0.267; learning rate 0.000407
Epoch 6 iteration 0017/0187: training loss 0.270; learning rate 0.000407
Epoch 6 iteration 0018/0187: training loss 0.268; learning rate 0.000407
Epoch 6 iteration 0019/0187: training loss 0.268; learning rate 0.000407
Epoch 6 iteration 0020/0187: training loss 0.268; learning rate 0.000407
Epoch 6 iteration 0021/0187: training loss 0.267; learning rate 0.000407
Epoch 6 iteration 0022/0187: training loss 0.266; learning rate 0.000407
Epoch 6 iteration 0023/0187: training loss 0.268; learning rate 0.000407
Epoch 6 iteration 0024/0187: training loss 0.266; learning rate 0.000407
Epoch 6 iteration 0025/0187: training loss 0.266; learning rate 0.000407
Epoch 6 iteration 0026/0187: training loss 0.265; learning rate 0.000407
Epoch 6 iteration 0027/0187: training loss 0.265; learning rate 0.000406
Epoch 6 iteration 0028/0187: training loss 0.263; learning rate 0.000406
Epoch 6 iteration 0029/0187: training loss 0.267; learning rate 0.000406
Epoch 6 iteration 0030/0187: training loss 0.267; learning rate 0.000406
Epoch 6 iteration 0031/0187: training loss 0.268; learning rate 0.000406
Epoch 6 iteration 0032/0187: training loss 0.264; learning rate 0.000406
Epoch 6 iteration 0033/0187: training loss 0.264; learning rate 0.000406
Epoch 6 iteration 0034/0187: training loss 0.261; learning rate 0.000406
Epoch 6 iteration 0035/0187: training loss 0.262; learning rate 0.000406
Epoch 6 iteration 0036/0187: training loss 0.260; learning rate 0.000406
Epoch 6 iteration 0037/0187: training loss 0.262; learning rate 0.000406
Epoch 6 iteration 0038/0187: training loss 0.260; learning rate 0.000406
Epoch 6 iteration 0039/0187: training loss 0.261; learning rate 0.000405
Epoch 6 iteration 0040/0187: training loss 0.262; learning rate 0.000405
Epoch 6 iteration 0041/0187: training loss 0.264; learning rate 0.000405
Epoch 6 iteration 0042/0187: training loss 0.262; learning rate 0.000405
Epoch 6 iteration 0043/0187: training loss 0.261; learning rate 0.000405
Epoch 6 iteration 0044/0187: training loss 0.266; learning rate 0.000405
Epoch 6 iteration 0045/0187: training loss 0.266; learning rate 0.000405
Epoch 6 iteration 0046/0187: training loss 0.266; learning rate 0.000405
Epoch 6 iteration 0047/0187: training loss 0.267; learning rate 0.000405
Epoch 6 iteration 0048/0187: training loss 0.267; learning rate 0.000405
Epoch 6 iteration 0049/0187: training loss 0.266; learning rate 0.000405
Epoch 6 iteration 0050/0187: training loss 0.265; learning rate 0.000405
Epoch 6 iteration 0051/0187: training loss 0.264; learning rate 0.000404
Epoch 6 iteration 0052/0187: training loss 0.265; learning rate 0.000404
Epoch 6 iteration 0053/0187: training loss 0.265; learning rate 0.000404
Epoch 6 iteration 0054/0187: training loss 0.265; learning rate 0.000404
Epoch 6 iteration 0055/0187: training loss 0.264; learning rate 0.000404
Epoch 6 iteration 0056/0187: training loss 0.263; learning rate 0.000404
Epoch 6 iteration 0057/0187: training loss 0.264; learning rate 0.000404
Epoch 6 iteration 0058/0187: training loss 0.264; learning rate 0.000404
Epoch 6 iteration 0059/0187: training loss 0.263; learning rate 0.000404
Epoch 6 iteration 0060/0187: training loss 0.262; learning rate 0.000404
Epoch 6 iteration 0061/0187: training loss 0.263; learning rate 0.000404
Epoch 6 iteration 0062/0187: training loss 0.262; learning rate 0.000404
Epoch 6 iteration 0063/0187: training loss 0.261; learning rate 0.000404
Epoch 6 iteration 0064/0187: training loss 0.262; learning rate 0.000403
Epoch 6 iteration 0065/0187: training loss 0.264; learning rate 0.000403
Epoch 6 iteration 0066/0187: training loss 0.264; learning rate 0.000403
Epoch 6 iteration 0067/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0068/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0069/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0070/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0071/0187: training loss 0.262; learning rate 0.000403
Epoch 6 iteration 0072/0187: training loss 0.262; learning rate 0.000403
Epoch 6 iteration 0073/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0074/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0075/0187: training loss 0.263; learning rate 0.000403
Epoch 6 iteration 0076/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0077/0187: training loss 0.263; learning rate 0.000402
Epoch 6 iteration 0078/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0079/0187: training loss 0.265; learning rate 0.000402
Epoch 6 iteration 0080/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0081/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0082/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0083/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0084/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0085/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0086/0187: training loss 0.264; learning rate 0.000402
Epoch 6 iteration 0087/0187: training loss 0.263; learning rate 0.000402
Epoch 6 iteration 0088/0187: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0089/0187: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0090/0187: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0091/0188: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0092/0188: training loss 0.262; learning rate 0.000401
Epoch 6 iteration 0093/0188: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0094/0188: training loss 0.262; learning rate 0.000401
Epoch 6 iteration 0095/0188: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0096/0188: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0097/0188: training loss 0.262; learning rate 0.000401
Epoch 6 iteration 0098/0188: training loss 0.262; learning rate 0.000401
Epoch 6 iteration 0099/0188: training loss 0.263; learning rate 0.000401
Epoch 6 iteration 0100/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0101/0188: training loss 0.264; learning rate 0.000400
Epoch 6 iteration 0102/0188: training loss 0.264; learning rate 0.000400
Epoch 6 iteration 0103/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0104/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0105/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0106/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0107/0188: training loss 0.264; learning rate 0.000400
Epoch 6 iteration 0108/0188: training loss 0.264; learning rate 0.000400
Epoch 6 iteration 0109/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0110/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0111/0188: training loss 0.263; learning rate 0.000400
Epoch 6 iteration 0112/0188: training loss 0.263; learning rate 0.000399
Epoch 6 iteration 0113/0188: training loss 0.263; learning rate 0.000399
Epoch 6 iteration 0114/0188: training loss 0.263; learning rate 0.000399
Epoch 6 iteration 0115/0188: training loss 0.263; learning rate 0.000399
Epoch 6 iteration 0116/0188: training loss 0.263; learning rate 0.000399
Epoch 6 iteration 0117/0188: training loss 0.262; learning rate 0.000399
Epoch 6 iteration 0118/0188: training loss 0.261; learning rate 0.000399
Epoch 6 iteration 0119/0188: training loss 0.261; learning rate 0.000399
Epoch 6 iteration 0120/0188: training loss 0.260; learning rate 0.000399
Epoch 6 iteration 0121/0188: training loss 0.260; learning rate 0.000399
Epoch 6 iteration 0122/0188: training loss 0.260; learning rate 0.000399
Epoch 6 iteration 0123/0188: training loss 0.259; learning rate 0.000399
Epoch 6 iteration 0124/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0125/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0126/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0127/0188: training loss 0.258; learning rate 0.000398
Epoch 6 iteration 0128/0188: training loss 0.258; learning rate 0.000398
Epoch 6 iteration 0129/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0130/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0131/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0132/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0133/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0134/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0135/0188: training loss 0.258; learning rate 0.000398
Epoch 6 iteration 0136/0188: training loss 0.259; learning rate 0.000398
Epoch 6 iteration 0137/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0138/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0139/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0140/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0141/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0142/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0143/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0144/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0145/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0146/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0147/0188: training loss 0.257; learning rate 0.000397
Epoch 6 iteration 0148/0188: training loss 0.258; learning rate 0.000397
Epoch 6 iteration 0149/0188: training loss 0.258; learning rate 0.000396
Epoch 6 iteration 0150/0188: training loss 0.258; learning rate 0.000396
Epoch 6 iteration 0151/0188: training loss 0.258; learning rate 0.000396
Epoch 6 iteration 0152/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0153/0188: training loss 0.258; learning rate 0.000396
Epoch 6 iteration 0154/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0155/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0156/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0157/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0158/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0159/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0160/0188: training loss 0.257; learning rate 0.000396
Epoch 6 iteration 0161/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0162/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0163/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0164/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0165/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0166/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0167/0188: training loss 0.256; learning rate 0.000395
Epoch 6 iteration 0168/0188: training loss 0.256; learning rate 0.000395
Epoch 6 iteration 0169/0188: training loss 0.256; learning rate 0.000395
Epoch 6 iteration 0170/0188: training loss 0.256; learning rate 0.000395
Epoch 6 iteration 0171/0188: training loss 0.257; learning rate 0.000395
Epoch 6 iteration 0172/0188: training loss 0.256; learning rate 0.000395
Epoch 6 iteration 0173/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0174/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0175/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0176/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0177/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0178/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0179/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0180/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0181/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0182/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0183/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0184/0188: training loss 0.256; learning rate 0.000394
Epoch 6 iteration 0185/0188: training loss 0.256; learning rate 0.000393
Epoch 6 iteration 0186/0188: training loss 0.256; learning rate 0.000393
Epoch 6 validation pixAcc: 0.331, mIoU: 0.198
Epoch 7 iteration 0001/0187: training loss 0.245; learning rate 0.000393
Epoch 7 iteration 0002/0187: training loss 0.229; learning rate 0.000393
Epoch 7 iteration 0003/0187: training loss 0.256; learning rate 0.000393
Epoch 7 iteration 0004/0187: training loss 0.259; learning rate 0.000393
Epoch 7 iteration 0005/0187: training loss 0.288; learning rate 0.000393
Epoch 7 iteration 0006/0187: training loss 0.284; learning rate 0.000393
Epoch 7 iteration 0007/0187: training loss 0.272; learning rate 0.000393
Epoch 7 iteration 0008/0187: training loss 0.266; learning rate 0.000393
Epoch 7 iteration 0009/0187: training loss 0.263; learning rate 0.000393
Epoch 7 iteration 0010/0187: training loss 0.262; learning rate 0.000392
Epoch 7 iteration 0011/0187: training loss 0.261; learning rate 0.000392
Epoch 7 iteration 0012/0187: training loss 0.253; learning rate 0.000392
Epoch 7 iteration 0013/0187: training loss 0.257; learning rate 0.000392
Epoch 7 iteration 0014/0187: training loss 0.256; learning rate 0.000392
Epoch 7 iteration 0015/0187: training loss 0.258; learning rate 0.000392
Epoch 7 iteration 0016/0187: training loss 0.256; learning rate 0.000392
Epoch 7 iteration 0017/0187: training loss 0.258; learning rate 0.000392
Epoch 7 iteration 0018/0187: training loss 0.255; learning rate 0.000392
Epoch 7 iteration 0019/0187: training loss 0.251; learning rate 0.000392
Epoch 7 iteration 0020/0187: training loss 0.250; learning rate 0.000392
Epoch 7 iteration 0021/0187: training loss 0.248; learning rate 0.000392
Epoch 7 iteration 0022/0187: training loss 0.251; learning rate 0.000391
Epoch 7 iteration 0023/0187: training loss 0.251; learning rate 0.000391
Epoch 7 iteration 0024/0187: training loss 0.248; learning rate 0.000391
Epoch 7 iteration 0025/0187: training loss 0.247; learning rate 0.000391
Epoch 7 iteration 0026/0187: training loss 0.248; learning rate 0.000391
Epoch 7 iteration 0027/0187: training loss 0.248; learning rate 0.000391
Epoch 7 iteration 0028/0187: training loss 0.247; learning rate 0.000391
Epoch 7 iteration 0029/0187: training loss 0.246; learning rate 0.000391
Epoch 7 iteration 0030/0187: training loss 0.244; learning rate 0.000391
Epoch 7 iteration 0031/0187: training loss 0.242; learning rate 0.000391
Epoch 7 iteration 0032/0187: training loss 0.243; learning rate 0.000391
Epoch 7 iteration 0033/0187: training loss 0.243; learning rate 0.000391
Epoch 7 iteration 0034/0187: training loss 0.245; learning rate 0.000391
Epoch 7 iteration 0035/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0036/0187: training loss 0.243; learning rate 0.000390
Epoch 7 iteration 0037/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0038/0187: training loss 0.246; learning rate 0.000390
Epoch 7 iteration 0039/0187: training loss 0.246; learning rate 0.000390
Epoch 7 iteration 0040/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0041/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0042/0187: training loss 0.246; learning rate 0.000390
Epoch 7 iteration 0043/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0044/0187: training loss 0.245; learning rate 0.000390
Epoch 7 iteration 0045/0187: training loss 0.244; learning rate 0.000390
Epoch 7 iteration 0046/0187: training loss 0.244; learning rate 0.000390
Epoch 7 iteration 0047/0187: training loss 0.243; learning rate 0.000389
Epoch 7 iteration 0048/0187: training loss 0.245; learning rate 0.000389
Epoch 7 iteration 0049/0187: training loss 0.247; learning rate 0.000389
Epoch 7 iteration 0050/0187: training loss 0.247; learning rate 0.000389
Epoch 7 iteration 0051/0187: training loss 0.249; learning rate 0.000389
Epoch 7 iteration 0052/0187: training loss 0.248; learning rate 0.000389
Epoch 7 iteration 0053/0187: training loss 0.248; learning rate 0.000389
Epoch 7 iteration 0054/0187: training loss 0.248; learning rate 0.000389
Epoch 7 iteration 0055/0187: training loss 0.249; learning rate 0.000389
Epoch 7 iteration 0056/0187: training loss 0.248; learning rate 0.000389
Epoch 7 iteration 0057/0187: training loss 0.250; learning rate 0.000389
Epoch 7 iteration 0058/0187: training loss 0.250; learning rate 0.000389
Epoch 7 iteration 0059/0187: training loss 0.250; learning rate 0.000388
Epoch 7 iteration 0060/0187: training loss 0.250; learning rate 0.000388
Epoch 7 iteration 0061/0187: training loss 0.250; learning rate 0.000388
Epoch 7 iteration 0062/0187: training loss 0.250; learning rate 0.000388
Epoch 7 iteration 0063/0187: training loss 0.249; learning rate 0.000388
Epoch 7 iteration 0064/0187: training loss 0.249; learning rate 0.000388
Epoch 7 iteration 0065/0187: training loss 0.250; learning rate 0.000388
Epoch 7 iteration 0066/0187: training loss 0.249; learning rate 0.000388
Epoch 7 iteration 0067/0187: training loss 0.249; learning rate 0.000388
Epoch 7 iteration 0068/0187: training loss 0.249; learning rate 0.000388
Epoch 7 iteration 0069/0187: training loss 0.248; learning rate 0.000388
Epoch 7 iteration 0070/0187: training loss 0.248; learning rate 0.000388
Epoch 7 iteration 0071/0187: training loss 0.246; learning rate 0.000387
Epoch 7 iteration 0072/0187: training loss 0.246; learning rate 0.000387
Epoch 7 iteration 0073/0187: training loss 0.246; learning rate 0.000387
Epoch 7 iteration 0074/0187: training loss 0.246; learning rate 0.000387
Epoch 7 iteration 0075/0187: training loss 0.247; learning rate 0.000387
Epoch 7 iteration 0076/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0077/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0078/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0079/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0080/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0081/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0082/0187: training loss 0.248; learning rate 0.000387
Epoch 7 iteration 0083/0187: training loss 0.249; learning rate 0.000386
Epoch 7 iteration 0084/0187: training loss 0.249; learning rate 0.000386
Epoch 7 iteration 0085/0187: training loss 0.249; learning rate 0.000386
Epoch 7 iteration 0086/0187: training loss 0.249; learning rate 0.000386
Epoch 7 iteration 0087/0187: training loss 0.250; learning rate 0.000386
Epoch 7 iteration 0088/0187: training loss 0.250; learning rate 0.000386
Epoch 7 iteration 0089/0187: training loss 0.250; learning rate 0.000386
Epoch 7 iteration 0090/0187: training loss 0.251; learning rate 0.000386
Epoch 7 iteration 0091/0187: training loss 0.251; learning rate 0.000386
Epoch 7 iteration 0092/0187: training loss 0.251; learning rate 0.000386
Epoch 7 iteration 0093/0187: training loss 0.250; learning rate 0.000386
Epoch 7 iteration 0094/0187: training loss 0.252; learning rate 0.000386
Epoch 7 iteration 0095/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0096/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0097/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0098/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0099/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0100/0187: training loss 0.252; learning rate 0.000385
Epoch 7 iteration 0101/0187: training loss 0.252; learning rate 0.000385
Epoch 7 iteration 0102/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0103/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0104/0187: training loss 0.254; learning rate 0.000385
Epoch 7 iteration 0105/0187: training loss 0.253; learning rate 0.000385
Epoch 7 iteration 0106/0187: training loss 0.254; learning rate 0.000385
Epoch 7 iteration 0107/0187: training loss 0.253; learning rate 0.000384
Epoch 7 iteration 0108/0187: training loss 0.253; learning rate 0.000384
Epoch 7 iteration 0109/0187: training loss 0.254; learning rate 0.000384
Epoch 7 iteration 0110/0187: training loss 0.254; learning rate 0.000384
Epoch 7 iteration 0111/0187: training loss 0.254; learning rate 0.000384
Epoch 7 iteration 0112/0187: training loss 0.254; learning rate 0.000384
Epoch 7 iteration 0113/0187: training loss 0.254; learning rate 0.000384
Epoch 7 iteration 0114/0187: training loss 0.255; learning rate 0.000384
Epoch 7 iteration 0115/0187: training loss 0.255; learning rate 0.000384
Epoch 7 iteration 0116/0187: training loss 0.255; learning rate 0.000384
Epoch 7 iteration 0117/0187: training loss 0.255; learning rate 0.000384
Epoch 7 iteration 0118/0187: training loss 0.255; learning rate 0.000384
Epoch 7 iteration 0119/0187: training loss 0.256; learning rate 0.000383
Epoch 7 iteration 0120/0187: training loss 0.256; learning rate 0.000383
Epoch 7 iteration 0121/0187: training loss 0.256; learning rate 0.000383
Epoch 7 iteration 0122/0187: training loss 0.255; learning rate 0.000383
Epoch 7 iteration 0123/0187: training loss 0.255; learning rate 0.000383
Epoch 7 iteration 0124/0187: training loss 0.255; learning rate 0.000383
Epoch 7 iteration 0125/0187: training loss 0.255; learning rate 0.000383
Epoch 7 iteration 0126/0187: training loss 0.254; learning rate 0.000383
Epoch 7 iteration 0127/0187: training loss 0.254; learning rate 0.000383
Epoch 7 iteration 0128/0187: training loss 0.254; learning rate 0.000383
Epoch 7 iteration 0129/0187: training loss 0.254; learning rate 0.000383
Epoch 7 iteration 0130/0187: training loss 0.254; learning rate 0.000383
Epoch 7 iteration 0131/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0132/0187: training loss 0.254; learning rate 0.000382
Epoch 7 iteration 0133/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0134/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0135/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0136/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0137/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0138/0187: training loss 0.254; learning rate 0.000382
Epoch 7 iteration 0139/0187: training loss 0.254; learning rate 0.000382
Epoch 7 iteration 0140/0187: training loss 0.254; learning rate 0.000382
Epoch 7 iteration 0141/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0142/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0143/0187: training loss 0.253; learning rate 0.000382
Epoch 7 iteration 0144/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0145/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0146/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0147/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0148/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0149/0187: training loss 0.254; learning rate 0.000381
Epoch 7 iteration 0150/0187: training loss 0.255; learning rate 0.000381
Epoch 7 iteration 0151/0187: training loss 0.256; learning rate 0.000381
Epoch 7 iteration 0152/0187: training loss 0.255; learning rate 0.000381
Epoch 7 iteration 0153/0187: training loss 0.255; learning rate 0.000381
Epoch 7 iteration 0154/0187: training loss 0.255; learning rate 0.000381
Epoch 7 iteration 0155/0187: training loss 0.255; learning rate 0.000381
Epoch 7 iteration 0156/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0157/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0158/0187: training loss 0.254; learning rate 0.000380
Epoch 7 iteration 0159/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0160/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0161/0187: training loss 0.254; learning rate 0.000380
Epoch 7 iteration 0162/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0163/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0164/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0165/0187: training loss 0.254; learning rate 0.000380
Epoch 7 iteration 0166/0187: training loss 0.254; learning rate 0.000380
Epoch 7 iteration 0167/0187: training loss 0.255; learning rate 0.000380
Epoch 7 iteration 0168/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0169/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0170/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0171/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0172/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0173/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0174/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0175/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0176/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0177/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0178/0187: training loss 0.256; learning rate 0.000379
Epoch 7 iteration 0179/0187: training loss 0.255; learning rate 0.000379
Epoch 7 iteration 0180/0187: training loss 0.255; learning rate 0.000378
Epoch 7 iteration 0181/0187: training loss 0.255; learning rate 0.000378
Epoch 7 iteration 0182/0187: training loss 0.255; learning rate 0.000378
Epoch 7 iteration 0183/0187: training loss 0.255; learning rate 0.000378
Epoch 7 iteration 0184/0187: training loss 0.255; learning rate 0.000378
Epoch 7 iteration 0185/0187: training loss 0.254; learning rate 0.000378
Epoch 7 iteration 0186/0187: training loss 0.254; learning rate 0.000378
Epoch 7 iteration 0187/0187: training loss 0.254; learning rate 0.000378
Epoch 7 validation pixAcc: 0.332, mIoU: 0.204
Epoch 8 iteration 0001/0187: training loss 0.232; learning rate 0.000378
Epoch 8 iteration 0002/0187: training loss 0.239; learning rate 0.000378
Epoch 8 iteration 0003/0187: training loss 0.239; learning rate 0.000378
Epoch 8 iteration 0004/0187: training loss 0.271; learning rate 0.000377
Epoch 8 iteration 0005/0187: training loss 0.257; learning rate 0.000377
Epoch 8 iteration 0006/0187: training loss 0.261; learning rate 0.000377
Epoch 8 iteration 0007/0187: training loss 0.260; learning rate 0.000377
Epoch 8 iteration 0008/0187: training loss 0.256; learning rate 0.000377
Epoch 8 iteration 0009/0187: training loss 0.256; learning rate 0.000377
Epoch 8 iteration 0010/0187: training loss 0.260; learning rate 0.000377
Epoch 8 iteration 0011/0187: training loss 0.263; learning rate 0.000377
Epoch 8 iteration 0012/0187: training loss 0.258; learning rate 0.000377
Epoch 8 iteration 0013/0187: training loss 0.253; learning rate 0.000377
Epoch 8 iteration 0014/0187: training loss 0.260; learning rate 0.000377
Epoch 8 iteration 0015/0187: training loss 0.261; learning rate 0.000377
Epoch 8 iteration 0016/0187: training loss 0.261; learning rate 0.000376
Epoch 8 iteration 0017/0187: training loss 0.257; learning rate 0.000376
Epoch 8 iteration 0018/0187: training loss 0.252; learning rate 0.000376
Epoch 8 iteration 0019/0187: training loss 0.250; learning rate 0.000376
Epoch 8 iteration 0020/0187: training loss 0.250; learning rate 0.000376
Epoch 8 iteration 0021/0187: training loss 0.248; learning rate 0.000376
Epoch 8 iteration 0022/0187: training loss 0.249; learning rate 0.000376
Epoch 8 iteration 0023/0187: training loss 0.247; learning rate 0.000376
Epoch 8 iteration 0024/0187: training loss 0.245; learning rate 0.000376
Epoch 8 iteration 0025/0187: training loss 0.248; learning rate 0.000376
Epoch 8 iteration 0026/0187: training loss 0.246; learning rate 0.000376
Epoch 8 iteration 0027/0187: training loss 0.244; learning rate 0.000376
Epoch 8 iteration 0028/0187: training loss 0.242; learning rate 0.000375
Epoch 8 iteration 0029/0187: training loss 0.244; learning rate 0.000375
Epoch 8 iteration 0030/0187: training loss 0.243; learning rate 0.000375
Epoch 8 iteration 0031/0187: training loss 0.244; learning rate 0.000375
Epoch 8 iteration 0032/0187: training loss 0.242; learning rate 0.000375
Epoch 8 iteration 0033/0187: training loss 0.242; learning rate 0.000375
Epoch 8 iteration 0034/0187: training loss 0.240; learning rate 0.000375
Epoch 8 iteration 0035/0187: training loss 0.242; learning rate 0.000375
Epoch 8 iteration 0036/0187: training loss 0.241; learning rate 0.000375
Epoch 8 iteration 0037/0187: training loss 0.243; learning rate 0.000375
Epoch 8 iteration 0038/0187: training loss 0.247; learning rate 0.000375
Epoch 8 iteration 0039/0187: training loss 0.250; learning rate 0.000375
Epoch 8 iteration 0040/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0041/0187: training loss 0.251; learning rate 0.000374
Epoch 8 iteration 0042/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0043/0187: training loss 0.249; learning rate 0.000374
Epoch 8 iteration 0044/0187: training loss 0.249; learning rate 0.000374
Epoch 8 iteration 0045/0187: training loss 0.249; learning rate 0.000374
Epoch 8 iteration 0046/0187: training loss 0.248; learning rate 0.000374
Epoch 8 iteration 0047/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0048/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0049/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0050/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0051/0187: training loss 0.250; learning rate 0.000374
Epoch 8 iteration 0052/0187: training loss 0.249; learning rate 0.000373
Epoch 8 iteration 0053/0187: training loss 0.248; learning rate 0.000373
Epoch 8 iteration 0054/0187: training loss 0.249; learning rate 0.000373
Epoch 8 iteration 0055/0187: training loss 0.249; learning rate 0.000373
Epoch 8 iteration 0056/0187: training loss 0.249; learning rate 0.000373
Epoch 8 iteration 0057/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0058/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0059/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0060/0187: training loss 0.249; learning rate 0.000373
Epoch 8 iteration 0061/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0062/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0063/0187: training loss 0.250; learning rate 0.000373
Epoch 8 iteration 0064/0187: training loss 0.250; learning rate 0.000372
Epoch 8 iteration 0065/0187: training loss 0.250; learning rate 0.000372
Epoch 8 iteration 0066/0187: training loss 0.249; learning rate 0.000372
Epoch 8 iteration 0067/0187: training loss 0.249; learning rate 0.000372
Epoch 8 iteration 0068/0187: training loss 0.249; learning rate 0.000372
Epoch 8 iteration 0069/0187: training loss 0.248; learning rate 0.000372
Epoch 8 iteration 0070/0187: training loss 0.249; learning rate 0.000372
Epoch 8 iteration 0071/0187: training loss 0.249; learning rate 0.000372
Epoch 8 iteration 0072/0187: training loss 0.250; learning rate 0.000372
Epoch 8 iteration 0073/0187: training loss 0.251; learning rate 0.000372
Epoch 8 iteration 0074/0187: training loss 0.251; learning rate 0.000372
Epoch 8 iteration 0075/0187: training loss 0.252; learning rate 0.000372
Epoch 8 iteration 0076/0187: training loss 0.251; learning rate 0.000371
Epoch 8 iteration 0077/0187: training loss 0.251; learning rate 0.000371
Epoch 8 iteration 0078/0187: training loss 0.250; learning rate 0.000371
Epoch 8 iteration 0079/0187: training loss 0.250; learning rate 0.000371
Epoch 8 iteration 0080/0187: training loss 0.250; learning rate 0.000371
Epoch 8 iteration 0081/0187: training loss 0.250; learning rate 0.000371
Epoch 8 iteration 0082/0187: training loss 0.249; learning rate 0.000371
Epoch 8 iteration 0083/0187: training loss 0.249; learning rate 0.000371
Epoch 8 iteration 0084/0187: training loss 0.248; learning rate 0.000371
Epoch 8 iteration 0085/0187: training loss 0.247; learning rate 0.000371
Epoch 8 iteration 0086/0187: training loss 0.248; learning rate 0.000371
Epoch 8 iteration 0087/0187: training loss 0.249; learning rate 0.000371
Epoch 8 iteration 0088/0187: training loss 0.249; learning rate 0.000370
Epoch 8 iteration 0089/0187: training loss 0.248; learning rate 0.000370
Epoch 8 iteration 0090/0187: training loss 0.249; learning rate 0.000370
Epoch 8 iteration 0091/0188: training loss 0.249; learning rate 0.000370
Epoch 8 iteration 0092/0188: training loss 0.249; learning rate 0.000370
Epoch 8 iteration 0093/0188: training loss 0.249; learning rate 0.000370
Epoch 8 iteration 0094/0188: training loss 0.248; learning rate 0.000370
Epoch 8 iteration 0095/0188: training loss 0.248; learning rate 0.000370
Epoch 8 iteration 0096/0188: training loss 0.247; learning rate 0.000370
Epoch 8 iteration 0097/0188: training loss 0.247; learning rate 0.000370
Epoch 8 iteration 0098/0188: training loss 0.248; learning rate 0.000370
Epoch 8 iteration 0099/0188: training loss 0.248; learning rate 0.000370
Epoch 8 iteration 0100/0188: training loss 0.248; learning rate 0.000369
Epoch 8 iteration 0101/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0102/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0103/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0104/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0105/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0106/0188: training loss 0.248; learning rate 0.000369
Epoch 8 iteration 0107/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0108/0188: training loss 0.249; learning rate 0.000369
Epoch 8 iteration 0109/0188: training loss 0.248; learning rate 0.000369
Epoch 8 iteration 0110/0188: training loss 0.248; learning rate 0.000369
Epoch 8 iteration 0111/0188: training loss 0.247; learning rate 0.000369
Epoch 8 iteration 0112/0188: training loss 0.247; learning rate 0.000368
Epoch 8 iteration 0113/0188: training loss 0.247; learning rate 0.000368
Epoch 8 iteration 0114/0188: training loss 0.247; learning rate 0.000368
Epoch 8 iteration 0115/0188: training loss 0.247; learning rate 0.000368
Epoch 8 iteration 0116/0188: training loss 0.247; learning rate 0.000368
Epoch 8 iteration 0117/0188: training loss 0.248; learning rate 0.000368
Epoch 8 iteration 0118/0188: training loss 0.248; learning rate 0.000368
Epoch 8 iteration 0119/0188: training loss 0.248; learning rate 0.000368
Epoch 8 iteration 0120/0188: training loss 0.249; learning rate 0.000368
Epoch 8 iteration 0121/0188: training loss 0.249; learning rate 0.000368
Epoch 8 iteration 0122/0188: training loss 0.250; learning rate 0.000368
Epoch 8 iteration 0123/0188: training loss 0.250; learning rate 0.000368
Epoch 8 iteration 0124/0188: training loss 0.250; learning rate 0.000368
Epoch 8 iteration 0125/0188: training loss 0.250; learning rate 0.000367
Epoch 8 iteration 0126/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0127/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0128/0188: training loss 0.252; learning rate 0.000367
Epoch 8 iteration 0129/0188: training loss 0.252; learning rate 0.000367
Epoch 8 iteration 0130/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0131/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0132/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0133/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0134/0188: training loss 0.251; learning rate 0.000367
Epoch 8 iteration 0135/0188: training loss 0.250; learning rate 0.000367
Epoch 8 iteration 0136/0188: training loss 0.250; learning rate 0.000367
Epoch 8 iteration 0137/0188: training loss 0.250; learning rate 0.000366
Epoch 8 iteration 0138/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0139/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0140/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0141/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0142/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0143/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0144/0188: training loss 0.249; learning rate 0.000366
Epoch 8 iteration 0145/0188: training loss 0.248; learning rate 0.000366
Epoch 8 iteration 0146/0188: training loss 0.248; learning rate 0.000366
Epoch 8 iteration 0147/0188: training loss 0.248; learning rate 0.000366
Epoch 8 iteration 0148/0188: training loss 0.248; learning rate 0.000366
Epoch 8 iteration 0149/0188: training loss 0.249; learning rate 0.000365
Epoch 8 iteration 0150/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0151/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0152/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0153/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0154/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0155/0188: training loss 0.247; learning rate 0.000365
Epoch 8 iteration 0156/0188: training loss 0.247; learning rate 0.000365
Epoch 8 iteration 0157/0188: training loss 0.248; learning rate 0.000365
Epoch 8 iteration 0158/0188: training loss 0.247; learning rate 0.000365
Epoch 8 iteration 0159/0188: training loss 0.247; learning rate 0.000365
Epoch 8 iteration 0160/0188: training loss 0.247; learning rate 0.000365
Epoch 8 iteration 0161/0188: training loss 0.247; learning rate 0.000364
Epoch 8 iteration 0162/0188: training loss 0.247; learning rate 0.000364
Epoch 8 iteration 0163/0188: training loss 0.247; learning rate 0.000364
Epoch 8 iteration 0164/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0165/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0166/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0167/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0168/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0169/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0170/0188: training loss 0.247; learning rate 0.000364
Epoch 8 iteration 0171/0188: training loss 0.248; learning rate 0.000364
Epoch 8 iteration 0172/0188: training loss 0.247; learning rate 0.000364
Epoch 8 iteration 0173/0188: training loss 0.247; learning rate 0.000363
Epoch 8 iteration 0174/0188: training loss 0.247; learning rate 0.000363
Epoch 8 iteration 0175/0188: training loss 0.248; learning rate 0.000363
Epoch 8 iteration 0176/0188: training loss 0.248; learning rate 0.000363
Epoch 8 iteration 0177/0188: training loss 0.248; learning rate 0.000363
Epoch 8 iteration 0178/0188: training loss 0.248; learning rate 0.000363
Epoch 8 iteration 0179/0188: training loss 0.249; learning rate 0.000363
Epoch 8 iteration 0180/0188: training loss 0.249; learning rate 0.000363
Epoch 8 iteration 0181/0188: training loss 0.249; learning rate 0.000363
Epoch 8 iteration 0182/0188: training loss 0.248; learning rate 0.000363
Epoch 8 iteration 0183/0188: training loss 0.249; learning rate 0.000363
Epoch 8 iteration 0184/0188: training loss 0.249; learning rate 0.000363
Epoch 8 iteration 0185/0188: training loss 0.249; learning rate 0.000362
Epoch 8 iteration 0186/0188: training loss 0.249; learning rate 0.000362
Epoch 8 validation pixAcc: 0.332, mIoU: 0.204
Epoch 9 iteration 0001/0187: training loss 0.238; learning rate 0.000362
Epoch 9 iteration 0002/0187: training loss 0.267; learning rate 0.000362
Epoch 9 iteration 0003/0187: training loss 0.265; learning rate 0.000362
Epoch 9 iteration 0004/0187: training loss 0.258; learning rate 0.000362
Epoch 9 iteration 0005/0187: training loss 0.245; learning rate 0.000362
Epoch 9 iteration 0006/0187: training loss 0.250; learning rate 0.000362
Epoch 9 iteration 0007/0187: training loss 0.258; learning rate 0.000362
Epoch 9 iteration 0008/0187: training loss 0.257; learning rate 0.000362
Epoch 9 iteration 0009/0187: training loss 0.251; learning rate 0.000362
Epoch 9 iteration 0010/0187: training loss 0.255; learning rate 0.000361
Epoch 9 iteration 0011/0187: training loss 0.255; learning rate 0.000361
Epoch 9 iteration 0012/0187: training loss 0.258; learning rate 0.000361
Epoch 9 iteration 0013/0187: training loss 0.255; learning rate 0.000361
Epoch 9 iteration 0014/0187: training loss 0.256; learning rate 0.000361
Epoch 9 iteration 0015/0187: training loss 0.256; learning rate 0.000361
Epoch 9 iteration 0016/0187: training loss 0.255; learning rate 0.000361
Epoch 9 iteration 0017/0187: training loss 0.251; learning rate 0.000361
Epoch 9 iteration 0018/0187: training loss 0.248; learning rate 0.000361
Epoch 9 iteration 0019/0187: training loss 0.246; learning rate 0.000361
Epoch 9 iteration 0020/0187: training loss 0.248; learning rate 0.000361
Epoch 9 iteration 0021/0187: training loss 0.249; learning rate 0.000361
Epoch 9 iteration 0022/0187: training loss 0.247; learning rate 0.000360
Epoch 9 iteration 0023/0187: training loss 0.245; learning rate 0.000360
Epoch 9 iteration 0024/0187: training loss 0.247; learning rate 0.000360
Epoch 9 iteration 0025/0187: training loss 0.246; learning rate 0.000360
Epoch 9 iteration 0026/0187: training loss 0.246; learning rate 0.000360
Epoch 9 iteration 0027/0187: training loss 0.248; learning rate 0.000360
Epoch 9 iteration 0028/0187: training loss 0.249; learning rate 0.000360
Epoch 9 iteration 0029/0187: training loss 0.248; learning rate 0.000360
Epoch 9 iteration 0030/0187: training loss 0.247; learning rate 0.000360
Epoch 9 iteration 0031/0187: training loss 0.250; learning rate 0.000360
Epoch 9 iteration 0032/0187: training loss 0.249; learning rate 0.000360
Epoch 9 iteration 0033/0187: training loss 0.251; learning rate 0.000360
Epoch 9 iteration 0034/0187: training loss 0.250; learning rate 0.000359
Epoch 9 iteration 0035/0187: training loss 0.251; learning rate 0.000359
Epoch 9 iteration 0036/0187: training loss 0.251; learning rate 0.000359
Epoch 9 iteration 0037/0187: training loss 0.253; learning rate 0.000359
Epoch 9 iteration 0038/0187: training loss 0.255; learning rate 0.000359
Epoch 9 iteration 0039/0187: training loss 0.256; learning rate 0.000359
Epoch 9 iteration 0040/0187: training loss 0.255; learning rate 0.000359
Epoch 9 iteration 0041/0187: training loss 0.257; learning rate 0.000359
Epoch 9 iteration 0042/0187: training loss 0.257; learning rate 0.000359
Epoch 9 iteration 0043/0187: training loss 0.258; learning rate 0.000359
Epoch 9 iteration 0044/0187: training loss 0.259; learning rate 0.000359
Epoch 9 iteration 0045/0187: training loss 0.259; learning rate 0.000359
Epoch 9 iteration 0046/0187: training loss 0.261; learning rate 0.000358
Epoch 9 iteration 0047/0187: training loss 0.260; learning rate 0.000358
Epoch 9 iteration 0048/0187: training loss 0.262; learning rate 0.000358
Epoch 9 iteration 0049/0187: training loss 0.262; learning rate 0.000358
Epoch 9 iteration 0050/0187: training loss 0.261; learning rate 0.000358
Epoch 9 iteration 0051/0187: training loss 0.260; learning rate 0.000358
Epoch 9 iteration 0052/0187: training loss 0.261; learning rate 0.000358
Epoch 9 iteration 0053/0187: training loss 0.260; learning rate 0.000358
Epoch 9 iteration 0054/0187: training loss 0.259; learning rate 0.000358
Epoch 9 iteration 0055/0187: training loss 0.259; learning rate 0.000358
Epoch 9 iteration 0056/0187: training loss 0.258; learning rate 0.000358
Epoch 9 iteration 0057/0187: training loss 0.258; learning rate 0.000358
Epoch 9 iteration 0058/0187: training loss 0.258; learning rate 0.000357
Epoch 9 iteration 0059/0187: training loss 0.258; learning rate 0.000357
Epoch 9 iteration 0060/0187: training loss 0.258; learning rate 0.000357
Epoch 9 iteration 0061/0187: training loss 0.258; learning rate 0.000357
Epoch 9 iteration 0062/0187: training loss 0.259; learning rate 0.000357
Epoch 9 iteration 0063/0187: training loss 0.259; learning rate 0.000357
Epoch 9 iteration 0064/0187: training loss 0.262; learning rate 0.000357
Epoch 9 iteration 0065/0187: training loss 0.262; learning rate 0.000357
Epoch 9 iteration 0066/0187: training loss 0.261; learning rate 0.000357
Epoch 9 iteration 0067/0187: training loss 0.262; learning rate 0.000357
Epoch 9 iteration 0068/0187: training loss 0.262; learning rate 0.000357
Epoch 9 iteration 0069/0187: training loss 0.262; learning rate 0.000357
Epoch 9 iteration 0070/0187: training loss 0.262; learning rate 0.000356
Epoch 9 iteration 0071/0187: training loss 0.261; learning rate 0.000356
Epoch 9 iteration 0072/0187: training loss 0.262; learning rate 0.000356
Epoch 9 iteration 0073/0187: training loss 0.262; learning rate 0.000356
Epoch 9 iteration 0074/0187: training loss 0.261; learning rate 0.000356
Epoch 9 iteration 0075/0187: training loss 0.261; learning rate 0.000356
Epoch 9 iteration 0076/0187: training loss 0.261; learning rate 0.000356
Epoch 9 iteration 0077/0187: training loss 0.260; learning rate 0.000356
Epoch 9 iteration 0078/0187: training loss 0.259; learning rate 0.000356
Epoch 9 iteration 0079/0187: training loss 0.260; learning rate 0.000356
Epoch 9 iteration 0080/0187: training loss 0.259; learning rate 0.000356
Epoch 9 iteration 0081/0187: training loss 0.261; learning rate 0.000356
Epoch 9 iteration 0082/0187: training loss 0.261; learning rate 0.000355
Epoch 9 iteration 0083/0187: training loss 0.260; learning rate 0.000355
Epoch 9 iteration 0084/0187: training loss 0.258; learning rate 0.000355
Epoch 9 iteration 0085/0187: training loss 0.258; learning rate 0.000355
Epoch 9 iteration 0086/0187: training loss 0.259; learning rate 0.000355
Epoch 9 iteration 0087/0187: training loss 0.258; learning rate 0.000355
Epoch 9 iteration 0088/0187: training loss 0.257; learning rate 0.000355
Epoch 9 iteration 0089/0187: training loss 0.256; learning rate 0.000355
Epoch 9 iteration 0090/0187: training loss 0.256; learning rate 0.000355
Epoch 9 iteration 0091/0187: training loss 0.257; learning rate 0.000355
Epoch 9 iteration 0092/0187: training loss 0.258; learning rate 0.000355
Epoch 9 iteration 0093/0187: training loss 0.258; learning rate 0.000355
Epoch 9 iteration 0094/0187: training loss 0.258; learning rate 0.000354
Epoch 9 iteration 0095/0187: training loss 0.258; learning rate 0.000354
Epoch 9 iteration 0096/0187: training loss 0.258; learning rate 0.000354
Epoch 9 iteration 0097/0187: training loss 0.258; learning rate 0.000354
Epoch 9 iteration 0098/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0099/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0100/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0101/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0102/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0103/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0104/0187: training loss 0.257; learning rate 0.000354
Epoch 9 iteration 0105/0187: training loss 0.256; learning rate 0.000354
Epoch 9 iteration 0106/0187: training loss 0.256; learning rate 0.000353
Epoch 9 iteration 0107/0187: training loss 0.256; learning rate 0.000353
Epoch 9 iteration 0108/0187: training loss 0.256; learning rate 0.000353
Epoch 9 iteration 0109/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0110/0187: training loss 0.254; learning rate 0.000353
Epoch 9 iteration 0111/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0112/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0113/0187: training loss 0.254; learning rate 0.000353
Epoch 9 iteration 0114/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0115/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0116/0187: training loss 0.254; learning rate 0.000353
Epoch 9 iteration 0117/0187: training loss 0.255; learning rate 0.000353
Epoch 9 iteration 0118/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0119/0187: training loss 0.254; learning rate 0.000352
Epoch 9 iteration 0120/0187: training loss 0.254; learning rate 0.000352
Epoch 9 iteration 0121/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0122/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0123/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0124/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0125/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0126/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0127/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0128/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0129/0187: training loss 0.255; learning rate 0.000352
Epoch 9 iteration 0130/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0131/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0132/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0133/0187: training loss 0.257; learning rate 0.000351
Epoch 9 iteration 0134/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0135/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0136/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0137/0187: training loss 0.257; learning rate 0.000351
Epoch 9 iteration 0138/0187: training loss 0.257; learning rate 0.000351
Epoch 9 iteration 0139/0187: training loss 0.257; learning rate 0.000351
Epoch 9 iteration 0140/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0141/0187: training loss 0.256; learning rate 0.000351
Epoch 9 iteration 0142/0187: training loss 0.257; learning rate 0.000350
Epoch 9 iteration 0143/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0144/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0145/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0146/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0147/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0148/0187: training loss 0.255; learning rate 0.000350
Epoch 9 iteration 0149/0187: training loss 0.255; learning rate 0.000350
Epoch 9 iteration 0150/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0151/0187: training loss 0.256; learning rate 0.000350
Epoch 9 iteration 0152/0187: training loss 0.257; learning rate 0.000350
Epoch 9 iteration 0153/0187: training loss 0.257; learning rate 0.000350
Epoch 9 iteration 0154/0187: training loss 0.257; learning rate 0.000349
Epoch 9 iteration 0155/0187: training loss 0.257; learning rate 0.000349
Epoch 9 iteration 0156/0187: training loss 0.257; learning rate 0.000349
Epoch 9 iteration 0157/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0158/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0159/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0160/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0161/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0162/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0163/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0164/0187: training loss 0.256; learning rate 0.000349
Epoch 9 iteration 0165/0187: training loss 0.255; learning rate 0.000349
Epoch 9 iteration 0166/0187: training loss 0.255; learning rate 0.000348
Epoch 9 iteration 0167/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0168/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0169/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0170/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0171/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0172/0187: training loss 0.256; learning rate 0.000348
Epoch 9 iteration 0173/0187: training loss 0.255; learning rate 0.000348
Epoch 9 iteration 0174/0187: training loss 0.255; learning rate 0.000348
Epoch 9 iteration 0175/0187: training loss 0.255; learning rate 0.000348
Epoch 9 iteration 0176/0187: training loss 0.255; learning rate 0.000348
Epoch 9 iteration 0177/0187: training loss 0.254; learning rate 0.000348
Epoch 9 iteration 0178/0187: training loss 0.254; learning rate 0.000347
Epoch 9 iteration 0179/0187: training loss 0.254; learning rate 0.000347
Epoch 9 iteration 0180/0187: training loss 0.254; learning rate 0.000347
Epoch 9 iteration 0181/0187: training loss 0.254; learning rate 0.000347
Epoch 9 iteration 0182/0187: training loss 0.253; learning rate 0.000347
Epoch 9 iteration 0183/0187: training loss 0.254; learning rate 0.000347
Epoch 9 iteration 0184/0187: training loss 0.253; learning rate 0.000347
Epoch 9 iteration 0185/0187: training loss 0.253; learning rate 0.000347
Epoch 9 iteration 0186/0187: training loss 0.253; learning rate 0.000347
Epoch 9 iteration 0187/0187: training loss 0.253; learning rate 0.000347
Epoch 9 validation pixAcc: 0.333, mIoU: 0.202
Epoch 10 iteration 0001/0187: training loss 0.239; learning rate 0.000347
Epoch 10 iteration 0002/0187: training loss 0.234; learning rate 0.000346
Epoch 10 iteration 0003/0187: training loss 0.227; learning rate 0.000346
Epoch 10 iteration 0004/0187: training loss 0.221; learning rate 0.000346
Epoch 10 iteration 0005/0187: training loss 0.225; learning rate 0.000346
Epoch 10 iteration 0006/0187: training loss 0.229; learning rate 0.000346
Epoch 10 iteration 0007/0187: training loss 0.228; learning rate 0.000346
Epoch 10 iteration 0008/0187: training loss 0.240; learning rate 0.000346
Epoch 10 iteration 0009/0187: training loss 0.239; learning rate 0.000346
Epoch 10 iteration 0010/0187: training loss 0.238; learning rate 0.000346
Epoch 10 iteration 0011/0187: training loss 0.233; learning rate 0.000346
Epoch 10 iteration 0012/0187: training loss 0.234; learning rate 0.000346
Epoch 10 iteration 0013/0187: training loss 0.233; learning rate 0.000346
Epoch 10 iteration 0014/0187: training loss 0.231; learning rate 0.000345
Epoch 10 iteration 0015/0187: training loss 0.232; learning rate 0.000345
Epoch 10 iteration 0016/0187: training loss 0.229; learning rate 0.000345
Epoch 10 iteration 0017/0187: training loss 0.228; learning rate 0.000345
Epoch 10 iteration 0018/0187: training loss 0.230; learning rate 0.000345
Epoch 10 iteration 0019/0187: training loss 0.231; learning rate 0.000345
Epoch 10 iteration 0020/0187: training loss 0.229; learning rate 0.000345
Epoch 10 iteration 0021/0187: training loss 0.227; learning rate 0.000345
Epoch 10 iteration 0022/0187: training loss 0.224; learning rate 0.000345
Epoch 10 iteration 0023/0187: training loss 0.226; learning rate 0.000345
Epoch 10 iteration 0024/0187: training loss 0.228; learning rate 0.000345
Epoch 10 iteration 0025/0187: training loss 0.232; learning rate 0.000345
Epoch 10 iteration 0026/0187: training loss 0.232; learning rate 0.000344
Epoch 10 iteration 0027/0187: training loss 0.235; learning rate 0.000344
Epoch 10 iteration 0028/0187: training loss 0.236; learning rate 0.000344
Epoch 10 iteration 0029/0187: training loss 0.237; learning rate 0.000344
Epoch 10 iteration 0030/0187: training loss 0.243; learning rate 0.000344
Epoch 10 iteration 0031/0187: training loss 0.243; learning rate 0.000344
Epoch 10 iteration 0032/0187: training loss 0.241; learning rate 0.000344
Epoch 10 iteration 0033/0187: training loss 0.242; learning rate 0.000344
Epoch 10 iteration 0034/0187: training loss 0.242; learning rate 0.000344
Epoch 10 iteration 0035/0187: training loss 0.242; learning rate 0.000344
Epoch 10 iteration 0036/0187: training loss 0.243; learning rate 0.000344
Epoch 10 iteration 0037/0187: training loss 0.244; learning rate 0.000344
Epoch 10 iteration 0038/0187: training loss 0.244; learning rate 0.000343
Epoch 10 iteration 0039/0187: training loss 0.243; learning rate 0.000343
Epoch 10 iteration 0040/0187: training loss 0.246; learning rate 0.000343
Epoch 10 iteration 0041/0187: training loss 0.247; learning rate 0.000343
Epoch 10 iteration 0042/0187: training loss 0.247; learning rate 0.000343
Epoch 10 iteration 0043/0187: training loss 0.247; learning rate 0.000343
Epoch 10 iteration 0044/0187: training loss 0.246; learning rate 0.000343
Epoch 10 iteration 0045/0187: training loss 0.246; learning rate 0.000343
Epoch 10 iteration 0046/0187: training loss 0.247; learning rate 0.000343
Epoch 10 iteration 0047/0187: training loss 0.245; learning rate 0.000343
Epoch 10 iteration 0048/0187: training loss 0.244; learning rate 0.000343
Epoch 10 iteration 0049/0187: training loss 0.244; learning rate 0.000343
Epoch 10 iteration 0050/0187: training loss 0.245; learning rate 0.000342
Epoch 10 iteration 0051/0187: training loss 0.245; learning rate 0.000342
Epoch 10 iteration 0052/0187: training loss 0.244; learning rate 0.000342
Epoch 10 iteration 0053/0187: training loss 0.243; learning rate 0.000342
Epoch 10 iteration 0054/0187: training loss 0.243; learning rate 0.000342
Epoch 10 iteration 0055/0187: training loss 0.244; learning rate 0.000342
Epoch 10 iteration 0056/0187: training loss 0.243; learning rate 0.000342
Epoch 10 iteration 0057/0187: training loss 0.244; learning rate 0.000342
Epoch 10 iteration 0058/0187: training loss 0.247; learning rate 0.000342
Epoch 10 iteration 0059/0187: training loss 0.249; learning rate 0.000342
Epoch 10 iteration 0060/0187: training loss 0.249; learning rate 0.000342
Epoch 10 iteration 0061/0187: training loss 0.248; learning rate 0.000341
Epoch 10 iteration 0062/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0063/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0064/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0065/0187: training loss 0.251; learning rate 0.000341
Epoch 10 iteration 0066/0187: training loss 0.250; learning rate 0.000341
Epoch 10 iteration 0067/0187: training loss 0.251; learning rate 0.000341
Epoch 10 iteration 0068/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0069/0187: training loss 0.248; learning rate 0.000341
Epoch 10 iteration 0070/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0071/0187: training loss 0.249; learning rate 0.000341
Epoch 10 iteration 0072/0187: training loss 0.251; learning rate 0.000341
Epoch 10 iteration 0073/0187: training loss 0.251; learning rate 0.000340
Epoch 10 iteration 0074/0187: training loss 0.252; learning rate 0.000340
Epoch 10 iteration 0075/0187: training loss 0.251; learning rate 0.000340
Epoch 10 iteration 0076/0187: training loss 0.252; learning rate 0.000340
Epoch 10 iteration 0077/0187: training loss 0.250; learning rate 0.000340
Epoch 10 iteration 0078/0187: training loss 0.251; learning rate 0.000340
Epoch 10 iteration 0079/0187: training loss 0.253; learning rate 0.000340
Epoch 10 iteration 0080/0187: training loss 0.252; learning rate 0.000340
Epoch 10 iteration 0081/0187: training loss 0.252; learning rate 0.000340
Epoch 10 iteration 0082/0187: training loss 0.252; learning rate 0.000340
Epoch 10 iteration 0083/0187: training loss 0.253; learning rate 0.000340
Epoch 10 iteration 0084/0187: training loss 0.253; learning rate 0.000340
Epoch 10 iteration 0085/0187: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0086/0187: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0087/0187: training loss 0.254; learning rate 0.000339
Epoch 10 iteration 0088/0187: training loss 0.254; learning rate 0.000339
Epoch 10 iteration 0089/0187: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0090/0187: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0091/0188: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0092/0188: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0093/0188: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0094/0188: training loss 0.254; learning rate 0.000339
Epoch 10 iteration 0095/0188: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0096/0188: training loss 0.253; learning rate 0.000339
Epoch 10 iteration 0097/0188: training loss 0.253; learning rate 0.000338
Epoch 10 iteration 0098/0188: training loss 0.253; learning rate 0.000338
Epoch 10 iteration 0099/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0100/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0101/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0102/0188: training loss 0.255; learning rate 0.000338
Epoch 10 iteration 0103/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0104/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0105/0188: training loss 0.254; learning rate 0.000338
Epoch 10 iteration 0106/0188: training loss 0.253; learning rate 0.000338
Epoch 10 iteration 0107/0188: training loss 0.253; learning rate 0.000338
Epoch 10 iteration 0108/0188: training loss 0.253; learning rate 0.000338
Epoch 10 iteration 0109/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0110/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0111/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0112/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0113/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0114/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0115/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0116/0188: training loss 0.254; learning rate 0.000337
Epoch 10 iteration 0117/0188: training loss 0.254; learning rate 0.000337
Epoch 10 iteration 0118/0188: training loss 0.254; learning rate 0.000337
Epoch 10 iteration 0119/0188: training loss 0.254; learning rate 0.000337
Epoch 10 iteration 0120/0188: training loss 0.253; learning rate 0.000337
Epoch 10 iteration 0121/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0122/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0123/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0124/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0125/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0126/0188: training loss 0.252; learning rate 0.000336
Epoch 10 iteration 0127/0188: training loss 0.252; learning rate 0.000336
Epoch 10 iteration 0128/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0129/0188: training loss 0.253; learning rate 0.000336
Epoch 10 iteration 0130/0188: training loss 0.252; learning rate 0.000336
Epoch 10 iteration 0131/0188: training loss 0.252; learning rate 0.000336
Epoch 10 iteration 0132/0188: training loss 0.252; learning rate 0.000336
Epoch 10 iteration 0133/0188: training loss 0.251; learning rate 0.000335
Epoch 10 iteration 0134/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0135/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0136/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0137/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0138/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0139/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0140/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0141/0188: training loss 0.253; learning rate 0.000335
Epoch 10 iteration 0142/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0143/0188: training loss 0.252; learning rate 0.000335
Epoch 10 iteration 0144/0188: training loss 0.253; learning rate 0.000335
Epoch 10 iteration 0145/0188: training loss 0.252; learning rate 0.000334
Epoch 10 iteration 0146/0188: training loss 0.253; learning rate 0.000334
Epoch 10 iteration 0147/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0148/0188: training loss 0.253; learning rate 0.000334
Epoch 10 iteration 0149/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0150/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0151/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0152/0188: training loss 0.255; learning rate 0.000334
Epoch 10 iteration 0153/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0154/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0155/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0156/0188: training loss 0.254; learning rate 0.000334
Epoch 10 iteration 0157/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0158/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0159/0188: training loss 0.253; learning rate 0.000333
Epoch 10 iteration 0160/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0161/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0162/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0163/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0164/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0165/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0166/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0167/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0168/0188: training loss 0.254; learning rate 0.000333
Epoch 10 iteration 0169/0188: training loss 0.253; learning rate 0.000332
Epoch 10 iteration 0170/0188: training loss 0.253; learning rate 0.000332
Epoch 10 iteration 0171/0188: training loss 0.253; learning rate 0.000332
Epoch 10 iteration 0172/0188: training loss 0.253; learning rate 0.000332
Epoch 10 iteration 0173/0188: training loss 0.253; learning rate 0.000332
Epoch 10 iteration 0174/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0175/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0176/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0177/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0178/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0179/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0180/0188: training loss 0.254; learning rate 0.000332
Epoch 10 iteration 0181/0188: training loss 0.254; learning rate 0.000331
Epoch 10 iteration 0182/0188: training loss 0.254; learning rate 0.000331
Epoch 10 iteration 0183/0188: training loss 0.254; learning rate 0.000331
Epoch 10 iteration 0184/0188: training loss 0.254; learning rate 0.000331
Epoch 10 iteration 0185/0188: training loss 0.255; learning rate 0.000331
Epoch 10 iteration 0186/0188: training loss 0.254; learning rate 0.000331
Epoch 10 validation pixAcc: 0.334, mIoU: 0.203
Epoch 11 iteration 0001/0187: training loss 0.283; learning rate 0.000331
Epoch 11 iteration 0002/0187: training loss 0.289; learning rate 0.000331
Epoch 11 iteration 0003/0187: training loss 0.286; learning rate 0.000331
Epoch 11 iteration 0004/0187: training loss 0.287; learning rate 0.000331
Epoch 11 iteration 0005/0187: training loss 0.283; learning rate 0.000331
Epoch 11 iteration 0006/0187: training loss 0.285; learning rate 0.000330
Epoch 11 iteration 0007/0187: training loss 0.273; learning rate 0.000330
Epoch 11 iteration 0008/0187: training loss 0.267; learning rate 0.000330
Epoch 11 iteration 0009/0187: training loss 0.263; learning rate 0.000330
Epoch 11 iteration 0010/0187: training loss 0.261; learning rate 0.000330
Epoch 11 iteration 0011/0187: training loss 0.253; learning rate 0.000330
Epoch 11 iteration 0012/0187: training loss 0.251; learning rate 0.000330
Epoch 11 iteration 0013/0187: training loss 0.256; learning rate 0.000330
Epoch 11 iteration 0014/0187: training loss 0.254; learning rate 0.000330
Epoch 11 iteration 0015/0187: training loss 0.250; learning rate 0.000330
Epoch 11 iteration 0016/0187: training loss 0.248; learning rate 0.000330
Epoch 11 iteration 0017/0187: training loss 0.249; learning rate 0.000330
Epoch 11 iteration 0018/0187: training loss 0.249; learning rate 0.000329
Epoch 11 iteration 0019/0187: training loss 0.249; learning rate 0.000329
Epoch 11 iteration 0020/0187: training loss 0.248; learning rate 0.000329
Epoch 11 iteration 0021/0187: training loss 0.247; learning rate 0.000329
Epoch 11 iteration 0022/0187: training loss 0.253; learning rate 0.000329
Epoch 11 iteration 0023/0187: training loss 0.253; learning rate 0.000329
Epoch 11 iteration 0024/0187: training loss 0.256; learning rate 0.000329
Epoch 11 iteration 0025/0187: training loss 0.259; learning rate 0.000329
Epoch 11 iteration 0026/0187: training loss 0.258; learning rate 0.000329
Epoch 11 iteration 0027/0187: training loss 0.258; learning rate 0.000329
Epoch 11 iteration 0028/0187: training loss 0.257; learning rate 0.000329
Epoch 11 iteration 0029/0187: training loss 0.256; learning rate 0.000328
Epoch 11 iteration 0030/0187: training loss 0.254; learning rate 0.000328
Epoch 11 iteration 0031/0187: training loss 0.252; learning rate 0.000328
Epoch 11 iteration 0032/0187: training loss 0.251; learning rate 0.000328
Epoch 11 iteration 0033/0187: training loss 0.251; learning rate 0.000328
Epoch 11 iteration 0034/0187: training loss 0.251; learning rate 0.000328
Epoch 11 iteration 0035/0187: training loss 0.252; learning rate 0.000328
Epoch 11 iteration 0036/0187: training loss 0.252; learning rate 0.000328
Epoch 11 iteration 0037/0187: training loss 0.252; learning rate 0.000328
Epoch 11 iteration 0038/0187: training loss 0.253; learning rate 0.000328
Epoch 11 iteration 0039/0187: training loss 0.253; learning rate 0.000328
Epoch 11 iteration 0040/0187: training loss 0.253; learning rate 0.000328
Epoch 11 iteration 0041/0187: training loss 0.251; learning rate 0.000327
Epoch 11 iteration 0042/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0043/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0044/0187: training loss 0.252; learning rate 0.000327
Epoch 11 iteration 0045/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0046/0187: training loss 0.255; learning rate 0.000327
Epoch 11 iteration 0047/0187: training loss 0.255; learning rate 0.000327
Epoch 11 iteration 0048/0187: training loss 0.254; learning rate 0.000327
Epoch 11 iteration 0049/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0050/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0051/0187: training loss 0.252; learning rate 0.000327
Epoch 11 iteration 0052/0187: training loss 0.253; learning rate 0.000327
Epoch 11 iteration 0053/0187: training loss 0.253; learning rate 0.000326
Epoch 11 iteration 0054/0187: training loss 0.254; learning rate 0.000326
Epoch 11 iteration 0055/0187: training loss 0.253; learning rate 0.000326
Epoch 11 iteration 0056/0187: training loss 0.253; learning rate 0.000326
Epoch 11 iteration 0057/0187: training loss 0.252; learning rate 0.000326
Epoch 11 iteration 0058/0187: training loss 0.251; learning rate 0.000326
Epoch 11 iteration 0059/0187: training loss 0.254; learning rate 0.000326
Epoch 11 iteration 0060/0187: training loss 0.255; learning rate 0.000326
Epoch 11 iteration 0061/0187: training loss 0.253; learning rate 0.000326
Epoch 11 iteration 0062/0187: training loss 0.253; learning rate 0.000326
Epoch 11 iteration 0063/0187: training loss 0.252; learning rate 0.000326
Epoch 11 iteration 0064/0187: training loss 0.252; learning rate 0.000326
Epoch 11 iteration 0065/0187: training loss 0.251; learning rate 0.000325
Epoch 11 iteration 0066/0187: training loss 0.250; learning rate 0.000325
Epoch 11 iteration 0067/0187: training loss 0.249; learning rate 0.000325
Epoch 11 iteration 0068/0187: training loss 0.250; learning rate 0.000325
Epoch 11 iteration 0069/0187: training loss 0.249; learning rate 0.000325
Epoch 11 iteration 0070/0187: training loss 0.250; learning rate 0.000325
Epoch 11 iteration 0071/0187: training loss 0.250; learning rate 0.000325
Epoch 11 iteration 0072/0187: training loss 0.251; learning rate 0.000325
Epoch 11 iteration 0073/0187: training loss 0.251; learning rate 0.000325
Epoch 11 iteration 0074/0187: training loss 0.251; learning rate 0.000325
Epoch 11 iteration 0075/0187: training loss 0.252; learning rate 0.000325
Epoch 11 iteration 0076/0187: training loss 0.253; learning rate 0.000325
Epoch 11 iteration 0077/0187: training loss 0.253; learning rate 0.000324
Epoch 11 iteration 0078/0187: training loss 0.252; learning rate 0.000324
Epoch 11 iteration 0079/0187: training loss 0.255; learning rate 0.000324
Epoch 11 iteration 0080/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0081/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0082/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0083/0187: training loss 0.253; learning rate 0.000324
Epoch 11 iteration 0084/0187: training loss 0.253; learning rate 0.000324
Epoch 11 iteration 0085/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0086/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0087/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0088/0187: training loss 0.254; learning rate 0.000324
Epoch 11 iteration 0089/0187: training loss 0.254; learning rate 0.000323
Epoch 11 iteration 0090/0187: training loss 0.253; learning rate 0.000323
Epoch 11 iteration 0091/0187: training loss 0.253; learning rate 0.000323
Epoch 11 iteration 0092/0187: training loss 0.253; learning rate 0.000323
Epoch 11 iteration 0093/0187: training loss 0.253; learning rate 0.000323
Epoch 11 iteration 0094/0187: training loss 0.253; learning rate 0.000323
Epoch 11 iteration 0095/0187: training loss 0.252; learning rate 0.000323
Epoch 11 iteration 0096/0187: training loss 0.252; learning rate 0.000323
Epoch 11 iteration 0097/0187: training loss 0.252; learning rate 0.000323
Epoch 11 iteration 0098/0187: training loss 0.251; learning rate 0.000323
Epoch 11 iteration 0099/0187: training loss 0.251; learning rate 0.000323
Epoch 11 iteration 0100/0187: training loss 0.250; learning rate 0.000323
Epoch 11 iteration 0101/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0102/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0103/0187: training loss 0.250; learning rate 0.000322
Epoch 11 iteration 0104/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0105/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0106/0187: training loss 0.252; learning rate 0.000322
Epoch 11 iteration 0107/0187: training loss 0.252; learning rate 0.000322
Epoch 11 iteration 0108/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0109/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0110/0187: training loss 0.251; learning rate 0.000322
Epoch 11 iteration 0111/0187: training loss 0.252; learning rate 0.000322
Epoch 11 iteration 0112/0187: training loss 0.252; learning rate 0.000322
Epoch 11 iteration 0113/0187: training loss 0.252; learning rate 0.000321
Epoch 11 iteration 0114/0187: training loss 0.252; learning rate 0.000321
Epoch 11 iteration 0115/0187: training loss 0.252; learning rate 0.000321
Epoch 11 iteration 0116/0187: training loss 0.252; learning rate 0.000321
Epoch 11 iteration 0117/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0118/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0119/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0120/0187: training loss 0.252; learning rate 0.000321
Epoch 11 iteration 0121/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0122/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0123/0187: training loss 0.251; learning rate 0.000321
Epoch 11 iteration 0124/0187: training loss 0.251; learning rate 0.000320
Epoch 11 iteration 0125/0187: training loss 0.251; learning rate 0.000320
Epoch 11 iteration 0126/0187: training loss 0.251; learning rate 0.000320
Epoch 11 iteration 0127/0187: training loss 0.250; learning rate 0.000320
Epoch 11 iteration 0128/0187: training loss 0.250; learning rate 0.000320
Epoch 11 iteration 0129/0187: training loss 0.249; learning rate 0.000320
Epoch 11 iteration 0130/0187: training loss 0.250; learning rate 0.000320
Epoch 11 iteration 0131/0187: training loss 0.249; learning rate 0.000320
Epoch 11 iteration 0132/0187: training loss 0.249; learning rate 0.000320
Epoch 11 iteration 0133/0187: training loss 0.250; learning rate 0.000320
Epoch 11 iteration 0134/0187: training loss 0.249; learning rate 0.000320
Epoch 11 iteration 0135/0187: training loss 0.249; learning rate 0.000320
Epoch 11 iteration 0136/0187: training loss 0.250; learning rate 0.000319
Epoch 11 iteration 0137/0187: training loss 0.249; learning rate 0.000319
Epoch 11 iteration 0138/0187: training loss 0.249; learning rate 0.000319
Epoch 11 iteration 0139/0187: training loss 0.249; learning rate 0.000319
Epoch 11 iteration 0140/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0141/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0142/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0143/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0144/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0145/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0146/0187: training loss 0.248; learning rate 0.000319
Epoch 11 iteration 0147/0187: training loss 0.249; learning rate 0.000319
Epoch 11 iteration 0148/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0149/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0150/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0151/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0152/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0153/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0154/0187: training loss 0.249; learning rate 0.000318
Epoch 11 iteration 0155/0187: training loss 0.250; learning rate 0.000318
Epoch 11 iteration 0156/0187: training loss 0.249; learning rate 0.000318
Epoch 11 iteration 0157/0187: training loss 0.249; learning rate 0.000318
Epoch 11 iteration 0158/0187: training loss 0.249; learning rate 0.000318
Epoch 11 iteration 0159/0187: training loss 0.249; learning rate 0.000318
Epoch 11 iteration 0160/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0161/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0162/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0163/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0164/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0165/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0166/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0167/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0168/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0169/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0170/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0171/0187: training loss 0.249; learning rate 0.000317
Epoch 11 iteration 0172/0187: training loss 0.249; learning rate 0.000316
Epoch 11 iteration 0173/0187: training loss 0.249; learning rate 0.000316
Epoch 11 iteration 0174/0187: training loss 0.249; learning rate 0.000316
Epoch 11 iteration 0175/0187: training loss 0.249; learning rate 0.000316
Epoch 11 iteration 0176/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0177/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0178/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0179/0187: training loss 0.251; learning rate 0.000316
Epoch 11 iteration 0180/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0181/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0182/0187: training loss 0.251; learning rate 0.000316
Epoch 11 iteration 0183/0187: training loss 0.250; learning rate 0.000316
Epoch 11 iteration 0184/0187: training loss 0.250; learning rate 0.000315
Epoch 11 iteration 0185/0187: training loss 0.250; learning rate 0.000315
Epoch 11 iteration 0186/0187: training loss 0.251; learning rate 0.000315
Epoch 11 iteration 0187/0187: training loss 0.251; learning rate 0.000315
Epoch 11 validation pixAcc: 0.334, mIoU: 0.202
Epoch 12 iteration 0001/0187: training loss 0.272; learning rate 0.000315
Epoch 12 iteration 0002/0187: training loss 0.244; learning rate 0.000315
Epoch 12 iteration 0003/0187: training loss 0.250; learning rate 0.000315
Epoch 12 iteration 0004/0187: training loss 0.236; learning rate 0.000315
Epoch 12 iteration 0005/0187: training loss 0.251; learning rate 0.000315
Epoch 12 iteration 0006/0187: training loss 0.250; learning rate 0.000315
Epoch 12 iteration 0007/0187: training loss 0.251; learning rate 0.000315
Epoch 12 iteration 0008/0187: training loss 0.254; learning rate 0.000314
Epoch 12 iteration 0009/0187: training loss 0.250; learning rate 0.000314
Epoch 12 iteration 0010/0187: training loss 0.253; learning rate 0.000314
Epoch 12 iteration 0011/0187: training loss 0.262; learning rate 0.000314
Epoch 12 iteration 0012/0187: training loss 0.255; learning rate 0.000314
Epoch 12 iteration 0013/0187: training loss 0.259; learning rate 0.000314
Epoch 12 iteration 0014/0187: training loss 0.259; learning rate 0.000314
Epoch 12 iteration 0015/0187: training loss 0.265; learning rate 0.000314
Epoch 12 iteration 0016/0187: training loss 0.262; learning rate 0.000314
Epoch 12 iteration 0017/0187: training loss 0.261; learning rate 0.000314
Epoch 12 iteration 0018/0187: training loss 0.263; learning rate 0.000314
Epoch 12 iteration 0019/0187: training loss 0.260; learning rate 0.000313
Epoch 12 iteration 0020/0187: training loss 0.262; learning rate 0.000313
Epoch 12 iteration 0021/0187: training loss 0.268; learning rate 0.000313
Epoch 12 iteration 0022/0187: training loss 0.266; learning rate 0.000313
Epoch 12 iteration 0023/0187: training loss 0.265; learning rate 0.000313
Epoch 12 iteration 0024/0187: training loss 0.262; learning rate 0.000313
Epoch 12 iteration 0025/0187: training loss 0.258; learning rate 0.000313
Epoch 12 iteration 0026/0187: training loss 0.257; learning rate 0.000313
Epoch 12 iteration 0027/0187: training loss 0.255; learning rate 0.000313
Epoch 12 iteration 0028/0187: training loss 0.253; learning rate 0.000313
Epoch 12 iteration 0029/0187: training loss 0.253; learning rate 0.000313
Epoch 12 iteration 0030/0187: training loss 0.253; learning rate 0.000313
Epoch 12 iteration 0031/0187: training loss 0.251; learning rate 0.000312
Epoch 12 iteration 0032/0187: training loss 0.254; learning rate 0.000312
Epoch 12 iteration 0033/0187: training loss 0.255; learning rate 0.000312
Epoch 12 iteration 0034/0187: training loss 0.254; learning rate 0.000312
Epoch 12 iteration 0035/0187: training loss 0.254; learning rate 0.000312
Epoch 12 iteration 0036/0187: training loss 0.254; learning rate 0.000312
Epoch 12 iteration 0037/0187: training loss 0.253; learning rate 0.000312
Epoch 12 iteration 0038/0187: training loss 0.254; learning rate 0.000312
Epoch 12 iteration 0039/0187: training loss 0.252; learning rate 0.000312
Epoch 12 iteration 0040/0187: training loss 0.251; learning rate 0.000312
Epoch 12 iteration 0041/0187: training loss 0.250; learning rate 0.000312
Epoch 12 iteration 0042/0187: training loss 0.251; learning rate 0.000312
Epoch 12 iteration 0043/0187: training loss 0.252; learning rate 0.000311
Epoch 12 iteration 0044/0187: training loss 0.251; learning rate 0.000311
Epoch 12 iteration 0045/0187: training loss 0.250; learning rate 0.000311
Epoch 12 iteration 0046/0187: training loss 0.250; learning rate 0.000311
Epoch 12 iteration 0047/0187: training loss 0.250; learning rate 0.000311
Epoch 12 iteration 0048/0187: training loss 0.248; learning rate 0.000311
Epoch 12 iteration 0049/0187: training loss 0.249; learning rate 0.000311
Epoch 12 iteration 0050/0187: training loss 0.248; learning rate 0.000311
Epoch 12 iteration 0051/0187: training loss 0.250; learning rate 0.000311
Epoch 12 iteration 0052/0187: training loss 0.251; learning rate 0.000311
Epoch 12 iteration 0053/0187: training loss 0.249; learning rate 0.000311
Epoch 12 iteration 0054/0187: training loss 0.250; learning rate 0.000311
Epoch 12 iteration 0055/0187: training loss 0.249; learning rate 0.000310
Epoch 12 iteration 0056/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0057/0187: training loss 0.250; learning rate 0.000310
Epoch 12 iteration 0058/0187: training loss 0.249; learning rate 0.000310
Epoch 12 iteration 0059/0187: training loss 0.249; learning rate 0.000310
Epoch 12 iteration 0060/0187: training loss 0.250; learning rate 0.000310
Epoch 12 iteration 0061/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0062/0187: training loss 0.250; learning rate 0.000310
Epoch 12 iteration 0063/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0064/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0065/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0066/0187: training loss 0.251; learning rate 0.000310
Epoch 12 iteration 0067/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0068/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0069/0187: training loss 0.251; learning rate 0.000309
Epoch 12 iteration 0070/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0071/0187: training loss 0.252; learning rate 0.000309
Epoch 12 iteration 0072/0187: training loss 0.252; learning rate 0.000309
Epoch 12 iteration 0073/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0074/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0075/0187: training loss 0.249; learning rate 0.000309
Epoch 12 iteration 0076/0187: training loss 0.249; learning rate 0.000309
Epoch 12 iteration 0077/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0078/0187: training loss 0.250; learning rate 0.000309
Epoch 12 iteration 0079/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0080/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0081/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0082/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0083/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0084/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0085/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0086/0187: training loss 0.251; learning rate 0.000308
Epoch 12 iteration 0087/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0088/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0089/0187: training loss 0.250; learning rate 0.000308
Epoch 12 iteration 0090/0187: training loss 0.250; learning rate 0.000307
Epoch 12 iteration 0091/0188: training loss 0.250; learning rate 0.000307
Epoch 12 iteration 0092/0188: training loss 0.251; learning rate 0.000307
Epoch 12 iteration 0093/0188: training loss 0.251; learning rate 0.000307
Epoch 12 iteration 0094/0188: training loss 0.250; learning rate 0.000307
Epoch 12 iteration 0095/0188: training loss 0.251; learning rate 0.000307
Epoch 12 iteration 0096/0188: training loss 0.252; learning rate 0.000307
Epoch 12 iteration 0097/0188: training loss 0.252; learning rate 0.000307
Epoch 12 iteration 0098/0188: training loss 0.252; learning rate 0.000307
Epoch 12 iteration 0099/0188: training loss 0.252; learning rate 0.000307
Epoch 12 iteration 0100/0188: training loss 0.251; learning rate 0.000307
Epoch 12 iteration 0101/0188: training loss 0.251; learning rate 0.000307
Epoch 12 iteration 0102/0188: training loss 0.252; learning rate 0.000306
Epoch 12 iteration 0103/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0104/0188: training loss 0.252; learning rate 0.000306
Epoch 12 iteration 0105/0188: training loss 0.252; learning rate 0.000306
Epoch 12 iteration 0106/0188: training loss 0.252; learning rate 0.000306
Epoch 12 iteration 0107/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0108/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0109/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0110/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0111/0188: training loss 0.252; learning rate 0.000306
Epoch 12 iteration 0112/0188: training loss 0.253; learning rate 0.000306
Epoch 12 iteration 0113/0188: training loss 0.254; learning rate 0.000306
Epoch 12 iteration 0114/0188: training loss 0.253; learning rate 0.000305
Epoch 12 iteration 0115/0188: training loss 0.253; learning rate 0.000305
Epoch 12 iteration 0116/0188: training loss 0.253; learning rate 0.000305
Epoch 12 iteration 0117/0188: training loss 0.253; learning rate 0.000305
Epoch 12 iteration 0118/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0119/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0120/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0121/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0122/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0123/0188: training loss 0.255; learning rate 0.000305
Epoch 12 iteration 0124/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0125/0188: training loss 0.254; learning rate 0.000305
Epoch 12 iteration 0126/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0127/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0128/0188: training loss 0.253; learning rate 0.000304
Epoch 12 iteration 0129/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0130/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0131/0188: training loss 0.253; learning rate 0.000304
Epoch 12 iteration 0132/0188: training loss 0.253; learning rate 0.000304
Epoch 12 iteration 0133/0188: training loss 0.253; learning rate 0.000304
Epoch 12 iteration 0134/0188: training loss 0.253; learning rate 0.000304
Epoch 12 iteration 0135/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0136/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0137/0188: training loss 0.254; learning rate 0.000304
Epoch 12 iteration 0138/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0139/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0140/0188: training loss 0.256; learning rate 0.000303
Epoch 12 iteration 0141/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0142/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0143/0188: training loss 0.256; learning rate 0.000303
Epoch 12 iteration 0144/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0145/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0146/0188: training loss 0.254; learning rate 0.000303
Epoch 12 iteration 0147/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0148/0188: training loss 0.255; learning rate 0.000303
Epoch 12 iteration 0149/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0150/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0151/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0152/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0153/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0154/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0155/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0156/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0157/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0158/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0159/0188: training loss 0.255; learning rate 0.000302
Epoch 12 iteration 0160/0188: training loss 0.256; learning rate 0.000302
Epoch 12 iteration 0161/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0162/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0163/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0164/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0165/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0166/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0167/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0168/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0169/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0170/0188: training loss 0.254; learning rate 0.000301
Epoch 12 iteration 0171/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0172/0188: training loss 0.255; learning rate 0.000301
Epoch 12 iteration 0173/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0174/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0175/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0176/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0177/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0178/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0179/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0180/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0181/0188: training loss 0.255; learning rate 0.000300
Epoch 12 iteration 0182/0188: training loss 0.256; learning rate 0.000300
Epoch 12 iteration 0183/0188: training loss 0.256; learning rate 0.000300
Epoch 12 iteration 0184/0188: training loss 0.256; learning rate 0.000300
Epoch 12 iteration 0185/0188: training loss 0.256; learning rate 0.000299
Epoch 12 iteration 0186/0188: training loss 0.256; learning rate 0.000299
Epoch 12 validation pixAcc: 0.332, mIoU: 0.202
Epoch 13 iteration 0001/0187: training loss 0.295; learning rate 0.000299
Epoch 13 iteration 0002/0187: training loss 0.282; learning rate 0.000299
Epoch 13 iteration 0003/0187: training loss 0.263; learning rate 0.000299
Epoch 13 iteration 0004/0187: training loss 0.263; learning rate 0.000299
Epoch 13 iteration 0005/0187: training loss 0.258; learning rate 0.000299
Epoch 13 iteration 0006/0187: training loss 0.247; learning rate 0.000299
Epoch 13 iteration 0007/0187: training loss 0.268; learning rate 0.000299
Epoch 13 iteration 0008/0187: training loss 0.274; learning rate 0.000299
Epoch 13 iteration 0009/0187: training loss 0.269; learning rate 0.000298
Epoch 13 iteration 0010/0187: training loss 0.265; learning rate 0.000298
Epoch 13 iteration 0011/0187: training loss 0.263; learning rate 0.000298
Epoch 13 iteration 0012/0187: training loss 0.262; learning rate 0.000298
Epoch 13 iteration 0013/0187: training loss 0.268; learning rate 0.000298
Epoch 13 iteration 0014/0187: training loss 0.261; learning rate 0.000298
Epoch 13 iteration 0015/0187: training loss 0.260; learning rate 0.000298
Epoch 13 iteration 0016/0187: training loss 0.256; learning rate 0.000298
Epoch 13 iteration 0017/0187: training loss 0.254; learning rate 0.000298
Epoch 13 iteration 0018/0187: training loss 0.256; learning rate 0.000298
Epoch 13 iteration 0019/0187: training loss 0.254; learning rate 0.000298
Epoch 13 iteration 0020/0187: training loss 0.252; learning rate 0.000298
Epoch 13 iteration 0021/0187: training loss 0.251; learning rate 0.000297
Epoch 13 iteration 0022/0187: training loss 0.252; learning rate 0.000297
Epoch 13 iteration 0023/0187: training loss 0.257; learning rate 0.000297
Epoch 13 iteration 0024/0187: training loss 0.257; learning rate 0.000297
Epoch 13 iteration 0025/0187: training loss 0.253; learning rate 0.000297
Epoch 13 iteration 0026/0187: training loss 0.253; learning rate 0.000297
Epoch 13 iteration 0027/0187: training loss 0.254; learning rate 0.000297
Epoch 13 iteration 0028/0187: training loss 0.255; learning rate 0.000297
Epoch 13 iteration 0029/0187: training loss 0.255; learning rate 0.000297
Epoch 13 iteration 0030/0187: training loss 0.254; learning rate 0.000297
Epoch 13 iteration 0031/0187: training loss 0.254; learning rate 0.000297
Epoch 13 iteration 0032/0187: training loss 0.256; learning rate 0.000297
Epoch 13 iteration 0033/0187: training loss 0.255; learning rate 0.000296
Epoch 13 iteration 0034/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0035/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0036/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0037/0187: training loss 0.260; learning rate 0.000296
Epoch 13 iteration 0038/0187: training loss 0.259; learning rate 0.000296
Epoch 13 iteration 0039/0187: training loss 0.258; learning rate 0.000296
Epoch 13 iteration 0040/0187: training loss 0.258; learning rate 0.000296
Epoch 13 iteration 0041/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0042/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0043/0187: training loss 0.256; learning rate 0.000296
Epoch 13 iteration 0044/0187: training loss 0.257; learning rate 0.000296
Epoch 13 iteration 0045/0187: training loss 0.258; learning rate 0.000295
Epoch 13 iteration 0046/0187: training loss 0.258; learning rate 0.000295
Epoch 13 iteration 0047/0187: training loss 0.256; learning rate 0.000295
Epoch 13 iteration 0048/0187: training loss 0.256; learning rate 0.000295
Epoch 13 iteration 0049/0187: training loss 0.256; learning rate 0.000295
Epoch 13 iteration 0050/0187: training loss 0.255; learning rate 0.000295
Epoch 13 iteration 0051/0187: training loss 0.254; learning rate 0.000295
Epoch 13 iteration 0052/0187: training loss 0.254; learning rate 0.000295
Epoch 13 iteration 0053/0187: training loss 0.254; learning rate 0.000295
Epoch 13 iteration 0054/0187: training loss 0.256; learning rate 0.000295
Epoch 13 iteration 0055/0187: training loss 0.256; learning rate 0.000295
Epoch 13 iteration 0056/0187: training loss 0.255; learning rate 0.000295
Epoch 13 iteration 0057/0187: training loss 0.255; learning rate 0.000294
Epoch 13 iteration 0058/0187: training loss 0.255; learning rate 0.000294
Epoch 13 iteration 0059/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0060/0187: training loss 0.255; learning rate 0.000294
Epoch 13 iteration 0061/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0062/0187: training loss 0.253; learning rate 0.000294
Epoch 13 iteration 0063/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0064/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0065/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0066/0187: training loss 0.254; learning rate 0.000294
Epoch 13 iteration 0067/0187: training loss 0.253; learning rate 0.000294
Epoch 13 iteration 0068/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0069/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0070/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0071/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0072/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0073/0187: training loss 0.251; learning rate 0.000293
Epoch 13 iteration 0074/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0075/0187: training loss 0.251; learning rate 0.000293
Epoch 13 iteration 0076/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0077/0187: training loss 0.253; learning rate 0.000293
Epoch 13 iteration 0078/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0079/0187: training loss 0.252; learning rate 0.000293
Epoch 13 iteration 0080/0187: training loss 0.252; learning rate 0.000292
Epoch 13 iteration 0081/0187: training loss 0.252; learning rate 0.000292
Epoch 13 iteration 0082/0187: training loss 0.253; learning rate 0.000292
Epoch 13 iteration 0083/0187: training loss 0.254; learning rate 0.000292
Epoch 13 iteration 0084/0187: training loss 0.254; learning rate 0.000292
Epoch 13 iteration 0085/0187: training loss 0.253; learning rate 0.000292
Epoch 13 iteration 0086/0187: training loss 0.254; learning rate 0.000292
Epoch 13 iteration 0087/0187: training loss 0.253; learning rate 0.000292
Epoch 13 iteration 0088/0187: training loss 0.252; learning rate 0.000292
Epoch 13 iteration 0089/0187: training loss 0.252; learning rate 0.000292
Epoch 13 iteration 0090/0187: training loss 0.252; learning rate 0.000292
Epoch 13 iteration 0091/0187: training loss 0.253; learning rate 0.000292
Epoch 13 iteration 0092/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0093/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0094/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0095/0187: training loss 0.251; learning rate 0.000291
Epoch 13 iteration 0096/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0097/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0098/0187: training loss 0.251; learning rate 0.000291
Epoch 13 iteration 0099/0187: training loss 0.251; learning rate 0.000291
Epoch 13 iteration 0100/0187: training loss 0.250; learning rate 0.000291
Epoch 13 iteration 0101/0187: training loss 0.251; learning rate 0.000291
Epoch 13 iteration 0102/0187: training loss 0.252; learning rate 0.000291
Epoch 13 iteration 0103/0187: training loss 0.252; learning rate 0.000290
Epoch 13 iteration 0104/0187: training loss 0.252; learning rate 0.000290
Epoch 13 iteration 0105/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0106/0187: training loss 0.250; learning rate 0.000290
Epoch 13 iteration 0107/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0108/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0109/0187: training loss 0.252; learning rate 0.000290
Epoch 13 iteration 0110/0187: training loss 0.252; learning rate 0.000290
Epoch 13 iteration 0111/0187: training loss 0.252; learning rate 0.000290
Epoch 13 iteration 0112/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0113/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0114/0187: training loss 0.251; learning rate 0.000290
Epoch 13 iteration 0115/0187: training loss 0.251; learning rate 0.000289
Epoch 13 iteration 0116/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0117/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0118/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0119/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0120/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0121/0187: training loss 0.249; learning rate 0.000289
Epoch 13 iteration 0122/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0123/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0124/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0125/0187: training loss 0.251; learning rate 0.000289
Epoch 13 iteration 0126/0187: training loss 0.250; learning rate 0.000289
Epoch 13 iteration 0127/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0128/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0129/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0130/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0131/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0132/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0133/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0134/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0135/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0136/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0137/0187: training loss 0.251; learning rate 0.000288
Epoch 13 iteration 0138/0187: training loss 0.250; learning rate 0.000288
Epoch 13 iteration 0139/0187: training loss 0.250; learning rate 0.000287
Epoch 13 iteration 0140/0187: training loss 0.251; learning rate 0.000287
Epoch 13 iteration 0141/0187: training loss 0.251; learning rate 0.000287
Epoch 13 iteration 0142/0187: training loss 0.251; learning rate 0.000287
Epoch 13 iteration 0143/0187: training loss 0.252; learning rate 0.000287
Epoch 13 iteration 0144/0187: training loss 0.252; learning rate 0.000287
Epoch 13 iteration 0145/0187: training loss 0.252; learning rate 0.000287
Epoch 13 iteration 0146/0187: training loss 0.252; learning rate 0.000287
Epoch 13 iteration 0147/0187: training loss 0.251; learning rate 0.000287
Epoch 13 iteration 0148/0187: training loss 0.252; learning rate 0.000287
Epoch 13 iteration 0149/0187: training loss 0.253; learning rate 0.000287
Epoch 13 iteration 0150/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0151/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0152/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0153/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0154/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0155/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0156/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0157/0187: training loss 0.253; learning rate 0.000286
Epoch 13 iteration 0158/0187: training loss 0.252; learning rate 0.000286
Epoch 13 iteration 0159/0187: training loss 0.253; learning rate 0.000286
Epoch 13 iteration 0160/0187: training loss 0.253; learning rate 0.000286
Epoch 13 iteration 0161/0187: training loss 0.253; learning rate 0.000286
Epoch 13 iteration 0162/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0163/0187: training loss 0.252; learning rate 0.000285
Epoch 13 iteration 0164/0187: training loss 0.252; learning rate 0.000285
Epoch 13 iteration 0165/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0166/0187: training loss 0.252; learning rate 0.000285
Epoch 13 iteration 0167/0187: training loss 0.252; learning rate 0.000285
Epoch 13 iteration 0168/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0169/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0170/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0171/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0172/0187: training loss 0.252; learning rate 0.000285
Epoch 13 iteration 0173/0187: training loss 0.253; learning rate 0.000285
Epoch 13 iteration 0174/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0175/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0176/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0177/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0178/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0179/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0180/0187: training loss 0.253; learning rate 0.000284
Epoch 13 iteration 0181/0187: training loss 0.254; learning rate 0.000284
Epoch 13 iteration 0182/0187: training loss 0.254; learning rate 0.000284
Epoch 13 iteration 0183/0187: training loss 0.254; learning rate 0.000284
Epoch 13 iteration 0184/0187: training loss 0.254; learning rate 0.000284
Epoch 13 iteration 0185/0187: training loss 0.254; learning rate 0.000284
Epoch 13 iteration 0186/0187: training loss 0.254; learning rate 0.000283
Epoch 13 iteration 0187/0187: training loss 0.254; learning rate 0.000283
Epoch 13 validation pixAcc: 0.334, mIoU: 0.202
Epoch 14 iteration 0001/0187: training loss 0.252; learning rate 0.000283
Epoch 14 iteration 0002/0187: training loss 0.278; learning rate 0.000283
Epoch 14 iteration 0003/0187: training loss 0.261; learning rate 0.000283
Epoch 14 iteration 0004/0187: training loss 0.266; learning rate 0.000283
Epoch 14 iteration 0005/0187: training loss 0.259; learning rate 0.000283
Epoch 14 iteration 0006/0187: training loss 0.252; learning rate 0.000283
Epoch 14 iteration 0007/0187: training loss 0.249; learning rate 0.000283
Epoch 14 iteration 0008/0187: training loss 0.246; learning rate 0.000283
Epoch 14 iteration 0009/0187: training loss 0.243; learning rate 0.000282
Epoch 14 iteration 0010/0187: training loss 0.239; learning rate 0.000282
Epoch 14 iteration 0011/0187: training loss 0.239; learning rate 0.000282
Epoch 14 iteration 0012/0187: training loss 0.239; learning rate 0.000282
Epoch 14 iteration 0013/0187: training loss 0.239; learning rate 0.000282
Epoch 14 iteration 0014/0187: training loss 0.249; learning rate 0.000282
Epoch 14 iteration 0015/0187: training loss 0.247; learning rate 0.000282
Epoch 14 iteration 0016/0187: training loss 0.248; learning rate 0.000282
Epoch 14 iteration 0017/0187: training loss 0.247; learning rate 0.000282
Epoch 14 iteration 0018/0187: training loss 0.252; learning rate 0.000282
Epoch 14 iteration 0019/0187: training loss 0.252; learning rate 0.000282
Epoch 14 iteration 0020/0187: training loss 0.252; learning rate 0.000282
Epoch 14 iteration 0021/0187: training loss 0.256; learning rate 0.000281
Epoch 14 iteration 0022/0187: training loss 0.257; learning rate 0.000281
Epoch 14 iteration 0023/0187: training loss 0.256; learning rate 0.000281
Epoch 14 iteration 0024/0187: training loss 0.253; learning rate 0.000281
Epoch 14 iteration 0025/0187: training loss 0.254; learning rate 0.000281
Epoch 14 iteration 0026/0187: training loss 0.254; learning rate 0.000281
Epoch 14 iteration 0027/0187: training loss 0.252; learning rate 0.000281
Epoch 14 iteration 0028/0187: training loss 0.255; learning rate 0.000281
Epoch 14 iteration 0029/0187: training loss 0.252; learning rate 0.000281
Epoch 14 iteration 0030/0187: training loss 0.249; learning rate 0.000281
Epoch 14 iteration 0031/0187: training loss 0.251; learning rate 0.000281
Epoch 14 iteration 0032/0187: training loss 0.255; learning rate 0.000281
Epoch 14 iteration 0033/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0034/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0035/0187: training loss 0.252; learning rate 0.000280
Epoch 14 iteration 0036/0187: training loss 0.252; learning rate 0.000280
Epoch 14 iteration 0037/0187: training loss 0.255; learning rate 0.000280
Epoch 14 iteration 0038/0187: training loss 0.255; learning rate 0.000280
Epoch 14 iteration 0039/0187: training loss 0.255; learning rate 0.000280
Epoch 14 iteration 0040/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0041/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0042/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0043/0187: training loss 0.254; learning rate 0.000280
Epoch 14 iteration 0044/0187: training loss 0.254; learning rate 0.000279
Epoch 14 iteration 0045/0187: training loss 0.254; learning rate 0.000279
Epoch 14 iteration 0046/0187: training loss 0.253; learning rate 0.000279
Epoch 14 iteration 0047/0187: training loss 0.252; learning rate 0.000279
Epoch 14 iteration 0048/0187: training loss 0.251; learning rate 0.000279
Epoch 14 iteration 0049/0187: training loss 0.253; learning rate 0.000279
Epoch 14 iteration 0050/0187: training loss 0.251; learning rate 0.000279
Epoch 14 iteration 0051/0187: training loss 0.251; learning rate 0.000279
Epoch 14 iteration 0052/0187: training loss 0.250; learning rate 0.000279
Epoch 14 iteration 0053/0187: training loss 0.250; learning rate 0.000279
Epoch 14 iteration 0054/0187: training loss 0.252; learning rate 0.000279
Epoch 14 iteration 0055/0187: training loss 0.250; learning rate 0.000279
Epoch 14 iteration 0056/0187: training loss 0.251; learning rate 0.000278
Epoch 14 iteration 0057/0187: training loss 0.250; learning rate 0.000278
Epoch 14 iteration 0058/0187: training loss 0.250; learning rate 0.000278
Epoch 14 iteration 0059/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0060/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0061/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0062/0187: training loss 0.248; learning rate 0.000278
Epoch 14 iteration 0063/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0064/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0065/0187: training loss 0.248; learning rate 0.000278
Epoch 14 iteration 0066/0187: training loss 0.248; learning rate 0.000278
Epoch 14 iteration 0067/0187: training loss 0.249; learning rate 0.000278
Epoch 14 iteration 0068/0187: training loss 0.251; learning rate 0.000277
Epoch 14 iteration 0069/0187: training loss 0.251; learning rate 0.000277
Epoch 14 iteration 0070/0187: training loss 0.250; learning rate 0.000277
Epoch 14 iteration 0071/0187: training loss 0.250; learning rate 0.000277
Epoch 14 iteration 0072/0187: training loss 0.250; learning rate 0.000277
Epoch 14 iteration 0073/0187: training loss 0.251; learning rate 0.000277
Epoch 14 iteration 0074/0187: training loss 0.252; learning rate 0.000277
Epoch 14 iteration 0075/0187: training loss 0.252; learning rate 0.000277
Epoch 14 iteration 0076/0187: training loss 0.254; learning rate 0.000277
Epoch 14 iteration 0077/0187: training loss 0.254; learning rate 0.000277
Epoch 14 iteration 0078/0187: training loss 0.253; learning rate 0.000277
Epoch 14 iteration 0079/0187: training loss 0.254; learning rate 0.000276
Epoch 14 iteration 0080/0187: training loss 0.254; learning rate 0.000276
Epoch 14 iteration 0081/0187: training loss 0.254; learning rate 0.000276
Epoch 14 iteration 0082/0187: training loss 0.254; learning rate 0.000276
Epoch 14 iteration 0083/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0084/0187: training loss 0.252; learning rate 0.000276
Epoch 14 iteration 0085/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0086/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0087/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0088/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0089/0187: training loss 0.253; learning rate 0.000276
Epoch 14 iteration 0090/0187: training loss 0.252; learning rate 0.000276
Epoch 14 iteration 0091/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0092/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0093/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0094/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0095/0188: training loss 0.252; learning rate 0.000275
Epoch 14 iteration 0096/0188: training loss 0.252; learning rate 0.000275
Epoch 14 iteration 0097/0188: training loss 0.252; learning rate 0.000275
Epoch 14 iteration 0098/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0099/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0100/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0101/0188: training loss 0.251; learning rate 0.000275
Epoch 14 iteration 0102/0188: training loss 0.252; learning rate 0.000275
Epoch 14 iteration 0103/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0104/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0105/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0106/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0107/0188: training loss 0.253; learning rate 0.000274
Epoch 14 iteration 0108/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0109/0188: training loss 0.252; learning rate 0.000274
Epoch 14 iteration 0110/0188: training loss 0.254; learning rate 0.000274
Epoch 14 iteration 0111/0188: training loss 0.253; learning rate 0.000274
Epoch 14 iteration 0112/0188: training loss 0.253; learning rate 0.000274
Epoch 14 iteration 0113/0188: training loss 0.253; learning rate 0.000274
Epoch 14 iteration 0114/0188: training loss 0.253; learning rate 0.000273
Epoch 14 iteration 0115/0188: training loss 0.253; learning rate 0.000273
Epoch 14 iteration 0116/0188: training loss 0.253; learning rate 0.000273
Epoch 14 iteration 0117/0188: training loss 0.253; learning rate 0.000273
Epoch 14 iteration 0118/0188: training loss 0.252; learning rate 0.000273
Epoch 14 iteration 0119/0188: training loss 0.252; learning rate 0.000273
Epoch 14 iteration 0120/0188: training loss 0.251; learning rate 0.000273
Epoch 14 iteration 0121/0188: training loss 0.251; learning rate 0.000273
Epoch 14 iteration 0122/0188: training loss 0.251; learning rate 0.000273
Epoch 14 iteration 0123/0188: training loss 0.250; learning rate 0.000273
Epoch 14 iteration 0124/0188: training loss 0.251; learning rate 0.000273
Epoch 14 iteration 0125/0188: training loss 0.250; learning rate 0.000273
Epoch 14 iteration 0126/0188: training loss 0.250; learning rate 0.000272
Epoch 14 iteration 0127/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0128/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0129/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0130/0188: training loss 0.250; learning rate 0.000272
Epoch 14 iteration 0131/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0132/0188: training loss 0.250; learning rate 0.000272
Epoch 14 iteration 0133/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0134/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0135/0188: training loss 0.251; learning rate 0.000272
Epoch 14 iteration 0136/0188: training loss 0.250; learning rate 0.000272
Epoch 14 iteration 0137/0188: training loss 0.250; learning rate 0.000272
Epoch 14 iteration 0138/0188: training loss 0.250; learning rate 0.000271
Epoch 14 iteration 0139/0188: training loss 0.251; learning rate 0.000271
Epoch 14 iteration 0140/0188: training loss 0.251; learning rate 0.000271
Epoch 14 iteration 0141/0188: training loss 0.251; learning rate 0.000271
Epoch 14 iteration 0142/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0143/0188: training loss 0.253; learning rate 0.000271
Epoch 14 iteration 0144/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0145/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0146/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0147/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0148/0188: training loss 0.252; learning rate 0.000271
Epoch 14 iteration 0149/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0150/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0151/0188: training loss 0.251; learning rate 0.000270
Epoch 14 iteration 0152/0188: training loss 0.251; learning rate 0.000270
Epoch 14 iteration 0153/0188: training loss 0.251; learning rate 0.000270
Epoch 14 iteration 0154/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0155/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0156/0188: training loss 0.253; learning rate 0.000270
Epoch 14 iteration 0157/0188: training loss 0.253; learning rate 0.000270
Epoch 14 iteration 0158/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0159/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0160/0188: training loss 0.252; learning rate 0.000270
Epoch 14 iteration 0161/0188: training loss 0.252; learning rate 0.000269
Epoch 14 iteration 0162/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0163/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0164/0188: training loss 0.252; learning rate 0.000269
Epoch 14 iteration 0165/0188: training loss 0.252; learning rate 0.000269
Epoch 14 iteration 0166/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0167/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0168/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0169/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0170/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0171/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0172/0188: training loss 0.251; learning rate 0.000269
Epoch 14 iteration 0173/0188: training loss 0.251; learning rate 0.000268
Epoch 14 iteration 0174/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0175/0188: training loss 0.251; learning rate 0.000268
Epoch 14 iteration 0176/0188: training loss 0.251; learning rate 0.000268
Epoch 14 iteration 0177/0188: training loss 0.251; learning rate 0.000268
Epoch 14 iteration 0178/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0179/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0180/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0181/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0182/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0183/0188: training loss 0.252; learning rate 0.000268
Epoch 14 iteration 0184/0188: training loss 0.252; learning rate 0.000267
Epoch 14 iteration 0185/0188: training loss 0.252; learning rate 0.000267
Epoch 14 iteration 0186/0188: training loss 0.252; learning rate 0.000267
Epoch 14 validation pixAcc: 0.335, mIoU: 0.203
Epoch 15 iteration 0001/0187: training loss 0.222; learning rate 0.000267
Epoch 15 iteration 0002/0187: training loss 0.228; learning rate 0.000267
Epoch 15 iteration 0003/0187: training loss 0.249; learning rate 0.000267
Epoch 15 iteration 0004/0187: training loss 0.262; learning rate 0.000267
Epoch 15 iteration 0005/0187: training loss 0.262; learning rate 0.000267
Epoch 15 iteration 0006/0187: training loss 0.264; learning rate 0.000267
Epoch 15 iteration 0007/0187: training loss 0.255; learning rate 0.000267
Epoch 15 iteration 0008/0187: training loss 0.253; learning rate 0.000267
Epoch 15 iteration 0009/0187: training loss 0.249; learning rate 0.000266
Epoch 15 iteration 0010/0187: training loss 0.245; learning rate 0.000266
Epoch 15 iteration 0011/0187: training loss 0.246; learning rate 0.000266
Epoch 15 iteration 0012/0187: training loss 0.241; learning rate 0.000266
Epoch 15 iteration 0013/0187: training loss 0.248; learning rate 0.000266
Epoch 15 iteration 0014/0187: training loss 0.244; learning rate 0.000266
Epoch 15 iteration 0015/0187: training loss 0.247; learning rate 0.000266
Epoch 15 iteration 0016/0187: training loss 0.247; learning rate 0.000266
Epoch 15 iteration 0017/0187: training loss 0.249; learning rate 0.000266
Epoch 15 iteration 0018/0187: training loss 0.247; learning rate 0.000266
Epoch 15 iteration 0019/0187: training loss 0.246; learning rate 0.000266
Epoch 15 iteration 0020/0187: training loss 0.245; learning rate 0.000265
Epoch 15 iteration 0021/0187: training loss 0.251; learning rate 0.000265
Epoch 15 iteration 0022/0187: training loss 0.252; learning rate 0.000265
Epoch 15 iteration 0023/0187: training loss 0.250; learning rate 0.000265
Epoch 15 iteration 0024/0187: training loss 0.249; learning rate 0.000265
Epoch 15 iteration 0025/0187: training loss 0.247; learning rate 0.000265
Epoch 15 iteration 0026/0187: training loss 0.249; learning rate 0.000265
Epoch 15 iteration 0027/0187: training loss 0.248; learning rate 0.000265
Epoch 15 iteration 0028/0187: training loss 0.249; learning rate 0.000265
Epoch 15 iteration 0029/0187: training loss 0.250; learning rate 0.000265
Epoch 15 iteration 0030/0187: training loss 0.249; learning rate 0.000265
Epoch 15 iteration 0031/0187: training loss 0.248; learning rate 0.000265
Epoch 15 iteration 0032/0187: training loss 0.249; learning rate 0.000264
Epoch 15 iteration 0033/0187: training loss 0.248; learning rate 0.000264
Epoch 15 iteration 0034/0187: training loss 0.247; learning rate 0.000264
Epoch 15 iteration 0035/0187: training loss 0.246; learning rate 0.000264
Epoch 15 iteration 0036/0187: training loss 0.246; learning rate 0.000264
Epoch 15 iteration 0037/0187: training loss 0.246; learning rate 0.000264
Epoch 15 iteration 0038/0187: training loss 0.245; learning rate 0.000264
Epoch 15 iteration 0039/0187: training loss 0.247; learning rate 0.000264
Epoch 15 iteration 0040/0187: training loss 0.246; learning rate 0.000264
Epoch 15 iteration 0041/0187: training loss 0.244; learning rate 0.000264
Epoch 15 iteration 0042/0187: training loss 0.245; learning rate 0.000264
Epoch 15 iteration 0043/0187: training loss 0.244; learning rate 0.000264
Epoch 15 iteration 0044/0187: training loss 0.245; learning rate 0.000263
Epoch 15 iteration 0045/0187: training loss 0.245; learning rate 0.000263
Epoch 15 iteration 0046/0187: training loss 0.244; learning rate 0.000263
Epoch 15 iteration 0047/0187: training loss 0.247; learning rate 0.000263
Epoch 15 iteration 0048/0187: training loss 0.247; learning rate 0.000263
Epoch 15 iteration 0049/0187: training loss 0.246; learning rate 0.000263
Epoch 15 iteration 0050/0187: training loss 0.247; learning rate 0.000263
Epoch 15 iteration 0051/0187: training loss 0.248; learning rate 0.000263
Epoch 15 iteration 0052/0187: training loss 0.248; learning rate 0.000263
Epoch 15 iteration 0053/0187: training loss 0.248; learning rate 0.000263
Epoch 15 iteration 0054/0187: training loss 0.247; learning rate 0.000263
Epoch 15 iteration 0055/0187: training loss 0.245; learning rate 0.000262
Epoch 15 iteration 0056/0187: training loss 0.245; learning rate 0.000262
Epoch 15 iteration 0057/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0058/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0059/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0060/0187: training loss 0.248; learning rate 0.000262
Epoch 15 iteration 0061/0187: training loss 0.248; learning rate 0.000262
Epoch 15 iteration 0062/0187: training loss 0.248; learning rate 0.000262
Epoch 15 iteration 0063/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0064/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0065/0187: training loss 0.247; learning rate 0.000262
Epoch 15 iteration 0066/0187: training loss 0.246; learning rate 0.000262
Epoch 15 iteration 0067/0187: training loss 0.246; learning rate 0.000261
Epoch 15 iteration 0068/0187: training loss 0.246; learning rate 0.000261
Epoch 15 iteration 0069/0187: training loss 0.246; learning rate 0.000261
Epoch 15 iteration 0070/0187: training loss 0.245; learning rate 0.000261
Epoch 15 iteration 0071/0187: training loss 0.248; learning rate 0.000261
Epoch 15 iteration 0072/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0073/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0074/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0075/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0076/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0077/0187: training loss 0.249; learning rate 0.000261
Epoch 15 iteration 0078/0187: training loss 0.248; learning rate 0.000260
Epoch 15 iteration 0079/0187: training loss 0.248; learning rate 0.000260
Epoch 15 iteration 0080/0187: training loss 0.248; learning rate 0.000260
Epoch 15 iteration 0081/0187: training loss 0.249; learning rate 0.000260
Epoch 15 iteration 0082/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0083/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0084/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0085/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0086/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0087/0187: training loss 0.251; learning rate 0.000260
Epoch 15 iteration 0088/0187: training loss 0.252; learning rate 0.000260
Epoch 15 iteration 0089/0187: training loss 0.252; learning rate 0.000260
Epoch 15 iteration 0090/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0091/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0092/0187: training loss 0.253; learning rate 0.000259
Epoch 15 iteration 0093/0187: training loss 0.253; learning rate 0.000259
Epoch 15 iteration 0094/0187: training loss 0.253; learning rate 0.000259
Epoch 15 iteration 0095/0187: training loss 0.253; learning rate 0.000259
Epoch 15 iteration 0096/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0097/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0098/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0099/0187: training loss 0.252; learning rate 0.000259
Epoch 15 iteration 0100/0187: training loss 0.251; learning rate 0.000259
Epoch 15 iteration 0101/0187: training loss 0.251; learning rate 0.000259
Epoch 15 iteration 0102/0187: training loss 0.251; learning rate 0.000258
Epoch 15 iteration 0103/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0104/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0105/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0106/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0107/0187: training loss 0.251; learning rate 0.000258
Epoch 15 iteration 0108/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0109/0187: training loss 0.253; learning rate 0.000258
Epoch 15 iteration 0110/0187: training loss 0.253; learning rate 0.000258
Epoch 15 iteration 0111/0187: training loss 0.253; learning rate 0.000258
Epoch 15 iteration 0112/0187: training loss 0.252; learning rate 0.000258
Epoch 15 iteration 0113/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0114/0187: training loss 0.251; learning rate 0.000257
Epoch 15 iteration 0115/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0116/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0117/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0118/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0119/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0120/0187: training loss 0.251; learning rate 0.000257
Epoch 15 iteration 0121/0187: training loss 0.251; learning rate 0.000257
Epoch 15 iteration 0122/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0123/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0124/0187: training loss 0.252; learning rate 0.000257
Epoch 15 iteration 0125/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0126/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0127/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0128/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0129/0187: training loss 0.253; learning rate 0.000256
Epoch 15 iteration 0130/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0131/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0132/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0133/0187: training loss 0.253; learning rate 0.000256
Epoch 15 iteration 0134/0187: training loss 0.253; learning rate 0.000256
Epoch 15 iteration 0135/0187: training loss 0.252; learning rate 0.000256
Epoch 15 iteration 0136/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0137/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0138/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0139/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0140/0187: training loss 0.253; learning rate 0.000255
Epoch 15 iteration 0141/0187: training loss 0.253; learning rate 0.000255
Epoch 15 iteration 0142/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0143/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0144/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0145/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0146/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0147/0187: training loss 0.252; learning rate 0.000255
Epoch 15 iteration 0148/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0149/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0150/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0151/0187: training loss 0.253; learning rate 0.000254
Epoch 15 iteration 0152/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0153/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0154/0187: training loss 0.251; learning rate 0.000254
Epoch 15 iteration 0155/0187: training loss 0.253; learning rate 0.000254
Epoch 15 iteration 0156/0187: training loss 0.252; learning rate 0.000254
Epoch 15 iteration 0157/0187: training loss 0.253; learning rate 0.000254
Epoch 15 iteration 0158/0187: training loss 0.253; learning rate 0.000254
Epoch 15 iteration 0159/0187: training loss 0.253; learning rate 0.000253
Epoch 15 iteration 0160/0187: training loss 0.252; learning rate 0.000253
Epoch 15 iteration 0161/0187: training loss 0.253; learning rate 0.000253
Epoch 15 iteration 0162/0187: training loss 0.253; learning rate 0.000253
Epoch 15 iteration 0163/0187: training loss 0.253; learning rate 0.000253
Epoch 15 iteration 0164/0187: training loss 0.252; learning rate 0.000253
Epoch 15 iteration 0165/0187: training loss 0.252; learning rate 0.000253
Epoch 15 iteration 0166/0187: training loss 0.251; learning rate 0.000253
Epoch 15 iteration 0167/0187: training loss 0.251; learning rate 0.000253
Epoch 15 iteration 0168/0187: training loss 0.251; learning rate 0.000253
Epoch 15 iteration 0169/0187: training loss 0.251; learning rate 0.000253
Epoch 15 iteration 0170/0187: training loss 0.250; learning rate 0.000253
Epoch 15 iteration 0171/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0172/0187: training loss 0.250; learning rate 0.000252
Epoch 15 iteration 0173/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0174/0187: training loss 0.250; learning rate 0.000252
Epoch 15 iteration 0175/0187: training loss 0.250; learning rate 0.000252
Epoch 15 iteration 0176/0187: training loss 0.250; learning rate 0.000252
Epoch 15 iteration 0177/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0178/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0179/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0180/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0181/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0182/0187: training loss 0.251; learning rate 0.000252
Epoch 15 iteration 0183/0187: training loss 0.251; learning rate 0.000251
Epoch 15 iteration 0184/0187: training loss 0.251; learning rate 0.000251
Epoch 15 iteration 0185/0187: training loss 0.251; learning rate 0.000251
Epoch 15 iteration 0186/0187: training loss 0.251; learning rate 0.000251
Epoch 15 iteration 0187/0187: training loss 0.250; learning rate 0.000251
Epoch 15 validation pixAcc: 0.335, mIoU: 0.202
Epoch 16 iteration 0001/0187: training loss 0.259; learning rate 0.000251
Epoch 16 iteration 0002/0187: training loss 0.243; learning rate 0.000251
Epoch 16 iteration 0003/0187: training loss 0.254; learning rate 0.000251
Epoch 16 iteration 0004/0187: training loss 0.259; learning rate 0.000251
Epoch 16 iteration 0005/0187: training loss 0.256; learning rate 0.000251
Epoch 16 iteration 0006/0187: training loss 0.249; learning rate 0.000250
Epoch 16 iteration 0007/0187: training loss 0.246; learning rate 0.000250
Epoch 16 iteration 0008/0187: training loss 0.247; learning rate 0.000250
Epoch 16 iteration 0009/0187: training loss 0.244; learning rate 0.000250
Epoch 16 iteration 0010/0187: training loss 0.242; learning rate 0.000250
Epoch 16 iteration 0011/0187: training loss 0.241; learning rate 0.000250
Epoch 16 iteration 0012/0187: training loss 0.243; learning rate 0.000250
Epoch 16 iteration 0013/0187: training loss 0.244; learning rate 0.000250
Epoch 16 iteration 0014/0187: training loss 0.245; learning rate 0.000250
Epoch 16 iteration 0015/0187: training loss 0.245; learning rate 0.000250
Epoch 16 iteration 0016/0187: training loss 0.248; learning rate 0.000250
Epoch 16 iteration 0017/0187: training loss 0.248; learning rate 0.000250
Epoch 16 iteration 0018/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0019/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0020/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0021/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0022/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0023/0187: training loss 0.250; learning rate 0.000249
Epoch 16 iteration 0024/0187: training loss 0.253; learning rate 0.000249
Epoch 16 iteration 0025/0187: training loss 0.251; learning rate 0.000249
Epoch 16 iteration 0026/0187: training loss 0.251; learning rate 0.000249
Epoch 16 iteration 0027/0187: training loss 0.251; learning rate 0.000249
Epoch 16 iteration 0028/0187: training loss 0.249; learning rate 0.000249
Epoch 16 iteration 0029/0187: training loss 0.250; learning rate 0.000248
Epoch 16 iteration 0030/0187: training loss 0.248; learning rate 0.000248
Epoch 16 iteration 0031/0187: training loss 0.247; learning rate 0.000248
Epoch 16 iteration 0032/0187: training loss 0.246; learning rate 0.000248
Epoch 16 iteration 0033/0187: training loss 0.243; learning rate 0.000248
Epoch 16 iteration 0034/0187: training loss 0.242; learning rate 0.000248
Epoch 16 iteration 0035/0187: training loss 0.243; learning rate 0.000248
Epoch 16 iteration 0036/0187: training loss 0.242; learning rate 0.000248
Epoch 16 iteration 0037/0187: training loss 0.240; learning rate 0.000248
Epoch 16 iteration 0038/0187: training loss 0.241; learning rate 0.000248
Epoch 16 iteration 0039/0187: training loss 0.241; learning rate 0.000248
Epoch 16 iteration 0040/0187: training loss 0.241; learning rate 0.000248
Epoch 16 iteration 0041/0187: training loss 0.243; learning rate 0.000247
Epoch 16 iteration 0042/0187: training loss 0.242; learning rate 0.000247
Epoch 16 iteration 0043/0187: training loss 0.242; learning rate 0.000247
Epoch 16 iteration 0044/0187: training loss 0.245; learning rate 0.000247
Epoch 16 iteration 0045/0187: training loss 0.250; learning rate 0.000247
Epoch 16 iteration 0046/0187: training loss 0.249; learning rate 0.000247
Epoch 16 iteration 0047/0187: training loss 0.249; learning rate 0.000247
Epoch 16 iteration 0048/0187: training loss 0.249; learning rate 0.000247
Epoch 16 iteration 0049/0187: training loss 0.248; learning rate 0.000247
Epoch 16 iteration 0050/0187: training loss 0.248; learning rate 0.000247
Epoch 16 iteration 0051/0187: training loss 0.249; learning rate 0.000247
Epoch 16 iteration 0052/0187: training loss 0.250; learning rate 0.000246
Epoch 16 iteration 0053/0187: training loss 0.254; learning rate 0.000246
Epoch 16 iteration 0054/0187: training loss 0.252; learning rate 0.000246
Epoch 16 iteration 0055/0187: training loss 0.251; learning rate 0.000246
Epoch 16 iteration 0056/0187: training loss 0.250; learning rate 0.000246
Epoch 16 iteration 0057/0187: training loss 0.250; learning rate 0.000246
Epoch 16 iteration 0058/0187: training loss 0.252; learning rate 0.000246
Epoch 16 iteration 0059/0187: training loss 0.253; learning rate 0.000246
Epoch 16 iteration 0060/0187: training loss 0.252; learning rate 0.000246
Epoch 16 iteration 0061/0187: training loss 0.251; learning rate 0.000246
Epoch 16 iteration 0062/0187: training loss 0.251; learning rate 0.000246
Epoch 16 iteration 0063/0187: training loss 0.252; learning rate 0.000246
Epoch 16 iteration 0064/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0065/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0066/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0067/0187: training loss 0.253; learning rate 0.000245
Epoch 16 iteration 0068/0187: training loss 0.252; learning rate 0.000245
Epoch 16 iteration 0069/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0070/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0071/0187: training loss 0.250; learning rate 0.000245
Epoch 16 iteration 0072/0187: training loss 0.250; learning rate 0.000245
Epoch 16 iteration 0073/0187: training loss 0.249; learning rate 0.000245
Epoch 16 iteration 0074/0187: training loss 0.251; learning rate 0.000245
Epoch 16 iteration 0075/0187: training loss 0.251; learning rate 0.000244
Epoch 16 iteration 0076/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0077/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0078/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0079/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0080/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0081/0187: training loss 0.253; learning rate 0.000244
Epoch 16 iteration 0082/0187: training loss 0.253; learning rate 0.000244
Epoch 16 iteration 0083/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0084/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0085/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0086/0187: training loss 0.252; learning rate 0.000244
Epoch 16 iteration 0087/0187: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0088/0187: training loss 0.251; learning rate 0.000243
Epoch 16 iteration 0089/0187: training loss 0.250; learning rate 0.000243
Epoch 16 iteration 0090/0187: training loss 0.250; learning rate 0.000243
Epoch 16 iteration 0091/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0092/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0093/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0094/0188: training loss 0.253; learning rate 0.000243
Epoch 16 iteration 0095/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0096/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0097/0188: training loss 0.252; learning rate 0.000243
Epoch 16 iteration 0098/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0099/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0100/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0101/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0102/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0103/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0104/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0105/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0106/0188: training loss 0.254; learning rate 0.000242
Epoch 16 iteration 0107/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0108/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0109/0188: training loss 0.253; learning rate 0.000242
Epoch 16 iteration 0110/0188: training loss 0.254; learning rate 0.000241
Epoch 16 iteration 0111/0188: training loss 0.254; learning rate 0.000241
Epoch 16 iteration 0112/0188: training loss 0.254; learning rate 0.000241
Epoch 16 iteration 0113/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0114/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0115/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0116/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0117/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0118/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0119/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0120/0188: training loss 0.255; learning rate 0.000241
Epoch 16 iteration 0121/0188: training loss 0.255; learning rate 0.000240
Epoch 16 iteration 0122/0188: training loss 0.256; learning rate 0.000240
Epoch 16 iteration 0123/0188: training loss 0.256; learning rate 0.000240
Epoch 16 iteration 0124/0188: training loss 0.256; learning rate 0.000240
Epoch 16 iteration 0125/0188: training loss 0.255; learning rate 0.000240
Epoch 16 iteration 0126/0188: training loss 0.255; learning rate 0.000240
Epoch 16 iteration 0127/0188: training loss 0.256; learning rate 0.000240
Epoch 16 iteration 0128/0188: training loss 0.255; learning rate 0.000240
Epoch 16 iteration 0129/0188: training loss 0.254; learning rate 0.000240
Epoch 16 iteration 0130/0188: training loss 0.254; learning rate 0.000240
Epoch 16 iteration 0131/0188: training loss 0.254; learning rate 0.000240
Epoch 16 iteration 0132/0188: training loss 0.254; learning rate 0.000240
Epoch 16 iteration 0133/0188: training loss 0.254; learning rate 0.000239
Epoch 16 iteration 0134/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0135/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0136/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0137/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0138/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0139/0188: training loss 0.254; learning rate 0.000239
Epoch 16 iteration 0140/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0141/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0142/0188: training loss 0.253; learning rate 0.000239
Epoch 16 iteration 0143/0188: training loss 0.254; learning rate 0.000239
Epoch 16 iteration 0144/0188: training loss 0.253; learning rate 0.000238
Epoch 16 iteration 0145/0188: training loss 0.253; learning rate 0.000238
Epoch 16 iteration 0146/0188: training loss 0.253; learning rate 0.000238
Epoch 16 iteration 0147/0188: training loss 0.253; learning rate 0.000238
Epoch 16 iteration 0148/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0149/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0150/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0151/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0152/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0153/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0154/0188: training loss 0.252; learning rate 0.000238
Epoch 16 iteration 0155/0188: training loss 0.251; learning rate 0.000238
Epoch 16 iteration 0156/0188: training loss 0.251; learning rate 0.000237
Epoch 16 iteration 0157/0188: training loss 0.251; learning rate 0.000237
Epoch 16 iteration 0158/0188: training loss 0.251; learning rate 0.000237
Epoch 16 iteration 0159/0188: training loss 0.251; learning rate 0.000237
Epoch 16 iteration 0160/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0161/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0162/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0163/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0164/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0165/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0166/0188: training loss 0.252; learning rate 0.000237
Epoch 16 iteration 0167/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0168/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0169/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0170/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0171/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0172/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0173/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0174/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0175/0188: training loss 0.253; learning rate 0.000236
Epoch 16 iteration 0176/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0177/0188: training loss 0.252; learning rate 0.000236
Epoch 16 iteration 0178/0188: training loss 0.253; learning rate 0.000236
Epoch 16 iteration 0179/0188: training loss 0.253; learning rate 0.000235
Epoch 16 iteration 0180/0188: training loss 0.253; learning rate 0.000235
Epoch 16 iteration 0181/0188: training loss 0.253; learning rate 0.000235
Epoch 16 iteration 0182/0188: training loss 0.254; learning rate 0.000235
Epoch 16 iteration 0183/0188: training loss 0.255; learning rate 0.000235
Epoch 16 iteration 0184/0188: training loss 0.255; learning rate 0.000235
Epoch 16 iteration 0185/0188: training loss 0.255; learning rate 0.000235
Epoch 16 iteration 0186/0188: training loss 0.255; learning rate 0.000235
Epoch 16 validation pixAcc: 0.335, mIoU: 0.203
Epoch 17 iteration 0001/0187: training loss 0.209; learning rate 0.000235
Epoch 17 iteration 0002/0187: training loss 0.214; learning rate 0.000235
Epoch 17 iteration 0003/0187: training loss 0.201; learning rate 0.000234
Epoch 17 iteration 0004/0187: training loss 0.206; learning rate 0.000234
Epoch 17 iteration 0005/0187: training loss 0.208; learning rate 0.000234
Epoch 17 iteration 0006/0187: training loss 0.212; learning rate 0.000234
Epoch 17 iteration 0007/0187: training loss 0.211; learning rate 0.000234
Epoch 17 iteration 0008/0187: training loss 0.215; learning rate 0.000234
Epoch 17 iteration 0009/0187: training loss 0.222; learning rate 0.000234
Epoch 17 iteration 0010/0187: training loss 0.222; learning rate 0.000234
Epoch 17 iteration 0011/0187: training loss 0.222; learning rate 0.000234
Epoch 17 iteration 0012/0187: training loss 0.221; learning rate 0.000234
Epoch 17 iteration 0013/0187: training loss 0.230; learning rate 0.000234
Epoch 17 iteration 0014/0187: training loss 0.234; learning rate 0.000234
Epoch 17 iteration 0015/0187: training loss 0.232; learning rate 0.000233
Epoch 17 iteration 0016/0187: training loss 0.241; learning rate 0.000233
Epoch 17 iteration 0017/0187: training loss 0.241; learning rate 0.000233
Epoch 17 iteration 0018/0187: training loss 0.241; learning rate 0.000233
Epoch 17 iteration 0019/0187: training loss 0.241; learning rate 0.000233
Epoch 17 iteration 0020/0187: training loss 0.243; learning rate 0.000233
Epoch 17 iteration 0021/0187: training loss 0.243; learning rate 0.000233
Epoch 17 iteration 0022/0187: training loss 0.240; learning rate 0.000233
Epoch 17 iteration 0023/0187: training loss 0.238; learning rate 0.000233
Epoch 17 iteration 0024/0187: training loss 0.240; learning rate 0.000233
Epoch 17 iteration 0025/0187: training loss 0.243; learning rate 0.000233
Epoch 17 iteration 0026/0187: training loss 0.245; learning rate 0.000232
Epoch 17 iteration 0027/0187: training loss 0.244; learning rate 0.000232
Epoch 17 iteration 0028/0187: training loss 0.243; learning rate 0.000232
Epoch 17 iteration 0029/0187: training loss 0.243; learning rate 0.000232
Epoch 17 iteration 0030/0187: training loss 0.242; learning rate 0.000232
Epoch 17 iteration 0031/0187: training loss 0.242; learning rate 0.000232
Epoch 17 iteration 0032/0187: training loss 0.241; learning rate 0.000232
Epoch 17 iteration 0033/0187: training loss 0.240; learning rate 0.000232
Epoch 17 iteration 0034/0187: training loss 0.242; learning rate 0.000232
Epoch 17 iteration 0035/0187: training loss 0.242; learning rate 0.000232
Epoch 17 iteration 0036/0187: training loss 0.244; learning rate 0.000232
Epoch 17 iteration 0037/0187: training loss 0.243; learning rate 0.000231
Epoch 17 iteration 0038/0187: training loss 0.242; learning rate 0.000231
Epoch 17 iteration 0039/0187: training loss 0.243; learning rate 0.000231
Epoch 17 iteration 0040/0187: training loss 0.242; learning rate 0.000231
Epoch 17 iteration 0041/0187: training loss 0.241; learning rate 0.000231
Epoch 17 iteration 0042/0187: training loss 0.241; learning rate 0.000231
Epoch 17 iteration 0043/0187: training loss 0.240; learning rate 0.000231
Epoch 17 iteration 0044/0187: training loss 0.241; learning rate 0.000231
Epoch 17 iteration 0045/0187: training loss 0.240; learning rate 0.000231
Epoch 17 iteration 0046/0187: training loss 0.240; learning rate 0.000231
Epoch 17 iteration 0047/0187: training loss 0.240; learning rate 0.000231
Epoch 17 iteration 0048/0187: training loss 0.240; learning rate 0.000231
Epoch 17 iteration 0049/0187: training loss 0.239; learning rate 0.000230
Epoch 17 iteration 0050/0187: training loss 0.238; learning rate 0.000230
Epoch 17 iteration 0051/0187: training loss 0.238; learning rate 0.000230
Epoch 17 iteration 0052/0187: training loss 0.237; learning rate 0.000230
Epoch 17 iteration 0053/0187: training loss 0.237; learning rate 0.000230
Epoch 17 iteration 0054/0187: training loss 0.237; learning rate 0.000230
Epoch 17 iteration 0055/0187: training loss 0.236; learning rate 0.000230
Epoch 17 iteration 0056/0187: training loss 0.236; learning rate 0.000230
Epoch 17 iteration 0057/0187: training loss 0.236; learning rate 0.000230
Epoch 17 iteration 0058/0187: training loss 0.236; learning rate 0.000230
Epoch 17 iteration 0059/0187: training loss 0.236; learning rate 0.000230
Epoch 17 iteration 0060/0187: training loss 0.237; learning rate 0.000229
Epoch 17 iteration 0061/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0062/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0063/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0064/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0065/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0066/0187: training loss 0.239; learning rate 0.000229
Epoch 17 iteration 0067/0187: training loss 0.240; learning rate 0.000229
Epoch 17 iteration 0068/0187: training loss 0.242; learning rate 0.000229
Epoch 17 iteration 0069/0187: training loss 0.242; learning rate 0.000229
Epoch 17 iteration 0070/0187: training loss 0.242; learning rate 0.000229
Epoch 17 iteration 0071/0187: training loss 0.242; learning rate 0.000229
Epoch 17 iteration 0072/0187: training loss 0.242; learning rate 0.000228
Epoch 17 iteration 0073/0187: training loss 0.241; learning rate 0.000228
Epoch 17 iteration 0074/0187: training loss 0.240; learning rate 0.000228
Epoch 17 iteration 0075/0187: training loss 0.241; learning rate 0.000228
Epoch 17 iteration 0076/0187: training loss 0.241; learning rate 0.000228
Epoch 17 iteration 0077/0187: training loss 0.240; learning rate 0.000228
Epoch 17 iteration 0078/0187: training loss 0.240; learning rate 0.000228
Epoch 17 iteration 0079/0187: training loss 0.240; learning rate 0.000228
Epoch 17 iteration 0080/0187: training loss 0.240; learning rate 0.000228
Epoch 17 iteration 0081/0187: training loss 0.241; learning rate 0.000228
Epoch 17 iteration 0082/0187: training loss 0.241; learning rate 0.000228
Epoch 17 iteration 0083/0187: training loss 0.241; learning rate 0.000227
Epoch 17 iteration 0084/0187: training loss 0.242; learning rate 0.000227
Epoch 17 iteration 0085/0187: training loss 0.242; learning rate 0.000227
Epoch 17 iteration 0086/0187: training loss 0.242; learning rate 0.000227
Epoch 17 iteration 0087/0187: training loss 0.241; learning rate 0.000227
Epoch 17 iteration 0088/0187: training loss 0.241; learning rate 0.000227
Epoch 17 iteration 0089/0187: training loss 0.241; learning rate 0.000227
Epoch 17 iteration 0090/0187: training loss 0.241; learning rate 0.000227
Epoch 17 iteration 0091/0187: training loss 0.242; learning rate 0.000227
Epoch 17 iteration 0092/0187: training loss 0.243; learning rate 0.000227
Epoch 17 iteration 0093/0187: training loss 0.243; learning rate 0.000227
Epoch 17 iteration 0094/0187: training loss 0.243; learning rate 0.000227
Epoch 17 iteration 0095/0187: training loss 0.244; learning rate 0.000226
Epoch 17 iteration 0096/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0097/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0098/0187: training loss 0.244; learning rate 0.000226
Epoch 17 iteration 0099/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0100/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0101/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0102/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0103/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0104/0187: training loss 0.243; learning rate 0.000226
Epoch 17 iteration 0105/0187: training loss 0.242; learning rate 0.000226
Epoch 17 iteration 0106/0187: training loss 0.242; learning rate 0.000225
Epoch 17 iteration 0107/0187: training loss 0.242; learning rate 0.000225
Epoch 17 iteration 0108/0187: training loss 0.243; learning rate 0.000225
Epoch 17 iteration 0109/0187: training loss 0.243; learning rate 0.000225
Epoch 17 iteration 0110/0187: training loss 0.243; learning rate 0.000225
Epoch 17 iteration 0111/0187: training loss 0.243; learning rate 0.000225
Epoch 17 iteration 0112/0187: training loss 0.244; learning rate 0.000225
Epoch 17 iteration 0113/0187: training loss 0.244; learning rate 0.000225
Epoch 17 iteration 0114/0187: training loss 0.244; learning rate 0.000225
Epoch 17 iteration 0115/0187: training loss 0.244; learning rate 0.000225
Epoch 17 iteration 0116/0187: training loss 0.244; learning rate 0.000225
Epoch 17 iteration 0117/0187: training loss 0.244; learning rate 0.000224
Epoch 17 iteration 0118/0187: training loss 0.244; learning rate 0.000224
Epoch 17 iteration 0119/0187: training loss 0.244; learning rate 0.000224
Epoch 17 iteration 0120/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0121/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0122/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0123/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0124/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0125/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0126/0187: training loss 0.246; learning rate 0.000224
Epoch 17 iteration 0127/0187: training loss 0.246; learning rate 0.000224
Epoch 17 iteration 0128/0187: training loss 0.245; learning rate 0.000224
Epoch 17 iteration 0129/0187: training loss 0.245; learning rate 0.000223
Epoch 17 iteration 0130/0187: training loss 0.246; learning rate 0.000223
Epoch 17 iteration 0131/0187: training loss 0.246; learning rate 0.000223
Epoch 17 iteration 0132/0187: training loss 0.246; learning rate 0.000223
Epoch 17 iteration 0133/0187: training loss 0.246; learning rate 0.000223
Epoch 17 iteration 0134/0187: training loss 0.246; learning rate 0.000223
Epoch 17 iteration 0135/0187: training loss 0.247; learning rate 0.000223
Epoch 17 iteration 0136/0187: training loss 0.247; learning rate 0.000223
Epoch 17 iteration 0137/0187: training loss 0.247; learning rate 0.000223
Epoch 17 iteration 0138/0187: training loss 0.248; learning rate 0.000223
Epoch 17 iteration 0139/0187: training loss 0.249; learning rate 0.000223
Epoch 17 iteration 0140/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0141/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0142/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0143/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0144/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0145/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0146/0187: training loss 0.249; learning rate 0.000222
Epoch 17 iteration 0147/0187: training loss 0.248; learning rate 0.000222
Epoch 17 iteration 0148/0187: training loss 0.248; learning rate 0.000222
Epoch 17 iteration 0149/0187: training loss 0.248; learning rate 0.000222
Epoch 17 iteration 0150/0187: training loss 0.248; learning rate 0.000222
Epoch 17 iteration 0151/0187: training loss 0.248; learning rate 0.000222
Epoch 17 iteration 0152/0187: training loss 0.248; learning rate 0.000221
Epoch 17 iteration 0153/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0154/0187: training loss 0.248; learning rate 0.000221
Epoch 17 iteration 0155/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0156/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0157/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0158/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0159/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0160/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0161/0187: training loss 0.250; learning rate 0.000221
Epoch 17 iteration 0162/0187: training loss 0.249; learning rate 0.000221
Epoch 17 iteration 0163/0187: training loss 0.249; learning rate 0.000220
Epoch 17 iteration 0164/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0165/0187: training loss 0.249; learning rate 0.000220
Epoch 17 iteration 0166/0187: training loss 0.249; learning rate 0.000220
Epoch 17 iteration 0167/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0168/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0169/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0170/0187: training loss 0.249; learning rate 0.000220
Epoch 17 iteration 0171/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0172/0187: training loss 0.249; learning rate 0.000220
Epoch 17 iteration 0173/0187: training loss 0.250; learning rate 0.000220
Epoch 17 iteration 0174/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0175/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0176/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0177/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0178/0187: training loss 0.248; learning rate 0.000219
Epoch 17 iteration 0179/0187: training loss 0.248; learning rate 0.000219
Epoch 17 iteration 0180/0187: training loss 0.248; learning rate 0.000219
Epoch 17 iteration 0181/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0182/0187: training loss 0.248; learning rate 0.000219
Epoch 17 iteration 0183/0187: training loss 0.248; learning rate 0.000219
Epoch 17 iteration 0184/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0185/0187: training loss 0.249; learning rate 0.000219
Epoch 17 iteration 0186/0187: training loss 0.249; learning rate 0.000218
Epoch 17 iteration 0187/0187: training loss 0.250; learning rate 0.000218
Epoch 17 validation pixAcc: 0.335, mIoU: 0.202
Epoch 18 iteration 0001/0187: training loss 0.242; learning rate 0.000218
Epoch 18 iteration 0002/0187: training loss 0.243; learning rate 0.000218
Epoch 18 iteration 0003/0187: training loss 0.242; learning rate 0.000218
Epoch 18 iteration 0004/0187: training loss 0.241; learning rate 0.000218
Epoch 18 iteration 0005/0187: training loss 0.243; learning rate 0.000218
Epoch 18 iteration 0006/0187: training loss 0.239; learning rate 0.000218
Epoch 18 iteration 0007/0187: training loss 0.249; learning rate 0.000218
Epoch 18 iteration 0008/0187: training loss 0.247; learning rate 0.000218
Epoch 18 iteration 0009/0187: training loss 0.249; learning rate 0.000217
Epoch 18 iteration 0010/0187: training loss 0.251; learning rate 0.000217
Epoch 18 iteration 0011/0187: training loss 0.254; learning rate 0.000217
Epoch 18 iteration 0012/0187: training loss 0.250; learning rate 0.000217
Epoch 18 iteration 0013/0187: training loss 0.252; learning rate 0.000217
Epoch 18 iteration 0014/0187: training loss 0.253; learning rate 0.000217
Epoch 18 iteration 0015/0187: training loss 0.251; learning rate 0.000217
Epoch 18 iteration 0016/0187: training loss 0.251; learning rate 0.000217
Epoch 18 iteration 0017/0187: training loss 0.252; learning rate 0.000217
Epoch 18 iteration 0018/0187: training loss 0.253; learning rate 0.000217
Epoch 18 iteration 0019/0187: training loss 0.258; learning rate 0.000217
Epoch 18 iteration 0020/0187: training loss 0.258; learning rate 0.000216
Epoch 18 iteration 0021/0187: training loss 0.257; learning rate 0.000216
Epoch 18 iteration 0022/0187: training loss 0.255; learning rate 0.000216
Epoch 18 iteration 0023/0187: training loss 0.252; learning rate 0.000216
Epoch 18 iteration 0024/0187: training loss 0.257; learning rate 0.000216
Epoch 18 iteration 0025/0187: training loss 0.260; learning rate 0.000216
Epoch 18 iteration 0026/0187: training loss 0.262; learning rate 0.000216
Epoch 18 iteration 0027/0187: training loss 0.259; learning rate 0.000216
Epoch 18 iteration 0028/0187: training loss 0.258; learning rate 0.000216
Epoch 18 iteration 0029/0187: training loss 0.257; learning rate 0.000216
Epoch 18 iteration 0030/0187: training loss 0.257; learning rate 0.000216
Epoch 18 iteration 0031/0187: training loss 0.257; learning rate 0.000216
Epoch 18 iteration 0032/0187: training loss 0.257; learning rate 0.000215
Epoch 18 iteration 0033/0187: training loss 0.256; learning rate 0.000215
Epoch 18 iteration 0034/0187: training loss 0.256; learning rate 0.000215
Epoch 18 iteration 0035/0187: training loss 0.257; learning rate 0.000215
Epoch 18 iteration 0036/0187: training loss 0.258; learning rate 0.000215
Epoch 18 iteration 0037/0187: training loss 0.258; learning rate 0.000215
Epoch 18 iteration 0038/0187: training loss 0.258; learning rate 0.000215
Epoch 18 iteration 0039/0187: training loss 0.257; learning rate 0.000215
Epoch 18 iteration 0040/0187: training loss 0.256; learning rate 0.000215
Epoch 18 iteration 0041/0187: training loss 0.254; learning rate 0.000215
Epoch 18 iteration 0042/0187: training loss 0.253; learning rate 0.000215
Epoch 18 iteration 0043/0187: training loss 0.253; learning rate 0.000214
Epoch 18 iteration 0044/0187: training loss 0.252; learning rate 0.000214
Epoch 18 iteration 0045/0187: training loss 0.250; learning rate 0.000214
Epoch 18 iteration 0046/0187: training loss 0.249; learning rate 0.000214
Epoch 18 iteration 0047/0187: training loss 0.249; learning rate 0.000214
Epoch 18 iteration 0048/0187: training loss 0.248; learning rate 0.000214
Epoch 18 iteration 0049/0187: training loss 0.249; learning rate 0.000214
Epoch 18 iteration 0050/0187: training loss 0.250; learning rate 0.000214
Epoch 18 iteration 0051/0187: training loss 0.250; learning rate 0.000214
Epoch 18 iteration 0052/0187: training loss 0.252; learning rate 0.000214
Epoch 18 iteration 0053/0187: training loss 0.252; learning rate 0.000214
Epoch 18 iteration 0054/0187: training loss 0.253; learning rate 0.000214
Epoch 18 iteration 0055/0187: training loss 0.252; learning rate 0.000213
Epoch 18 iteration 0056/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0057/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0058/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0059/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0060/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0061/0187: training loss 0.254; learning rate 0.000213
Epoch 18 iteration 0062/0187: training loss 0.253; learning rate 0.000213
Epoch 18 iteration 0063/0187: training loss 0.254; learning rate 0.000213
Epoch 18 iteration 0064/0187: training loss 0.254; learning rate 0.000213
Epoch 18 iteration 0065/0187: training loss 0.254; learning rate 0.000213
Epoch 18 iteration 0066/0187: training loss 0.253; learning rate 0.000212
Epoch 18 iteration 0067/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0068/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0069/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0070/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0071/0187: training loss 0.252; learning rate 0.000212
Epoch 18 iteration 0072/0187: training loss 0.252; learning rate 0.000212
Epoch 18 iteration 0073/0187: training loss 0.252; learning rate 0.000212
Epoch 18 iteration 0074/0187: training loss 0.252; learning rate 0.000212
Epoch 18 iteration 0075/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0076/0187: training loss 0.251; learning rate 0.000212
Epoch 18 iteration 0077/0187: training loss 0.251; learning rate 0.000211
Epoch 18 iteration 0078/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0079/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0080/0187: training loss 0.249; learning rate 0.000211
Epoch 18 iteration 0081/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0082/0187: training loss 0.251; learning rate 0.000211
Epoch 18 iteration 0083/0187: training loss 0.251; learning rate 0.000211
Epoch 18 iteration 0084/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0085/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0086/0187: training loss 0.249; learning rate 0.000211
Epoch 18 iteration 0087/0187: training loss 0.249; learning rate 0.000211
Epoch 18 iteration 0088/0187: training loss 0.250; learning rate 0.000211
Epoch 18 iteration 0089/0187: training loss nan; learning rate 0.000210
Epoch 18 iteration 0090/0187: training loss nan; learning rate 0.000210
Epoch 18 iteration 0091/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0092/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0093/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0094/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0095/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0096/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0097/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0098/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0099/0188: training loss nan; learning rate 0.000210
Epoch 18 iteration 0100/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0101/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0102/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0103/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0104/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0105/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0106/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0107/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0108/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0109/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0110/0188: training loss nan; learning rate 0.000209
Epoch 18 iteration 0111/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0112/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0113/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0114/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0115/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0116/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0117/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0118/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0119/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0120/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0121/0188: training loss nan; learning rate 0.000208
Epoch 18 iteration 0122/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0123/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0124/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0125/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0126/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0127/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0128/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0129/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0130/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0131/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0132/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0133/0188: training loss nan; learning rate 0.000207
Epoch 18 iteration 0134/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0135/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0136/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0137/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0138/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0139/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0140/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0141/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0142/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0143/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0144/0188: training loss nan; learning rate 0.000206
Epoch 18 iteration 0145/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0146/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0147/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0148/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0149/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0150/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0151/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0152/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0153/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0154/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0155/0188: training loss nan; learning rate 0.000205
Epoch 18 iteration 0156/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0157/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0158/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0159/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0160/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0161/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0162/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0163/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0164/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0165/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0166/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0167/0188: training loss nan; learning rate 0.000204
Epoch 18 iteration 0168/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0169/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0170/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0171/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0172/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0173/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0174/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0175/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0176/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0177/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0178/0188: training loss nan; learning rate 0.000203
Epoch 18 iteration 0179/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0180/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0181/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0182/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0183/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0184/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0185/0188: training loss nan; learning rate 0.000202
Epoch 18 iteration 0186/0188: training loss nan; learning rate 0.000202
Epoch 18 validation pixAcc: 0.336, mIoU: 0.201
Epoch 19 iteration 0001/0187: training loss 0.192; learning rate 0.000202
Epoch 19 iteration 0002/0187: training loss 0.251; learning rate 0.000202
Epoch 19 iteration 0003/0187: training loss 0.258; learning rate 0.000201
Epoch 19 iteration 0004/0187: training loss 0.265; learning rate 0.000201
Epoch 19 iteration 0005/0187: training loss 0.254; learning rate 0.000201
Epoch 19 iteration 0006/0187: training loss 0.249; learning rate 0.000201
Epoch 19 iteration 0007/0187: training loss 0.252; learning rate 0.000201
Epoch 19 iteration 0008/0187: training loss 0.252; learning rate 0.000201
Epoch 19 iteration 0009/0187: training loss 0.254; learning rate 0.000201
Epoch 19 iteration 0010/0187: training loss 0.258; learning rate 0.000201
Epoch 19 iteration 0011/0187: training loss 0.256; learning rate 0.000201
Epoch 19 iteration 0012/0187: training loss 0.257; learning rate 0.000201
Epoch 19 iteration 0013/0187: training loss 0.251; learning rate 0.000201
Epoch 19 iteration 0014/0187: training loss 0.247; learning rate 0.000200
Epoch 19 iteration 0015/0187: training loss 0.243; learning rate 0.000200
Epoch 19 iteration 0016/0187: training loss 0.247; learning rate 0.000200
Epoch 19 iteration 0017/0187: training loss 0.252; learning rate 0.000200
Epoch 19 iteration 0018/0187: training loss 0.249; learning rate 0.000200
Epoch 19 iteration 0019/0187: training loss 0.250; learning rate 0.000200
Epoch 19 iteration 0020/0187: training loss 0.248; learning rate 0.000200
Epoch 19 iteration 0021/0187: training loss 0.248; learning rate 0.000200
Epoch 19 iteration 0022/0187: training loss 0.248; learning rate 0.000200
Epoch 19 iteration 0023/0187: training loss 0.247; learning rate 0.000200
Epoch 19 iteration 0024/0187: training loss 0.247; learning rate 0.000200
Epoch 19 iteration 0025/0187: training loss 0.245; learning rate 0.000200
Epoch 19 iteration 0026/0187: training loss 0.247; learning rate 0.000199
Epoch 19 iteration 0027/0187: training loss 0.247; learning rate 0.000199
Epoch 19 iteration 0028/0187: training loss 0.247; learning rate 0.000199
Epoch 19 iteration 0029/0187: training loss 0.246; learning rate 0.000199
Epoch 19 iteration 0030/0187: training loss 0.248; learning rate 0.000199
Epoch 19 iteration 0031/0187: training loss 0.249; learning rate 0.000199
Epoch 19 iteration 0032/0187: training loss 0.248; learning rate 0.000199
Epoch 19 iteration 0033/0187: training loss 0.250; learning rate 0.000199
Epoch 19 iteration 0034/0187: training loss 0.250; learning rate 0.000199
Epoch 19 iteration 0035/0187: training loss 0.247; learning rate 0.000199
Epoch 19 iteration 0036/0187: training loss 0.247; learning rate 0.000199
Epoch 19 iteration 0037/0187: training loss 0.247; learning rate 0.000198
Epoch 19 iteration 0038/0187: training loss 0.247; learning rate 0.000198
Epoch 19 iteration 0039/0187: training loss 0.249; learning rate 0.000198
Epoch 19 iteration 0040/0187: training loss 0.248; learning rate 0.000198
Epoch 19 iteration 0041/0187: training loss 0.248; learning rate 0.000198
Epoch 19 iteration 0042/0187: training loss 0.250; learning rate 0.000198
Epoch 19 iteration 0043/0187: training loss 0.250; learning rate 0.000198
Epoch 19 iteration 0044/0187: training loss 0.250; learning rate 0.000198
Epoch 19 iteration 0045/0187: training loss 0.250; learning rate 0.000198
Epoch 19 iteration 0046/0187: training loss 0.251; learning rate 0.000198
Epoch 19 iteration 0047/0187: training loss 0.250; learning rate 0.000198
Epoch 19 iteration 0048/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0049/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0050/0187: training loss 0.249; learning rate 0.000197
Epoch 19 iteration 0051/0187: training loss 0.248; learning rate 0.000197
Epoch 19 iteration 0052/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0053/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0054/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0055/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0056/0187: training loss 0.251; learning rate 0.000197
Epoch 19 iteration 0057/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0058/0187: training loss 0.250; learning rate 0.000197
Epoch 19 iteration 0059/0187: training loss 0.251; learning rate 0.000196
Epoch 19 iteration 0060/0187: training loss 0.250; learning rate 0.000196
Epoch 19 iteration 0061/0187: training loss 0.250; learning rate 0.000196
Epoch 19 iteration 0062/0187: training loss 0.249; learning rate 0.000196
Epoch 19 iteration 0063/0187: training loss 0.248; learning rate 0.000196
Epoch 19 iteration 0064/0187: training loss 0.248; learning rate 0.000196
Epoch 19 iteration 0065/0187: training loss 0.249; learning rate 0.000196
Epoch 19 iteration 0066/0187: training loss 0.248; learning rate 0.000196
Epoch 19 iteration 0067/0187: training loss 0.249; learning rate 0.000196
Epoch 19 iteration 0068/0187: training loss 0.249; learning rate 0.000196
Epoch 19 iteration 0069/0187: training loss 0.249; learning rate 0.000196
Epoch 19 iteration 0070/0187: training loss 0.250; learning rate 0.000196
Epoch 19 iteration 0071/0187: training loss 0.252; learning rate 0.000195
Epoch 19 iteration 0072/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0073/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0074/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0075/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0076/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0077/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0078/0187: training loss 0.251; learning rate 0.000195
Epoch 19 iteration 0079/0187: training loss 0.250; learning rate 0.000195
Epoch 19 iteration 0080/0187: training loss 0.250; learning rate 0.000195
Epoch 19 iteration 0081/0187: training loss 0.249; learning rate 0.000195
Epoch 19 iteration 0082/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0083/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0084/0187: training loss 0.250; learning rate 0.000194
Epoch 19 iteration 0085/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0086/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0087/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0088/0187: training loss 0.248; learning rate 0.000194
Epoch 19 iteration 0089/0187: training loss 0.250; learning rate 0.000194
Epoch 19 iteration 0090/0187: training loss 0.249; learning rate 0.000194
Epoch 19 iteration 0091/0187: training loss 0.248; learning rate 0.000194
Epoch 19 iteration 0092/0187: training loss 0.248; learning rate 0.000194
Epoch 19 iteration 0093/0187: training loss 0.248; learning rate 0.000193
Epoch 19 iteration 0094/0187: training loss 0.248; learning rate 0.000193
Epoch 19 iteration 0095/0187: training loss 0.247; learning rate 0.000193
Epoch 19 iteration 0096/0187: training loss 0.248; learning rate 0.000193
Epoch 19 iteration 0097/0187: training loss 0.248; learning rate 0.000193
Epoch 19 iteration 0098/0187: training loss 0.249; learning rate 0.000193
Epoch 19 iteration 0099/0187: training loss 0.249; learning rate 0.000193
Epoch 19 iteration 0100/0187: training loss 0.249; learning rate 0.000193
Epoch 19 iteration 0101/0187: training loss 0.249; learning rate 0.000193
Epoch 19 iteration 0102/0187: training loss 0.250; learning rate 0.000193
Epoch 19 iteration 0103/0187: training loss 0.250; learning rate 0.000193
Epoch 19 iteration 0104/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0105/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0106/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0107/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0108/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0109/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0110/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0111/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0112/0187: training loss 0.249; learning rate 0.000192
Epoch 19 iteration 0113/0187: training loss 0.249; learning rate 0.000192
Epoch 19 iteration 0114/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0115/0187: training loss 0.250; learning rate 0.000192
Epoch 19 iteration 0116/0187: training loss 0.250; learning rate 0.000191
Epoch 19 iteration 0117/0187: training loss 0.249; learning rate 0.000191
Epoch 19 iteration 0118/0187: training loss 0.249; learning rate 0.000191
Epoch 19 iteration 0119/0187: training loss 0.250; learning rate 0.000191
Epoch 19 iteration 0120/0187: training loss 0.249; learning rate 0.000191
Epoch 19 iteration 0121/0187: training loss 0.248; learning rate 0.000191
Epoch 19 iteration 0122/0187: training loss 0.247; learning rate 0.000191
Epoch 19 iteration 0123/0187: training loss 0.248; learning rate 0.000191
Epoch 19 iteration 0124/0187: training loss 0.248; learning rate 0.000191
Epoch 19 iteration 0125/0187: training loss 0.248; learning rate 0.000191
Epoch 19 iteration 0126/0187: training loss 0.247; learning rate 0.000191
Epoch 19 iteration 0127/0187: training loss 0.247; learning rate 0.000190
Epoch 19 iteration 0128/0187: training loss 0.246; learning rate 0.000190
Epoch 19 iteration 0129/0187: training loss 0.246; learning rate 0.000190
Epoch 19 iteration 0130/0187: training loss 0.247; learning rate 0.000190
Epoch 19 iteration 0131/0187: training loss 0.248; learning rate 0.000190
Epoch 19 iteration 0132/0187: training loss 0.248; learning rate 0.000190
Epoch 19 iteration 0133/0187: training loss 0.248; learning rate 0.000190
Epoch 19 iteration 0134/0187: training loss 0.249; learning rate 0.000190
Epoch 19 iteration 0135/0187: training loss 0.249; learning rate 0.000190
Epoch 19 iteration 0136/0187: training loss 0.249; learning rate 0.000190
Epoch 19 iteration 0137/0187: training loss 0.249; learning rate 0.000190
Epoch 19 iteration 0138/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0139/0187: training loss 0.248; learning rate 0.000189
Epoch 19 iteration 0140/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0141/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0142/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0143/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0144/0187: training loss 0.249; learning rate 0.000189
Epoch 19 iteration 0145/0187: training loss 0.250; learning rate 0.000189
Epoch 19 iteration 0146/0187: training loss 0.250; learning rate 0.000189
Epoch 19 iteration 0147/0187: training loss 0.250; learning rate 0.000189
Epoch 19 iteration 0148/0187: training loss 0.250; learning rate 0.000189
Epoch 19 iteration 0149/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0150/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0151/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0152/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0153/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0154/0187: training loss 0.249; learning rate 0.000188
Epoch 19 iteration 0155/0187: training loss 0.249; learning rate 0.000188
Epoch 19 iteration 0156/0187: training loss 0.249; learning rate 0.000188
Epoch 19 iteration 0157/0187: training loss 0.249; learning rate 0.000188
Epoch 19 iteration 0158/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0159/0187: training loss 0.250; learning rate 0.000188
Epoch 19 iteration 0160/0187: training loss 0.250; learning rate 0.000187
Epoch 19 iteration 0161/0187: training loss 0.250; learning rate 0.000187
Epoch 19 iteration 0162/0187: training loss 0.250; learning rate 0.000187
Epoch 19 iteration 0163/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0164/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0165/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0166/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0167/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0168/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0169/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0170/0187: training loss 0.251; learning rate 0.000187
Epoch 19 iteration 0171/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0172/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0173/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0174/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0175/0187: training loss 0.252; learning rate 0.000186
Epoch 19 iteration 0176/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0177/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0178/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0179/0187: training loss 0.252; learning rate 0.000186
Epoch 19 iteration 0180/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0181/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0182/0187: training loss 0.251; learning rate 0.000186
Epoch 19 iteration 0183/0187: training loss 0.252; learning rate 0.000185
Epoch 19 iteration 0184/0187: training loss 0.252; learning rate 0.000185
Epoch 19 iteration 0185/0187: training loss 0.252; learning rate 0.000185
Epoch 19 iteration 0186/0187: training loss 0.252; learning rate 0.000185
Epoch 19 iteration 0187/0187: training loss 0.252; learning rate 0.000185
Epoch 19 validation pixAcc: 0.334, mIoU: 0.202
Epoch 20 iteration 0001/0187: training loss 0.217; learning rate 0.000185
Epoch 20 iteration 0002/0187: training loss 0.203; learning rate 0.000185
Epoch 20 iteration 0003/0187: training loss 0.197; learning rate 0.000185
Epoch 20 iteration 0004/0187: training loss 0.209; learning rate 0.000185
Epoch 20 iteration 0005/0187: training loss 0.226; learning rate 0.000185
Epoch 20 iteration 0006/0187: training loss 0.231; learning rate 0.000184
Epoch 20 iteration 0007/0187: training loss 0.230; learning rate 0.000184
Epoch 20 iteration 0008/0187: training loss 0.230; learning rate 0.000184
Epoch 20 iteration 0009/0187: training loss 0.226; learning rate 0.000184
Epoch 20 iteration 0010/0187: training loss 0.223; learning rate 0.000184
Epoch 20 iteration 0011/0187: training loss 0.222; learning rate 0.000184
Epoch 20 iteration 0012/0187: training loss 0.227; learning rate 0.000184
Epoch 20 iteration 0013/0187: training loss 0.229; learning rate 0.000184
Epoch 20 iteration 0014/0187: training loss 0.228; learning rate 0.000184
Epoch 20 iteration 0015/0187: training loss 0.226; learning rate 0.000184
Epoch 20 iteration 0016/0187: training loss 0.231; learning rate 0.000184
Epoch 20 iteration 0017/0187: training loss 0.233; learning rate 0.000183
Epoch 20 iteration 0018/0187: training loss 0.236; learning rate 0.000183
Epoch 20 iteration 0019/0187: training loss 0.233; learning rate 0.000183
Epoch 20 iteration 0020/0187: training loss 0.235; learning rate 0.000183
Epoch 20 iteration 0021/0187: training loss 0.238; learning rate 0.000183
Epoch 20 iteration 0022/0187: training loss 0.252; learning rate 0.000183
Epoch 20 iteration 0023/0187: training loss 0.254; learning rate 0.000183
Epoch 20 iteration 0024/0187: training loss 0.254; learning rate 0.000183
Epoch 20 iteration 0025/0187: training loss 0.256; learning rate 0.000183
Epoch 20 iteration 0026/0187: training loss 0.255; learning rate 0.000183
Epoch 20 iteration 0027/0187: training loss 0.255; learning rate 0.000183
Epoch 20 iteration 0028/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0029/0187: training loss 0.257; learning rate 0.000182
Epoch 20 iteration 0030/0187: training loss 0.254; learning rate 0.000182
Epoch 20 iteration 0031/0187: training loss 0.254; learning rate 0.000182
Epoch 20 iteration 0032/0187: training loss 0.254; learning rate 0.000182
Epoch 20 iteration 0033/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0034/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0035/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0036/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0037/0187: training loss 0.253; learning rate 0.000182
Epoch 20 iteration 0038/0187: training loss 0.252; learning rate 0.000182
Epoch 20 iteration 0039/0187: training loss 0.254; learning rate 0.000181
Epoch 20 iteration 0040/0187: training loss 0.255; learning rate 0.000181
Epoch 20 iteration 0041/0187: training loss 0.253; learning rate 0.000181
Epoch 20 iteration 0042/0187: training loss 0.251; learning rate 0.000181
Epoch 20 iteration 0043/0187: training loss 0.251; learning rate 0.000181
Epoch 20 iteration 0044/0187: training loss 0.249; learning rate 0.000181
Epoch 20 iteration 0045/0187: training loss 0.248; learning rate 0.000181
Epoch 20 iteration 0046/0187: training loss 0.250; learning rate 0.000181
Epoch 20 iteration 0047/0187: training loss 0.250; learning rate 0.000181
Epoch 20 iteration 0048/0187: training loss 0.250; learning rate 0.000181
Epoch 20 iteration 0049/0187: training loss 0.252; learning rate 0.000181
Epoch 20 iteration 0050/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0051/0187: training loss 0.251; learning rate 0.000180
Epoch 20 iteration 0052/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0053/0187: training loss 0.253; learning rate 0.000180
Epoch 20 iteration 0054/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0055/0187: training loss 0.254; learning rate 0.000180
Epoch 20 iteration 0056/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0057/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0058/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0059/0187: training loss 0.251; learning rate 0.000180
Epoch 20 iteration 0060/0187: training loss 0.253; learning rate 0.000180
Epoch 20 iteration 0061/0187: training loss 0.252; learning rate 0.000180
Epoch 20 iteration 0062/0187: training loss 0.253; learning rate 0.000179
Epoch 20 iteration 0063/0187: training loss 0.252; learning rate 0.000179
Epoch 20 iteration 0064/0187: training loss 0.252; learning rate 0.000179
Epoch 20 iteration 0065/0187: training loss 0.252; learning rate 0.000179
Epoch 20 iteration 0066/0187: training loss 0.252; learning rate 0.000179
Epoch 20 iteration 0067/0187: training loss 0.253; learning rate 0.000179
Epoch 20 iteration 0068/0187: training loss 0.253; learning rate 0.000179
Epoch 20 iteration 0069/0187: training loss 0.253; learning rate 0.000179
Epoch 20 iteration 0070/0187: training loss 0.254; learning rate 0.000179
Epoch 20 iteration 0071/0187: training loss 0.256; learning rate 0.000179
Epoch 20 iteration 0072/0187: training loss 0.255; learning rate 0.000179
Epoch 20 iteration 0073/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0074/0187: training loss 0.256; learning rate 0.000178
Epoch 20 iteration 0075/0187: training loss 0.256; learning rate 0.000178
Epoch 20 iteration 0076/0187: training loss 0.256; learning rate 0.000178
Epoch 20 iteration 0077/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0078/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0079/0187: training loss 0.256; learning rate 0.000178
Epoch 20 iteration 0080/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0081/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0082/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0083/0187: training loss 0.257; learning rate 0.000178
Epoch 20 iteration 0084/0187: training loss 0.257; learning rate 0.000177
Epoch 20 iteration 0085/0187: training loss 0.256; learning rate 0.000177
Epoch 20 iteration 0086/0187: training loss 0.256; learning rate 0.000177
Epoch 20 iteration 0087/0187: training loss 0.256; learning rate 0.000177
Epoch 20 iteration 0088/0187: training loss 0.255; learning rate 0.000177
Epoch 20 iteration 0089/0187: training loss 0.255; learning rate 0.000177
Epoch 20 iteration 0090/0187: training loss 0.255; learning rate 0.000177
Epoch 20 iteration 0091/0188: training loss 0.255; learning rate 0.000177
Epoch 20 iteration 0092/0188: training loss 0.258; learning rate 0.000177
Epoch 20 iteration 0093/0188: training loss 0.258; learning rate 0.000177
Epoch 20 iteration 0094/0188: training loss 0.259; learning rate 0.000177
Epoch 20 iteration 0095/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0096/0188: training loss 0.258; learning rate 0.000176
Epoch 20 iteration 0097/0188: training loss 0.257; learning rate 0.000176
Epoch 20 iteration 0098/0188: training loss 0.258; learning rate 0.000176
Epoch 20 iteration 0099/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0100/0188: training loss 0.258; learning rate 0.000176
Epoch 20 iteration 0101/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0102/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0103/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0104/0188: training loss 0.259; learning rate 0.000176
Epoch 20 iteration 0105/0188: training loss 0.258; learning rate 0.000176
Epoch 20 iteration 0106/0188: training loss 0.259; learning rate 0.000175
Epoch 20 iteration 0107/0188: training loss 0.260; learning rate 0.000175
Epoch 20 iteration 0108/0188: training loss 0.260; learning rate 0.000175
Epoch 20 iteration 0109/0188: training loss 0.259; learning rate 0.000175
Epoch 20 iteration 0110/0188: training loss 0.260; learning rate 0.000175
Epoch 20 iteration 0111/0188: training loss 0.261; learning rate 0.000175
Epoch 20 iteration 0112/0188: training loss 0.261; learning rate 0.000175
Epoch 20 iteration 0113/0188: training loss 0.261; learning rate 0.000175
Epoch 20 iteration 0114/0188: training loss 0.261; learning rate 0.000175
Epoch 20 iteration 0115/0188: training loss 0.260; learning rate 0.000175
Epoch 20 iteration 0116/0188: training loss 0.259; learning rate 0.000175
Epoch 20 iteration 0117/0188: training loss 0.259; learning rate 0.000174
Epoch 20 iteration 0118/0188: training loss 0.259; learning rate 0.000174
Epoch 20 iteration 0119/0188: training loss 0.259; learning rate 0.000174
Epoch 20 iteration 0120/0188: training loss 0.259; learning rate 0.000174
Epoch 20 iteration 0121/0188: training loss 0.259; learning rate 0.000174
Epoch 20 iteration 0122/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0123/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0124/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0125/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0126/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0127/0188: training loss 0.258; learning rate 0.000174
Epoch 20 iteration 0128/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0129/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0130/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0131/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0132/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0133/0188: training loss 0.258; learning rate 0.000173
Epoch 20 iteration 0134/0188: training loss 0.257; learning rate 0.000173
Epoch 20 iteration 0135/0188: training loss 0.257; learning rate 0.000173
Epoch 20 iteration 0136/0188: training loss 0.257; learning rate 0.000173
Epoch 20 iteration 0137/0188: training loss 0.256; learning rate 0.000173
Epoch 20 iteration 0138/0188: training loss 0.257; learning rate 0.000173
Epoch 20 iteration 0139/0188: training loss 0.257; learning rate 0.000172
Epoch 20 iteration 0140/0188: training loss 0.256; learning rate 0.000172
Epoch 20 iteration 0141/0188: training loss 0.257; learning rate 0.000172
Epoch 20 iteration 0142/0188: training loss 0.256; learning rate 0.000172
Epoch 20 iteration 0143/0188: training loss 0.256; learning rate 0.000172
Epoch 20 iteration 0144/0188: training loss 0.256; learning rate 0.000172
Epoch 20 iteration 0145/0188: training loss 0.256; learning rate 0.000172
Epoch 20 iteration 0146/0188: training loss 0.255; learning rate 0.000172
Epoch 20 iteration 0147/0188: training loss 0.255; learning rate 0.000172
Epoch 20 iteration 0148/0188: training loss 0.255; learning rate 0.000172
Epoch 20 iteration 0149/0188: training loss 0.255; learning rate 0.000172
Epoch 20 iteration 0150/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0151/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0152/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0153/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0154/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0155/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0156/0188: training loss 0.254; learning rate 0.000171
Epoch 20 iteration 0157/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0158/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0159/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0160/0188: training loss 0.255; learning rate 0.000171
Epoch 20 iteration 0161/0188: training loss 0.256; learning rate 0.000170
Epoch 20 iteration 0162/0188: training loss 0.256; learning rate 0.000170
Epoch 20 iteration 0163/0188: training loss 0.256; learning rate 0.000170
Epoch 20 iteration 0164/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0165/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0166/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0167/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0168/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0169/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0170/0188: training loss 0.255; learning rate 0.000170
Epoch 20 iteration 0171/0188: training loss 0.254; learning rate 0.000170
Epoch 20 iteration 0172/0188: training loss 0.255; learning rate 0.000169
Epoch 20 iteration 0173/0188: training loss 0.255; learning rate 0.000169
Epoch 20 iteration 0174/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0175/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0176/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0177/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0178/0188: training loss 0.255; learning rate 0.000169
Epoch 20 iteration 0179/0188: training loss 0.255; learning rate 0.000169
Epoch 20 iteration 0180/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0181/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0182/0188: training loss 0.254; learning rate 0.000169
Epoch 20 iteration 0183/0188: training loss 0.254; learning rate 0.000168
Epoch 20 iteration 0184/0188: training loss 0.254; learning rate 0.000168
Epoch 20 iteration 0185/0188: training loss 0.254; learning rate 0.000168
Epoch 20 iteration 0186/0188: training loss 0.254; learning rate 0.000168
Epoch 20 validation pixAcc: 0.337, mIoU: 0.201
Epoch 21 iteration 0001/0187: training loss 0.221; learning rate 0.000168
Epoch 21 iteration 0002/0187: training loss 0.243; learning rate 0.000168
Epoch 21 iteration 0003/0187: training loss 0.261; learning rate 0.000168
Epoch 21 iteration 0004/0187: training loss 0.241; learning rate 0.000168
Epoch 21 iteration 0005/0187: training loss 0.249; learning rate 0.000168
Epoch 21 iteration 0006/0187: training loss 0.250; learning rate 0.000168
Epoch 21 iteration 0007/0187: training loss 0.241; learning rate 0.000167
Epoch 21 iteration 0008/0187: training loss 0.243; learning rate 0.000167
Epoch 21 iteration 0009/0187: training loss 0.243; learning rate 0.000167
Epoch 21 iteration 0010/0187: training loss 0.239; learning rate 0.000167
Epoch 21 iteration 0011/0187: training loss 0.237; learning rate 0.000167
Epoch 21 iteration 0012/0187: training loss 0.237; learning rate 0.000167
Epoch 21 iteration 0013/0187: training loss 0.235; learning rate 0.000167
Epoch 21 iteration 0014/0187: training loss 0.230; learning rate 0.000167
Epoch 21 iteration 0015/0187: training loss 0.226; learning rate 0.000167
Epoch 21 iteration 0016/0187: training loss 0.225; learning rate 0.000167
Epoch 21 iteration 0017/0187: training loss 0.230; learning rate 0.000167
Epoch 21 iteration 0018/0187: training loss 0.229; learning rate 0.000167
Epoch 21 iteration 0019/0187: training loss 0.232; learning rate 0.000166
Epoch 21 iteration 0020/0187: training loss 0.233; learning rate 0.000166
Epoch 21 iteration 0021/0187: training loss 0.235; learning rate 0.000166
Epoch 21 iteration 0022/0187: training loss 0.238; learning rate 0.000166
Epoch 21 iteration 0023/0187: training loss 0.237; learning rate 0.000166
Epoch 21 iteration 0024/0187: training loss 0.238; learning rate 0.000166
Epoch 21 iteration 0025/0187: training loss 0.240; learning rate 0.000166
Epoch 21 iteration 0026/0187: training loss 0.240; learning rate 0.000166
Epoch 21 iteration 0027/0187: training loss 0.239; learning rate 0.000166
Epoch 21 iteration 0028/0187: training loss 0.238; learning rate 0.000166
Epoch 21 iteration 0029/0187: training loss 0.238; learning rate 0.000166
Epoch 21 iteration 0030/0187: training loss 0.242; learning rate 0.000165
Epoch 21 iteration 0031/0187: training loss 0.240; learning rate 0.000165
Epoch 21 iteration 0032/0187: training loss 0.241; learning rate 0.000165
Epoch 21 iteration 0033/0187: training loss 0.245; learning rate 0.000165
Epoch 21 iteration 0034/0187: training loss 0.245; learning rate 0.000165
Epoch 21 iteration 0035/0187: training loss 0.246; learning rate 0.000165
Epoch 21 iteration 0036/0187: training loss 0.246; learning rate 0.000165
Epoch 21 iteration 0037/0187: training loss 0.245; learning rate 0.000165
Epoch 21 iteration 0038/0187: training loss 0.244; learning rate 0.000165
Epoch 21 iteration 0039/0187: training loss 0.246; learning rate 0.000165
Epoch 21 iteration 0040/0187: training loss 0.247; learning rate 0.000165
Epoch 21 iteration 0041/0187: training loss 0.245; learning rate 0.000164
Epoch 21 iteration 0042/0187: training loss 0.247; learning rate 0.000164
Epoch 21 iteration 0043/0187: training loss 0.248; learning rate 0.000164
Epoch 21 iteration 0044/0187: training loss 0.246; learning rate 0.000164
Epoch 21 iteration 0045/0187: training loss 0.247; learning rate 0.000164
Epoch 21 iteration 0046/0187: training loss 0.246; learning rate 0.000164
Epoch 21 iteration 0047/0187: training loss 0.245; learning rate 0.000164
Epoch 21 iteration 0048/0187: training loss 0.245; learning rate 0.000164
Epoch 21 iteration 0049/0187: training loss 0.247; learning rate 0.000164
Epoch 21 iteration 0050/0187: training loss 0.246; learning rate 0.000164
Epoch 21 iteration 0051/0187: training loss 0.246; learning rate 0.000164
Epoch 21 iteration 0052/0187: training loss 0.246; learning rate 0.000163
Epoch 21 iteration 0053/0187: training loss 0.247; learning rate 0.000163
Epoch 21 iteration 0054/0187: training loss 0.246; learning rate 0.000163
Epoch 21 iteration 0055/0187: training loss 0.247; learning rate 0.000163
Epoch 21 iteration 0056/0187: training loss 0.248; learning rate 0.000163
Epoch 21 iteration 0057/0187: training loss 0.247; learning rate 0.000163
Epoch 21 iteration 0058/0187: training loss 0.247; learning rate 0.000163
Epoch 21 iteration 0059/0187: training loss 0.246; learning rate 0.000163
Epoch 21 iteration 0060/0187: training loss 0.246; learning rate 0.000163
Epoch 21 iteration 0061/0187: training loss 0.245; learning rate 0.000163
Epoch 21 iteration 0062/0187: training loss 0.245; learning rate 0.000163
Epoch 21 iteration 0063/0187: training loss 0.247; learning rate 0.000162
Epoch 21 iteration 0064/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0065/0187: training loss 0.247; learning rate 0.000162
Epoch 21 iteration 0066/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0067/0187: training loss 0.249; learning rate 0.000162
Epoch 21 iteration 0068/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0069/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0070/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0071/0187: training loss 0.248; learning rate 0.000162
Epoch 21 iteration 0072/0187: training loss 0.250; learning rate 0.000162
Epoch 21 iteration 0073/0187: training loss 0.251; learning rate 0.000162
Epoch 21 iteration 0074/0187: training loss 0.251; learning rate 0.000161
Epoch 21 iteration 0075/0187: training loss 0.251; learning rate 0.000161
Epoch 21 iteration 0076/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0077/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0078/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0079/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0080/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0081/0187: training loss 0.251; learning rate 0.000161
Epoch 21 iteration 0082/0187: training loss 0.251; learning rate 0.000161
Epoch 21 iteration 0083/0187: training loss 0.251; learning rate 0.000161
Epoch 21 iteration 0084/0187: training loss 0.250; learning rate 0.000161
Epoch 21 iteration 0085/0187: training loss 0.250; learning rate 0.000160
Epoch 21 iteration 0086/0187: training loss 0.250; learning rate 0.000160
Epoch 21 iteration 0087/0187: training loss 0.251; learning rate 0.000160
Epoch 21 iteration 0088/0187: training loss 0.251; learning rate 0.000160
Epoch 21 iteration 0089/0187: training loss 0.252; learning rate 0.000160
Epoch 21 iteration 0090/0187: training loss 0.252; learning rate 0.000160
Epoch 21 iteration 0091/0187: training loss 0.251; learning rate 0.000160
Epoch 21 iteration 0092/0187: training loss 0.250; learning rate 0.000160
Epoch 21 iteration 0093/0187: training loss 0.251; learning rate 0.000160
Epoch 21 iteration 0094/0187: training loss 0.250; learning rate 0.000160
Epoch 21 iteration 0095/0187: training loss 0.251; learning rate 0.000160
Epoch 21 iteration 0096/0187: training loss 0.250; learning rate 0.000159
Epoch 21 iteration 0097/0187: training loss 0.251; learning rate 0.000159
Epoch 21 iteration 0098/0187: training loss 0.253; learning rate 0.000159
Epoch 21 iteration 0099/0187: training loss 0.252; learning rate 0.000159
Epoch 21 iteration 0100/0187: training loss 0.252; learning rate 0.000159
Epoch 21 iteration 0101/0187: training loss 0.253; learning rate 0.000159
Epoch 21 iteration 0102/0187: training loss 0.253; learning rate 0.000159
Epoch 21 iteration 0103/0187: training loss 0.253; learning rate 0.000159
Epoch 21 iteration 0104/0187: training loss 0.253; learning rate 0.000159
Epoch 21 iteration 0105/0187: training loss 0.254; learning rate 0.000159
Epoch 21 iteration 0106/0187: training loss 0.254; learning rate 0.000159
Epoch 21 iteration 0107/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0108/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0109/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0110/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0111/0187: training loss 0.255; learning rate 0.000158
Epoch 21 iteration 0112/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0113/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0114/0187: training loss 0.255; learning rate 0.000158
Epoch 21 iteration 0115/0187: training loss 0.254; learning rate 0.000158
Epoch 21 iteration 0116/0187: training loss 0.256; learning rate 0.000158
Epoch 21 iteration 0117/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0118/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0119/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0120/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0121/0187: training loss 0.254; learning rate 0.000157
Epoch 21 iteration 0122/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0123/0187: training loss 0.255; learning rate 0.000157
Epoch 21 iteration 0124/0187: training loss 0.254; learning rate 0.000157
Epoch 21 iteration 0125/0187: training loss 0.253; learning rate 0.000157
Epoch 21 iteration 0126/0187: training loss 0.254; learning rate 0.000157
Epoch 21 iteration 0127/0187: training loss 0.254; learning rate 0.000157
Epoch 21 iteration 0128/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0129/0187: training loss 0.254; learning rate 0.000156
Epoch 21 iteration 0130/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0131/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0132/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0133/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0134/0187: training loss 0.252; learning rate 0.000156
Epoch 21 iteration 0135/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0136/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0137/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0138/0187: training loss 0.253; learning rate 0.000156
Epoch 21 iteration 0139/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0140/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0141/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0142/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0143/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0144/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0145/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0146/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0147/0187: training loss 0.253; learning rate 0.000155
Epoch 21 iteration 0148/0187: training loss 0.254; learning rate 0.000155
Epoch 21 iteration 0149/0187: training loss 0.254; learning rate 0.000155
Epoch 21 iteration 0150/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0151/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0152/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0153/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0154/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0155/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0156/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0157/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0158/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0159/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0160/0187: training loss 0.254; learning rate 0.000154
Epoch 21 iteration 0161/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0162/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0163/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0164/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0165/0187: training loss 0.252; learning rate 0.000153
Epoch 21 iteration 0166/0187: training loss 0.252; learning rate 0.000153
Epoch 21 iteration 0167/0187: training loss 0.252; learning rate 0.000153
Epoch 21 iteration 0168/0187: training loss 0.252; learning rate 0.000153
Epoch 21 iteration 0169/0187: training loss 0.252; learning rate 0.000153
Epoch 21 iteration 0170/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0171/0187: training loss 0.253; learning rate 0.000153
Epoch 21 iteration 0172/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0173/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0174/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0175/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0176/0187: training loss 0.252; learning rate 0.000152
Epoch 21 iteration 0177/0187: training loss 0.252; learning rate 0.000152
Epoch 21 iteration 0178/0187: training loss 0.252; learning rate 0.000152
Epoch 21 iteration 0179/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0180/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0181/0187: training loss 0.253; learning rate 0.000152
Epoch 21 iteration 0182/0187: training loss 0.252; learning rate 0.000152
Epoch 21 iteration 0183/0187: training loss 0.252; learning rate 0.000151
Epoch 21 iteration 0184/0187: training loss 0.252; learning rate 0.000151
Epoch 21 iteration 0185/0187: training loss 0.252; learning rate 0.000151
Epoch 21 iteration 0186/0187: training loss 0.252; learning rate 0.000151
Epoch 21 iteration 0187/0187: training loss 0.252; learning rate 0.000151
Epoch 21 validation pixAcc: 0.336, mIoU: 0.203
Epoch 22 iteration 0001/0187: training loss 0.294; learning rate 0.000151
Epoch 22 iteration 0002/0187: training loss 0.290; learning rate 0.000151
Epoch 22 iteration 0003/0187: training loss 0.306; learning rate 0.000151
Epoch 22 iteration 0004/0187: training loss 0.280; learning rate 0.000151
Epoch 22 iteration 0005/0187: training loss 0.273; learning rate 0.000151
Epoch 22 iteration 0006/0187: training loss 0.272; learning rate 0.000150
Epoch 22 iteration 0007/0187: training loss 0.292; learning rate 0.000150
Epoch 22 iteration 0008/0187: training loss 0.302; learning rate 0.000150
Epoch 22 iteration 0009/0187: training loss 0.290; learning rate 0.000150
Epoch 22 iteration 0010/0187: training loss 0.285; learning rate 0.000150
Epoch 22 iteration 0011/0187: training loss 0.284; learning rate 0.000150
Epoch 22 iteration 0012/0187: training loss 0.279; learning rate 0.000150
Epoch 22 iteration 0013/0187: training loss 0.279; learning rate 0.000150
Epoch 22 iteration 0014/0187: training loss 0.279; learning rate 0.000150
Epoch 22 iteration 0015/0187: training loss 0.283; learning rate 0.000150
Epoch 22 iteration 0016/0187: training loss 0.285; learning rate 0.000150
Epoch 22 iteration 0017/0187: training loss 0.291; learning rate 0.000149
Epoch 22 iteration 0018/0187: training loss 0.287; learning rate 0.000149
Epoch 22 iteration 0019/0187: training loss 0.282; learning rate 0.000149
Epoch 22 iteration 0020/0187: training loss 0.279; learning rate 0.000149
Epoch 22 iteration 0021/0187: training loss 0.282; learning rate 0.000149
Epoch 22 iteration 0022/0187: training loss 0.276; learning rate 0.000149
Epoch 22 iteration 0023/0187: training loss 0.275; learning rate 0.000149
Epoch 22 iteration 0024/0187: training loss 0.275; learning rate 0.000149
Epoch 22 iteration 0025/0187: training loss 0.272; learning rate 0.000149
Epoch 22 iteration 0026/0187: training loss 0.270; learning rate 0.000149
Epoch 22 iteration 0027/0187: training loss 0.268; learning rate 0.000149
Epoch 22 iteration 0028/0187: training loss 0.269; learning rate 0.000148
Epoch 22 iteration 0029/0187: training loss 0.268; learning rate 0.000148
Epoch 22 iteration 0030/0187: training loss 0.269; learning rate 0.000148
Epoch 22 iteration 0031/0187: training loss 0.269; learning rate 0.000148
Epoch 22 iteration 0032/0187: training loss 0.267; learning rate 0.000148
Epoch 22 iteration 0033/0187: training loss 0.267; learning rate 0.000148
Epoch 22 iteration 0034/0187: training loss 0.266; learning rate 0.000148
Epoch 22 iteration 0035/0187: training loss 0.265; learning rate 0.000148
Epoch 22 iteration 0036/0187: training loss 0.264; learning rate 0.000148
Epoch 22 iteration 0037/0187: training loss 0.262; learning rate 0.000148
Epoch 22 iteration 0038/0187: training loss 0.262; learning rate 0.000148
Epoch 22 iteration 0039/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0040/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0041/0187: training loss 0.264; learning rate 0.000147
Epoch 22 iteration 0042/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0043/0187: training loss 0.261; learning rate 0.000147
Epoch 22 iteration 0044/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0045/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0046/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0047/0187: training loss 0.264; learning rate 0.000147
Epoch 22 iteration 0048/0187: training loss 0.263; learning rate 0.000147
Epoch 22 iteration 0049/0187: training loss 0.262; learning rate 0.000147
Epoch 22 iteration 0050/0187: training loss 0.262; learning rate 0.000146
Epoch 22 iteration 0051/0187: training loss 0.261; learning rate 0.000146
Epoch 22 iteration 0052/0187: training loss 0.260; learning rate 0.000146
Epoch 22 iteration 0053/0187: training loss 0.261; learning rate 0.000146
Epoch 22 iteration 0054/0187: training loss 0.263; learning rate 0.000146
Epoch 22 iteration 0055/0187: training loss 0.263; learning rate 0.000146
Epoch 22 iteration 0056/0187: training loss 0.262; learning rate 0.000146
Epoch 22 iteration 0057/0187: training loss 0.263; learning rate 0.000146
Epoch 22 iteration 0058/0187: training loss 0.264; learning rate 0.000146
Epoch 22 iteration 0059/0187: training loss 0.266; learning rate 0.000146
Epoch 22 iteration 0060/0187: training loss 0.267; learning rate 0.000145
Epoch 22 iteration 0061/0187: training loss 0.268; learning rate 0.000145
Epoch 22 iteration 0062/0187: training loss 0.268; learning rate 0.000145
Epoch 22 iteration 0063/0187: training loss 0.267; learning rate 0.000145
Epoch 22 iteration 0064/0187: training loss 0.266; learning rate 0.000145
Epoch 22 iteration 0065/0187: training loss 0.265; learning rate 0.000145
Epoch 22 iteration 0066/0187: training loss 0.265; learning rate 0.000145
Epoch 22 iteration 0067/0187: training loss 0.265; learning rate 0.000145
Epoch 22 iteration 0068/0187: training loss 0.267; learning rate 0.000145
Epoch 22 iteration 0069/0187: training loss 0.268; learning rate 0.000145
Epoch 22 iteration 0070/0187: training loss 0.267; learning rate 0.000145
Epoch 22 iteration 0071/0187: training loss 0.266; learning rate 0.000144
Epoch 22 iteration 0072/0187: training loss 0.265; learning rate 0.000144
Epoch 22 iteration 0073/0187: training loss 0.265; learning rate 0.000144
Epoch 22 iteration 0074/0187: training loss 0.264; learning rate 0.000144
Epoch 22 iteration 0075/0187: training loss 0.263; learning rate 0.000144
Epoch 22 iteration 0076/0187: training loss 0.263; learning rate 0.000144
Epoch 22 iteration 0077/0187: training loss 0.264; learning rate 0.000144
Epoch 22 iteration 0078/0187: training loss 0.264; learning rate 0.000144
Epoch 22 iteration 0079/0187: training loss 0.263; learning rate 0.000144
Epoch 22 iteration 0080/0187: training loss 0.263; learning rate 0.000144
Epoch 22 iteration 0081/0187: training loss 0.265; learning rate 0.000144
Epoch 22 iteration 0082/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0083/0187: training loss 0.264; learning rate 0.000143
Epoch 22 iteration 0084/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0085/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0086/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0087/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0088/0187: training loss 0.264; learning rate 0.000143
Epoch 22 iteration 0089/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0090/0187: training loss 0.265; learning rate 0.000143
Epoch 22 iteration 0091/0188: training loss 0.264; learning rate 0.000143
Epoch 22 iteration 0092/0188: training loss 0.264; learning rate 0.000143
Epoch 22 iteration 0093/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0094/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0095/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0096/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0097/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0098/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0099/0188: training loss 0.262; learning rate 0.000142
Epoch 22 iteration 0100/0188: training loss 0.263; learning rate 0.000142
Epoch 22 iteration 0101/0188: training loss 0.262; learning rate 0.000142
Epoch 22 iteration 0102/0188: training loss 0.262; learning rate 0.000142
Epoch 22 iteration 0103/0188: training loss 0.262; learning rate 0.000142
Epoch 22 iteration 0104/0188: training loss 0.261; learning rate 0.000141
Epoch 22 iteration 0105/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0106/0188: training loss 0.261; learning rate 0.000141
Epoch 22 iteration 0107/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0108/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0109/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0110/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0111/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0112/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0113/0188: training loss 0.260; learning rate 0.000141
Epoch 22 iteration 0114/0188: training loss 0.259; learning rate 0.000141
Epoch 22 iteration 0115/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0116/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0117/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0118/0188: training loss 0.258; learning rate 0.000140
Epoch 22 iteration 0119/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0120/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0121/0188: training loss 0.258; learning rate 0.000140
Epoch 22 iteration 0122/0188: training loss 0.258; learning rate 0.000140
Epoch 22 iteration 0123/0188: training loss 0.258; learning rate 0.000140
Epoch 22 iteration 0124/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0125/0188: training loss 0.259; learning rate 0.000140
Epoch 22 iteration 0126/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0127/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0128/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0129/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0130/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0131/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0132/0188: training loss 0.258; learning rate 0.000139
Epoch 22 iteration 0133/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0134/0188: training loss 0.259; learning rate 0.000139
Epoch 22 iteration 0135/0188: training loss 0.258; learning rate 0.000139
Epoch 22 iteration 0136/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0137/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0138/0188: training loss 0.257; learning rate 0.000138
Epoch 22 iteration 0139/0188: training loss 0.257; learning rate 0.000138
Epoch 22 iteration 0140/0188: training loss 0.257; learning rate 0.000138
Epoch 22 iteration 0141/0188: training loss 0.257; learning rate 0.000138
Epoch 22 iteration 0142/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0143/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0144/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0145/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0146/0188: training loss 0.258; learning rate 0.000138
Epoch 22 iteration 0147/0188: training loss 0.258; learning rate 0.000137
Epoch 22 iteration 0148/0188: training loss 0.259; learning rate 0.000137
Epoch 22 iteration 0149/0188: training loss 0.258; learning rate 0.000137
Epoch 22 iteration 0150/0188: training loss 0.259; learning rate 0.000137
Epoch 22 iteration 0151/0188: training loss 0.260; learning rate 0.000137
Epoch 22 iteration 0152/0188: training loss 0.260; learning rate 0.000137
Epoch 22 iteration 0153/0188: training loss 0.260; learning rate 0.000137
Epoch 22 iteration 0154/0188: training loss 0.260; learning rate 0.000137
Epoch 22 iteration 0155/0188: training loss 0.260; learning rate 0.000137
Epoch 22 iteration 0156/0188: training loss 0.259; learning rate 0.000137
Epoch 22 iteration 0157/0188: training loss 0.259; learning rate 0.000137
Epoch 22 iteration 0158/0188: training loss 0.259; learning rate 0.000136
Epoch 22 iteration 0159/0188: training loss 0.259; learning rate 0.000136
Epoch 22 iteration 0160/0188: training loss 0.259; learning rate 0.000136
Epoch 22 iteration 0161/0188: training loss 0.259; learning rate 0.000136
Epoch 22 iteration 0162/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0163/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0164/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0165/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0166/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0167/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0168/0188: training loss 0.258; learning rate 0.000136
Epoch 22 iteration 0169/0188: training loss 0.258; learning rate 0.000135
Epoch 22 iteration 0170/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0171/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0172/0188: training loss 0.256; learning rate 0.000135
Epoch 22 iteration 0173/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0174/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0175/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0176/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0177/0188: training loss 0.258; learning rate 0.000135
Epoch 22 iteration 0178/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0179/0188: training loss 0.257; learning rate 0.000135
Epoch 22 iteration 0180/0188: training loss 0.257; learning rate 0.000134
Epoch 22 iteration 0181/0188: training loss 0.257; learning rate 0.000134
Epoch 22 iteration 0182/0188: training loss 0.258; learning rate 0.000134
Epoch 22 iteration 0183/0188: training loss 0.258; learning rate 0.000134
Epoch 22 iteration 0184/0188: training loss 0.258; learning rate 0.000134
Epoch 22 iteration 0185/0188: training loss 0.257; learning rate 0.000134
Epoch 22 iteration 0186/0188: training loss 0.257; learning rate 0.000134
Epoch 22 validation pixAcc: 0.337, mIoU: 0.202
Epoch 23 iteration 0001/0187: training loss 0.218; learning rate 0.000134
Epoch 23 iteration 0002/0187: training loss 0.238; learning rate 0.000134
Epoch 23 iteration 0003/0187: training loss 0.223; learning rate 0.000133
Epoch 23 iteration 0004/0187: training loss 0.258; learning rate 0.000133
Epoch 23 iteration 0005/0187: training loss 0.268; learning rate 0.000133
Epoch 23 iteration 0006/0187: training loss 0.273; learning rate 0.000133
Epoch 23 iteration 0007/0187: training loss 0.276; learning rate 0.000133
Epoch 23 iteration 0008/0187: training loss 0.271; learning rate 0.000133
Epoch 23 iteration 0009/0187: training loss 0.262; learning rate 0.000133
Epoch 23 iteration 0010/0187: training loss 0.257; learning rate 0.000133
Epoch 23 iteration 0011/0187: training loss 0.258; learning rate 0.000133
Epoch 23 iteration 0012/0187: training loss 0.251; learning rate 0.000133
Epoch 23 iteration 0013/0187: training loss 0.253; learning rate 0.000133
Epoch 23 iteration 0014/0187: training loss 0.252; learning rate 0.000132
Epoch 23 iteration 0015/0187: training loss 0.248; learning rate 0.000132
Epoch 23 iteration 0016/0187: training loss 0.252; learning rate 0.000132
Epoch 23 iteration 0017/0187: training loss 0.252; learning rate 0.000132
Epoch 23 iteration 0018/0187: training loss 0.253; learning rate 0.000132
Epoch 23 iteration 0019/0187: training loss 0.252; learning rate 0.000132
Epoch 23 iteration 0020/0187: training loss 0.258; learning rate 0.000132
Epoch 23 iteration 0021/0187: training loss 0.256; learning rate 0.000132
Epoch 23 iteration 0022/0187: training loss 0.257; learning rate 0.000132
Epoch 23 iteration 0023/0187: training loss 0.254; learning rate 0.000132
Epoch 23 iteration 0024/0187: training loss 0.251; learning rate 0.000132
Epoch 23 iteration 0025/0187: training loss 0.251; learning rate 0.000131
Epoch 23 iteration 0026/0187: training loss 0.252; learning rate 0.000131
Epoch 23 iteration 0027/0187: training loss 0.255; learning rate 0.000131
Epoch 23 iteration 0028/0187: training loss 0.256; learning rate 0.000131
Epoch 23 iteration 0029/0187: training loss 0.254; learning rate 0.000131
Epoch 23 iteration 0030/0187: training loss 0.256; learning rate 0.000131
Epoch 23 iteration 0031/0187: training loss 0.255; learning rate 0.000131
Epoch 23 iteration 0032/0187: training loss 0.256; learning rate 0.000131
Epoch 23 iteration 0033/0187: training loss 0.257; learning rate 0.000131
Epoch 23 iteration 0034/0187: training loss 0.258; learning rate 0.000131
Epoch 23 iteration 0035/0187: training loss 0.259; learning rate 0.000131
Epoch 23 iteration 0036/0187: training loss 0.259; learning rate 0.000130
Epoch 23 iteration 0037/0187: training loss 0.264; learning rate 0.000130
Epoch 23 iteration 0038/0187: training loss 0.264; learning rate 0.000130
Epoch 23 iteration 0039/0187: training loss 0.266; learning rate 0.000130
Epoch 23 iteration 0040/0187: training loss 0.266; learning rate 0.000130
Epoch 23 iteration 0041/0187: training loss 0.265; learning rate 0.000130
Epoch 23 iteration 0042/0187: training loss 0.266; learning rate 0.000130
Epoch 23 iteration 0043/0187: training loss 0.267; learning rate 0.000130
Epoch 23 iteration 0044/0187: training loss 0.267; learning rate 0.000130
Epoch 23 iteration 0045/0187: training loss 0.267; learning rate 0.000130
Epoch 23 iteration 0046/0187: training loss 0.270; learning rate 0.000129
Epoch 23 iteration 0047/0187: training loss 0.269; learning rate 0.000129
Epoch 23 iteration 0048/0187: training loss 0.268; learning rate 0.000129
Epoch 23 iteration 0049/0187: training loss 0.268; learning rate 0.000129
Epoch 23 iteration 0050/0187: training loss 0.269; learning rate 0.000129
Epoch 23 iteration 0051/0187: training loss 0.269; learning rate 0.000129
Epoch 23 iteration 0052/0187: training loss 0.268; learning rate 0.000129
Epoch 23 iteration 0053/0187: training loss 0.267; learning rate 0.000129
Epoch 23 iteration 0054/0187: training loss 0.267; learning rate 0.000129
Epoch 23 iteration 0055/0187: training loss 0.267; learning rate 0.000129
Epoch 23 iteration 0056/0187: training loss 0.269; learning rate 0.000129
Epoch 23 iteration 0057/0187: training loss 0.269; learning rate 0.000128
Epoch 23 iteration 0058/0187: training loss 0.268; learning rate 0.000128
Epoch 23 iteration 0059/0187: training loss 0.267; learning rate 0.000128
Epoch 23 iteration 0060/0187: training loss 0.266; learning rate 0.000128
Epoch 23 iteration 0061/0187: training loss 0.267; learning rate 0.000128
Epoch 23 iteration 0062/0187: training loss 0.266; learning rate 0.000128
Epoch 23 iteration 0063/0187: training loss 0.265; learning rate 0.000128
Epoch 23 iteration 0064/0187: training loss 0.265; learning rate 0.000128
Epoch 23 iteration 0065/0187: training loss 0.265; learning rate 0.000128
Epoch 23 iteration 0066/0187: training loss 0.266; learning rate 0.000128
Epoch 23 iteration 0067/0187: training loss 0.265; learning rate 0.000128
Epoch 23 iteration 0068/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0069/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0070/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0071/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0072/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0073/0187: training loss 0.264; learning rate 0.000127
Epoch 23 iteration 0074/0187: training loss 0.263; learning rate 0.000127
Epoch 23 iteration 0075/0187: training loss 0.262; learning rate 0.000127
Epoch 23 iteration 0076/0187: training loss 0.262; learning rate 0.000127
Epoch 23 iteration 0077/0187: training loss 0.261; learning rate 0.000127
Epoch 23 iteration 0078/0187: training loss 0.261; learning rate 0.000126
Epoch 23 iteration 0079/0187: training loss 0.260; learning rate 0.000126
Epoch 23 iteration 0080/0187: training loss 0.260; learning rate 0.000126
Epoch 23 iteration 0081/0187: training loss 0.260; learning rate 0.000126
Epoch 23 iteration 0082/0187: training loss 0.261; learning rate 0.000126
Epoch 23 iteration 0083/0187: training loss 0.261; learning rate 0.000126
Epoch 23 iteration 0084/0187: training loss 0.260; learning rate 0.000126
Epoch 23 iteration 0085/0187: training loss 0.259; learning rate 0.000126
Epoch 23 iteration 0086/0187: training loss 0.258; learning rate 0.000126
Epoch 23 iteration 0087/0187: training loss 0.258; learning rate 0.000126
Epoch 23 iteration 0088/0187: training loss 0.258; learning rate 0.000126
Epoch 23 iteration 0089/0187: training loss 0.258; learning rate 0.000125
Epoch 23 iteration 0090/0187: training loss 0.257; learning rate 0.000125
Epoch 23 iteration 0091/0187: training loss 0.257; learning rate 0.000125
Epoch 23 iteration 0092/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0093/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0094/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0095/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0096/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0097/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0098/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0099/0187: training loss nan; learning rate 0.000125
Epoch 23 iteration 0100/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0101/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0102/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0103/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0104/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0105/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0106/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0107/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0108/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0109/0187: training loss nan; learning rate 0.000124
Epoch 23 iteration 0110/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0111/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0112/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0113/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0114/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0115/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0116/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0117/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0118/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0119/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0120/0187: training loss nan; learning rate 0.000123
Epoch 23 iteration 0121/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0122/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0123/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0124/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0125/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0126/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0127/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0128/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0129/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0130/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0131/0187: training loss nan; learning rate 0.000122
Epoch 23 iteration 0132/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0133/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0134/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0135/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0136/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0137/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0138/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0139/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0140/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0141/0187: training loss nan; learning rate 0.000121
Epoch 23 iteration 0142/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0143/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0144/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0145/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0146/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0147/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0148/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0149/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0150/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0151/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0152/0187: training loss nan; learning rate 0.000120
Epoch 23 iteration 0153/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0154/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0155/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0156/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0157/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0158/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0159/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0160/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0161/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0162/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0163/0187: training loss nan; learning rate 0.000119
Epoch 23 iteration 0164/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0165/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0166/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0167/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0168/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0169/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0170/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0171/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0172/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0173/0187: training loss nan; learning rate 0.000118
Epoch 23 iteration 0174/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0175/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0176/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0177/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0178/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0179/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0180/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0181/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0182/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0183/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0184/0187: training loss nan; learning rate 0.000117
Epoch 23 iteration 0185/0187: training loss nan; learning rate 0.000116
Epoch 23 iteration 0186/0187: training loss nan; learning rate 0.000116
Epoch 23 iteration 0187/0187: training loss nan; learning rate 0.000116
Epoch 23 validation pixAcc: 0.337, mIoU: 0.203
Epoch 24 iteration 0001/0187: training loss 0.283; learning rate 0.000116
Epoch 24 iteration 0002/0187: training loss 0.257; learning rate 0.000116
Epoch 24 iteration 0003/0187: training loss 0.243; learning rate 0.000116
Epoch 24 iteration 0004/0187: training loss 0.225; learning rate 0.000116
Epoch 24 iteration 0005/0187: training loss 0.225; learning rate 0.000116
Epoch 24 iteration 0006/0187: training loss 0.263; learning rate 0.000116
Epoch 24 iteration 0007/0187: training loss 0.271; learning rate 0.000115
Epoch 24 iteration 0008/0187: training loss 0.269; learning rate 0.000115
Epoch 24 iteration 0009/0187: training loss 0.258; learning rate 0.000115
Epoch 24 iteration 0010/0187: training loss 0.264; learning rate 0.000115
Epoch 24 iteration 0011/0187: training loss 0.256; learning rate 0.000115
Epoch 24 iteration 0012/0187: training loss 0.255; learning rate 0.000115
Epoch 24 iteration 0013/0187: training loss 0.255; learning rate 0.000115
Epoch 24 iteration 0014/0187: training loss 0.257; learning rate 0.000115
Epoch 24 iteration 0015/0187: training loss 0.260; learning rate 0.000115
Epoch 24 iteration 0016/0187: training loss 0.260; learning rate 0.000115
Epoch 24 iteration 0017/0187: training loss 0.262; learning rate 0.000115
Epoch 24 iteration 0018/0187: training loss 0.263; learning rate 0.000114
Epoch 24 iteration 0019/0187: training loss 0.262; learning rate 0.000114
Epoch 24 iteration 0020/0187: training loss 0.264; learning rate 0.000114
Epoch 24 iteration 0021/0187: training loss 0.264; learning rate 0.000114
Epoch 24 iteration 0022/0187: training loss 0.264; learning rate 0.000114
Epoch 24 iteration 0023/0187: training loss 0.267; learning rate 0.000114
Epoch 24 iteration 0024/0187: training loss 0.264; learning rate 0.000114
Epoch 24 iteration 0025/0187: training loss 0.261; learning rate 0.000114
Epoch 24 iteration 0026/0187: training loss 0.263; learning rate 0.000114
Epoch 24 iteration 0027/0187: training loss 0.262; learning rate 0.000114
Epoch 24 iteration 0028/0187: training loss 0.260; learning rate 0.000114
Epoch 24 iteration 0029/0187: training loss 0.260; learning rate 0.000113
Epoch 24 iteration 0030/0187: training loss 0.260; learning rate 0.000113
Epoch 24 iteration 0031/0187: training loss 0.257; learning rate 0.000113
Epoch 24 iteration 0032/0187: training loss 0.256; learning rate 0.000113
Epoch 24 iteration 0033/0187: training loss 0.254; learning rate 0.000113
Epoch 24 iteration 0034/0187: training loss 0.252; learning rate 0.000113
Epoch 24 iteration 0035/0187: training loss 0.253; learning rate 0.000113
Epoch 24 iteration 0036/0187: training loss 0.256; learning rate 0.000113
Epoch 24 iteration 0037/0187: training loss 0.256; learning rate 0.000113
Epoch 24 iteration 0038/0187: training loss 0.256; learning rate 0.000113
Epoch 24 iteration 0039/0187: training loss 0.255; learning rate 0.000112
Epoch 24 iteration 0040/0187: training loss 0.254; learning rate 0.000112
Epoch 24 iteration 0041/0187: training loss 0.255; learning rate 0.000112
Epoch 24 iteration 0042/0187: training loss 0.253; learning rate 0.000112
Epoch 24 iteration 0043/0187: training loss 0.252; learning rate 0.000112
Epoch 24 iteration 0044/0187: training loss 0.251; learning rate 0.000112
Epoch 24 iteration 0045/0187: training loss 0.250; learning rate 0.000112
Epoch 24 iteration 0046/0187: training loss 0.249; learning rate 0.000112
Epoch 24 iteration 0047/0187: training loss 0.251; learning rate 0.000112
Epoch 24 iteration 0048/0187: training loss 0.251; learning rate 0.000112
Epoch 24 iteration 0049/0187: training loss 0.251; learning rate 0.000112
Epoch 24 iteration 0050/0187: training loss 0.251; learning rate 0.000111
Epoch 24 iteration 0051/0187: training loss 0.250; learning rate 0.000111
Epoch 24 iteration 0052/0187: training loss 0.253; learning rate 0.000111
Epoch 24 iteration 0053/0187: training loss 0.253; learning rate 0.000111
Epoch 24 iteration 0054/0187: training loss 0.252; learning rate 0.000111
Epoch 24 iteration 0055/0187: training loss 0.252; learning rate 0.000111
Epoch 24 iteration 0056/0187: training loss 0.252; learning rate 0.000111
Epoch 24 iteration 0057/0187: training loss 0.251; learning rate 0.000111
Epoch 24 iteration 0058/0187: training loss 0.251; learning rate 0.000111
Epoch 24 iteration 0059/0187: training loss 0.250; learning rate 0.000111
Epoch 24 iteration 0060/0187: training loss 0.249; learning rate 0.000110
Epoch 24 iteration 0061/0187: training loss 0.250; learning rate 0.000110
Epoch 24 iteration 0062/0187: training loss 0.249; learning rate 0.000110
Epoch 24 iteration 0063/0187: training loss 0.250; learning rate 0.000110
Epoch 24 iteration 0064/0187: training loss 0.251; learning rate 0.000110
Epoch 24 iteration 0065/0187: training loss 0.250; learning rate 0.000110
Epoch 24 iteration 0066/0187: training loss 0.250; learning rate 0.000110
Epoch 24 iteration 0067/0187: training loss 0.251; learning rate 0.000110
Epoch 24 iteration 0068/0187: training loss 0.252; learning rate 0.000110
Epoch 24 iteration 0069/0187: training loss 0.252; learning rate 0.000110
Epoch 24 iteration 0070/0187: training loss 0.255; learning rate 0.000110
Epoch 24 iteration 0071/0187: training loss 0.255; learning rate 0.000109
Epoch 24 iteration 0072/0187: training loss 0.256; learning rate 0.000109
Epoch 24 iteration 0073/0187: training loss 0.256; learning rate 0.000109
Epoch 24 iteration 0074/0187: training loss 0.257; learning rate 0.000109
Epoch 24 iteration 0075/0187: training loss 0.256; learning rate 0.000109
Epoch 24 iteration 0076/0187: training loss 0.257; learning rate 0.000109
Epoch 24 iteration 0077/0187: training loss 0.256; learning rate 0.000109
Epoch 24 iteration 0078/0187: training loss 0.257; learning rate 0.000109
Epoch 24 iteration 0079/0187: training loss 0.257; learning rate 0.000109
Epoch 24 iteration 0080/0187: training loss 0.257; learning rate 0.000109
Epoch 24 iteration 0081/0187: training loss 0.255; learning rate 0.000108
Epoch 24 iteration 0082/0187: training loss 0.256; learning rate 0.000108
Epoch 24 iteration 0083/0187: training loss 0.256; learning rate 0.000108
Epoch 24 iteration 0084/0187: training loss 0.256; learning rate 0.000108
Epoch 24 iteration 0085/0187: training loss 0.257; learning rate 0.000108
Epoch 24 iteration 0086/0187: training loss 0.256; learning rate 0.000108
Epoch 24 iteration 0087/0187: training loss 0.257; learning rate 0.000108
Epoch 24 iteration 0088/0187: training loss 0.257; learning rate 0.000108
Epoch 24 iteration 0089/0187: training loss 0.258; learning rate 0.000108
Epoch 24 iteration 0090/0187: training loss 0.257; learning rate 0.000108
Epoch 24 iteration 0091/0188: training loss 0.257; learning rate 0.000108
Epoch 24 iteration 0092/0188: training loss 0.258; learning rate 0.000107
Epoch 24 iteration 0093/0188: training loss 0.257; learning rate 0.000107
Epoch 24 iteration 0094/0188: training loss 0.257; learning rate 0.000107
Epoch 24 iteration 0095/0188: training loss 0.257; learning rate 0.000107
Epoch 24 iteration 0096/0188: training loss 0.256; learning rate 0.000107
Epoch 24 iteration 0097/0188: training loss 0.256; learning rate 0.000107
Epoch 24 iteration 0098/0188: training loss 0.255; learning rate 0.000107
Epoch 24 iteration 0099/0188: training loss 0.255; learning rate 0.000107
Epoch 24 iteration 0100/0188: training loss 0.255; learning rate 0.000107
Epoch 24 iteration 0101/0188: training loss 0.255; learning rate 0.000107
Epoch 24 iteration 0102/0188: training loss 0.256; learning rate 0.000106
Epoch 24 iteration 0103/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0104/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0105/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0106/0188: training loss 0.254; learning rate 0.000106
Epoch 24 iteration 0107/0188: training loss 0.254; learning rate 0.000106
Epoch 24 iteration 0108/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0109/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0110/0188: training loss 0.255; learning rate 0.000106
Epoch 24 iteration 0111/0188: training loss 0.257; learning rate 0.000106
Epoch 24 iteration 0112/0188: training loss 0.257; learning rate 0.000106
Epoch 24 iteration 0113/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0114/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0115/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0116/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0117/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0118/0188: training loss 0.256; learning rate 0.000105
Epoch 24 iteration 0119/0188: training loss 0.256; learning rate 0.000105
Epoch 24 iteration 0120/0188: training loss 0.256; learning rate 0.000105
Epoch 24 iteration 0121/0188: training loss 0.256; learning rate 0.000105
Epoch 24 iteration 0122/0188: training loss 0.257; learning rate 0.000105
Epoch 24 iteration 0123/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0124/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0125/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0126/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0127/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0128/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0129/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0130/0188: training loss 0.256; learning rate 0.000104
Epoch 24 iteration 0131/0188: training loss 0.256; learning rate 0.000104
Epoch 24 iteration 0132/0188: training loss 0.257; learning rate 0.000104
Epoch 24 iteration 0133/0188: training loss 0.256; learning rate 0.000104
Epoch 24 iteration 0134/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0135/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0136/0188: training loss 0.257; learning rate 0.000103
Epoch 24 iteration 0137/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0138/0188: training loss 0.255; learning rate 0.000103
Epoch 24 iteration 0139/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0140/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0141/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0142/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0143/0188: training loss 0.256; learning rate 0.000103
Epoch 24 iteration 0144/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0145/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0146/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0147/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0148/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0149/0188: training loss 0.256; learning rate 0.000102
Epoch 24 iteration 0150/0188: training loss 0.255; learning rate 0.000102
Epoch 24 iteration 0151/0188: training loss 0.255; learning rate 0.000102
Epoch 24 iteration 0152/0188: training loss 0.255; learning rate 0.000102
Epoch 24 iteration 0153/0188: training loss 0.255; learning rate 0.000102
Epoch 24 iteration 0154/0188: training loss 0.255; learning rate 0.000102
Epoch 24 iteration 0155/0188: training loss 0.255; learning rate 0.000101
Epoch 24 iteration 0156/0188: training loss 0.254; learning rate 0.000101
Epoch 24 iteration 0157/0188: training loss 0.254; learning rate 0.000101
Epoch 24 iteration 0158/0188: training loss 0.254; learning rate 0.000101
Epoch 24 iteration 0159/0188: training loss 0.254; learning rate 0.000101
Epoch 24 iteration 0160/0188: training loss 0.255; learning rate 0.000101
Epoch 24 iteration 0161/0188: training loss 0.254; learning rate 0.000101
Epoch 24 iteration 0162/0188: training loss 0.255; learning rate 0.000101
Epoch 24 iteration 0163/0188: training loss 0.255; learning rate 0.000101
Epoch 24 iteration 0164/0188: training loss 0.255; learning rate 0.000101
Epoch 24 iteration 0165/0188: training loss 0.255; learning rate 0.000100
Epoch 24 iteration 0166/0188: training loss 0.255; learning rate 0.000100
Epoch 24 iteration 0167/0188: training loss 0.255; learning rate 0.000100
Epoch 24 iteration 0168/0188: training loss 0.255; learning rate 0.000100
Epoch 24 iteration 0169/0188: training loss 0.255; learning rate 0.000100
Epoch 24 iteration 0170/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0171/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0172/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0173/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0174/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0175/0188: training loss 0.254; learning rate 0.000100
Epoch 24 iteration 0176/0188: training loss 0.254; learning rate 0.000099
Epoch 24 iteration 0177/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0178/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0179/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0180/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0181/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0182/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0183/0188: training loss 0.253; learning rate 0.000099
Epoch 24 iteration 0184/0188: training loss 0.254; learning rate 0.000099
Epoch 24 iteration 0185/0188: training loss 0.254; learning rate 0.000099
Epoch 24 iteration 0186/0188: training loss 0.254; learning rate 0.000098
Epoch 24 validation pixAcc: 0.337, mIoU: 0.202
Epoch 25 iteration 0001/0187: training loss 0.282; learning rate 0.000098
Epoch 25 iteration 0002/0187: training loss 0.248; learning rate 0.000098
Epoch 25 iteration 0003/0187: training loss 0.259; learning rate 0.000098
Epoch 25 iteration 0004/0187: training loss 0.254; learning rate 0.000098
Epoch 25 iteration 0005/0187: training loss 0.257; learning rate 0.000098
Epoch 25 iteration 0006/0187: training loss 0.262; learning rate 0.000098
Epoch 25 iteration 0007/0187: training loss 0.255; learning rate 0.000098
Epoch 25 iteration 0008/0187: training loss 0.271; learning rate 0.000098
Epoch 25 iteration 0009/0187: training loss 0.262; learning rate 0.000097
Epoch 25 iteration 0010/0187: training loss 0.253; learning rate 0.000097
Epoch 25 iteration 0011/0187: training loss 0.249; learning rate 0.000097
Epoch 25 iteration 0012/0187: training loss 0.249; learning rate 0.000097
Epoch 25 iteration 0013/0187: training loss 0.251; learning rate 0.000097
Epoch 25 iteration 0014/0187: training loss 0.249; learning rate 0.000097
Epoch 25 iteration 0015/0187: training loss 0.247; learning rate 0.000097
Epoch 25 iteration 0016/0187: training loss 0.244; learning rate 0.000097
Epoch 25 iteration 0017/0187: training loss 0.246; learning rate 0.000097
Epoch 25 iteration 0018/0187: training loss 0.242; learning rate 0.000097
Epoch 25 iteration 0019/0187: training loss 0.241; learning rate 0.000097
Epoch 25 iteration 0020/0187: training loss 0.240; learning rate 0.000096
Epoch 25 iteration 0021/0187: training loss 0.236; learning rate 0.000096
Epoch 25 iteration 0022/0187: training loss 0.234; learning rate 0.000096
Epoch 25 iteration 0023/0187: training loss 0.234; learning rate 0.000096
Epoch 25 iteration 0024/0187: training loss 0.235; learning rate 0.000096
Epoch 25 iteration 0025/0187: training loss 0.238; learning rate 0.000096
Epoch 25 iteration 0026/0187: training loss 0.240; learning rate 0.000096
Epoch 25 iteration 0027/0187: training loss 0.239; learning rate 0.000096
Epoch 25 iteration 0028/0187: training loss 0.239; learning rate 0.000096
Epoch 25 iteration 0029/0187: training loss 0.243; learning rate 0.000096
Epoch 25 iteration 0030/0187: training loss 0.243; learning rate 0.000095
Epoch 25 iteration 0031/0187: training loss 0.242; learning rate 0.000095
Epoch 25 iteration 0032/0187: training loss 0.241; learning rate 0.000095
Epoch 25 iteration 0033/0187: training loss 0.241; learning rate 0.000095
Epoch 25 iteration 0034/0187: training loss 0.240; learning rate 0.000095
Epoch 25 iteration 0035/0187: training loss 0.239; learning rate 0.000095
Epoch 25 iteration 0036/0187: training loss 0.243; learning rate 0.000095
Epoch 25 iteration 0037/0187: training loss 0.246; learning rate 0.000095
Epoch 25 iteration 0038/0187: training loss 0.247; learning rate 0.000095
Epoch 25 iteration 0039/0187: training loss 0.245; learning rate 0.000095
Epoch 25 iteration 0040/0187: training loss 0.247; learning rate 0.000095
Epoch 25 iteration 0041/0187: training loss 0.247; learning rate 0.000094
Epoch 25 iteration 0042/0187: training loss 0.246; learning rate 0.000094
Epoch 25 iteration 0043/0187: training loss 0.247; learning rate 0.000094
Epoch 25 iteration 0044/0187: training loss 0.247; learning rate 0.000094
Epoch 25 iteration 0045/0187: training loss 0.251; learning rate 0.000094
Epoch 25 iteration 0046/0187: training loss 0.250; learning rate 0.000094
Epoch 25 iteration 0047/0187: training loss 0.249; learning rate 0.000094
Epoch 25 iteration 0048/0187: training loss 0.248; learning rate 0.000094
Epoch 25 iteration 0049/0187: training loss 0.249; learning rate 0.000094
Epoch 25 iteration 0050/0187: training loss 0.248; learning rate 0.000094
Epoch 25 iteration 0051/0187: training loss 0.249; learning rate 0.000093
Epoch 25 iteration 0052/0187: training loss 0.250; learning rate 0.000093
Epoch 25 iteration 0053/0187: training loss 0.249; learning rate 0.000093
Epoch 25 iteration 0054/0187: training loss 0.248; learning rate 0.000093
Epoch 25 iteration 0055/0187: training loss 0.249; learning rate 0.000093
Epoch 25 iteration 0056/0187: training loss 0.248; learning rate 0.000093
Epoch 25 iteration 0057/0187: training loss 0.250; learning rate 0.000093
Epoch 25 iteration 0058/0187: training loss 0.249; learning rate 0.000093
Epoch 25 iteration 0059/0187: training loss 0.248; learning rate 0.000093
Epoch 25 iteration 0060/0187: training loss 0.248; learning rate 0.000093
Epoch 25 iteration 0061/0187: training loss 0.247; learning rate 0.000092
Epoch 25 iteration 0062/0187: training loss 0.247; learning rate 0.000092
Epoch 25 iteration 0063/0187: training loss 0.248; learning rate 0.000092
Epoch 25 iteration 0064/0187: training loss 0.249; learning rate 0.000092
Epoch 25 iteration 0065/0187: training loss 0.248; learning rate 0.000092
Epoch 25 iteration 0066/0187: training loss 0.248; learning rate 0.000092
Epoch 25 iteration 0067/0187: training loss 0.248; learning rate 0.000092
Epoch 25 iteration 0068/0187: training loss 0.249; learning rate 0.000092
Epoch 25 iteration 0069/0187: training loss 0.250; learning rate 0.000092
Epoch 25 iteration 0070/0187: training loss 0.250; learning rate 0.000092
Epoch 25 iteration 0071/0187: training loss 0.249; learning rate 0.000092
Epoch 25 iteration 0072/0187: training loss 0.248; learning rate 0.000091
Epoch 25 iteration 0073/0187: training loss 0.248; learning rate 0.000091
Epoch 25 iteration 0074/0187: training loss 0.249; learning rate 0.000091
Epoch 25 iteration 0075/0187: training loss 0.249; learning rate 0.000091
Epoch 25 iteration 0076/0187: training loss 0.249; learning rate 0.000091
Epoch 25 iteration 0077/0187: training loss 0.249; learning rate 0.000091
Epoch 25 iteration 0078/0187: training loss 0.248; learning rate 0.000091
Epoch 25 iteration 0079/0187: training loss 0.250; learning rate 0.000091
Epoch 25 iteration 0080/0187: training loss 0.250; learning rate 0.000091
Epoch 25 iteration 0081/0187: training loss 0.250; learning rate 0.000091
Epoch 25 iteration 0082/0187: training loss 0.250; learning rate 0.000090
Epoch 25 iteration 0083/0187: training loss 0.250; learning rate 0.000090
Epoch 25 iteration 0084/0187: training loss 0.250; learning rate 0.000090
Epoch 25 iteration 0085/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0086/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0087/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0088/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0089/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0090/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0091/0187: training loss 0.252; learning rate 0.000090
Epoch 25 iteration 0092/0187: training loss 0.252; learning rate 0.000089
Epoch 25 iteration 0093/0187: training loss 0.252; learning rate 0.000089
Epoch 25 iteration 0094/0187: training loss 0.252; learning rate 0.000089
Epoch 25 iteration 0095/0187: training loss 0.253; learning rate 0.000089
Epoch 25 iteration 0096/0187: training loss 0.253; learning rate 0.000089
Epoch 25 iteration 0097/0187: training loss 0.254; learning rate 0.000089
Epoch 25 iteration 0098/0187: training loss 0.254; learning rate 0.000089
Epoch 25 iteration 0099/0187: training loss 0.254; learning rate 0.000089
Epoch 25 iteration 0100/0187: training loss 0.254; learning rate 0.000089
Epoch 25 iteration 0101/0187: training loss 0.253; learning rate 0.000089
Epoch 25 iteration 0102/0187: training loss 0.253; learning rate 0.000088
Epoch 25 iteration 0103/0187: training loss 0.253; learning rate 0.000088
Epoch 25 iteration 0104/0187: training loss 0.253; learning rate 0.000088
Epoch 25 iteration 0105/0187: training loss 0.253; learning rate 0.000088
Epoch 25 iteration 0106/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0107/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0108/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0109/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0110/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0111/0187: training loss 0.252; learning rate 0.000088
Epoch 25 iteration 0112/0187: training loss 0.253; learning rate 0.000088
Epoch 25 iteration 0113/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0114/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0115/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0116/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0117/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0118/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0119/0187: training loss 0.253; learning rate 0.000087
Epoch 25 iteration 0120/0187: training loss 0.254; learning rate 0.000087
Epoch 25 iteration 0121/0187: training loss 0.254; learning rate 0.000087
Epoch 25 iteration 0122/0187: training loss 0.254; learning rate 0.000087
Epoch 25 iteration 0123/0187: training loss 0.254; learning rate 0.000086
Epoch 25 iteration 0124/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0125/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0126/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0127/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0128/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0129/0187: training loss 0.252; learning rate 0.000086
Epoch 25 iteration 0130/0187: training loss 0.252; learning rate 0.000086
Epoch 25 iteration 0131/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0132/0187: training loss 0.253; learning rate 0.000086
Epoch 25 iteration 0133/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0134/0187: training loss 0.252; learning rate 0.000085
Epoch 25 iteration 0135/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0136/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0137/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0138/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0139/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0140/0187: training loss 0.254; learning rate 0.000085
Epoch 25 iteration 0141/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0142/0187: training loss 0.253; learning rate 0.000085
Epoch 25 iteration 0143/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0144/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0145/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0146/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0147/0187: training loss 0.252; learning rate 0.000084
Epoch 25 iteration 0148/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0149/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0150/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0151/0187: training loss 0.253; learning rate 0.000084
Epoch 25 iteration 0152/0187: training loss 0.252; learning rate 0.000084
Epoch 25 iteration 0153/0187: training loss 0.252; learning rate 0.000084
Epoch 25 iteration 0154/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0155/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0156/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0157/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0158/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0159/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0160/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0161/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0162/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0163/0187: training loss 0.252; learning rate 0.000083
Epoch 25 iteration 0164/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0165/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0166/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0167/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0168/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0169/0187: training loss 0.253; learning rate 0.000082
Epoch 25 iteration 0170/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0171/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0172/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0173/0187: training loss 0.252; learning rate 0.000082
Epoch 25 iteration 0174/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0175/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0176/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0177/0187: training loss 0.251; learning rate 0.000081
Epoch 25 iteration 0178/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0179/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0180/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0181/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0182/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0183/0187: training loss 0.252; learning rate 0.000081
Epoch 25 iteration 0184/0187: training loss 0.252; learning rate 0.000080
Epoch 25 iteration 0185/0187: training loss 0.252; learning rate 0.000080
Epoch 25 iteration 0186/0187: training loss 0.252; learning rate 0.000080
Epoch 25 iteration 0187/0187: training loss 0.252; learning rate 0.000080
Epoch 25 validation pixAcc: 0.338, mIoU: 0.205
Epoch 26 iteration 0001/0187: training loss 0.234; learning rate 0.000080
Epoch 26 iteration 0002/0187: training loss 0.237; learning rate 0.000080
Epoch 26 iteration 0003/0187: training loss 0.236; learning rate 0.000080
Epoch 26 iteration 0004/0187: training loss 0.247; learning rate 0.000080
Epoch 26 iteration 0005/0187: training loss 0.245; learning rate 0.000080
Epoch 26 iteration 0006/0187: training loss 0.242; learning rate 0.000079
Epoch 26 iteration 0007/0187: training loss 0.245; learning rate 0.000079
Epoch 26 iteration 0008/0187: training loss 0.241; learning rate 0.000079
Epoch 26 iteration 0009/0187: training loss 0.236; learning rate 0.000079
Epoch 26 iteration 0010/0187: training loss 0.233; learning rate 0.000079
Epoch 26 iteration 0011/0187: training loss 0.243; learning rate 0.000079
Epoch 26 iteration 0012/0187: training loss 0.257; learning rate 0.000079
Epoch 26 iteration 0013/0187: training loss 0.250; learning rate 0.000079
Epoch 26 iteration 0014/0187: training loss 0.246; learning rate 0.000079
Epoch 26 iteration 0015/0187: training loss 0.244; learning rate 0.000079
Epoch 26 iteration 0016/0187: training loss 0.242; learning rate 0.000079
Epoch 26 iteration 0017/0187: training loss 0.240; learning rate 0.000078
Epoch 26 iteration 0018/0187: training loss 0.245; learning rate 0.000078
Epoch 26 iteration 0019/0187: training loss 0.249; learning rate 0.000078
Epoch 26 iteration 0020/0187: training loss 0.247; learning rate 0.000078
Epoch 26 iteration 0021/0187: training loss 0.247; learning rate 0.000078
Epoch 26 iteration 0022/0187: training loss 0.250; learning rate 0.000078
Epoch 26 iteration 0023/0187: training loss 0.250; learning rate 0.000078
Epoch 26 iteration 0024/0187: training loss 0.248; learning rate 0.000078
Epoch 26 iteration 0025/0187: training loss 0.250; learning rate 0.000078
Epoch 26 iteration 0026/0187: training loss 0.255; learning rate 0.000078
Epoch 26 iteration 0027/0187: training loss 0.257; learning rate 0.000077
Epoch 26 iteration 0028/0187: training loss 0.257; learning rate 0.000077
Epoch 26 iteration 0029/0187: training loss 0.255; learning rate 0.000077
Epoch 26 iteration 0030/0187: training loss 0.258; learning rate 0.000077
Epoch 26 iteration 0031/0187: training loss 0.257; learning rate 0.000077
Epoch 26 iteration 0032/0187: training loss 0.256; learning rate 0.000077
Epoch 26 iteration 0033/0187: training loss 0.254; learning rate 0.000077
Epoch 26 iteration 0034/0187: training loss 0.254; learning rate 0.000077
Epoch 26 iteration 0035/0187: training loss 0.254; learning rate 0.000077
Epoch 26 iteration 0036/0187: training loss 0.256; learning rate 0.000077
Epoch 26 iteration 0037/0187: training loss 0.257; learning rate 0.000076
Epoch 26 iteration 0038/0187: training loss 0.258; learning rate 0.000076
Epoch 26 iteration 0039/0187: training loss 0.258; learning rate 0.000076
Epoch 26 iteration 0040/0187: training loss 0.257; learning rate 0.000076
Epoch 26 iteration 0041/0187: training loss 0.259; learning rate 0.000076
Epoch 26 iteration 0042/0187: training loss 0.260; learning rate 0.000076
Epoch 26 iteration 0043/0187: training loss 0.260; learning rate 0.000076
Epoch 26 iteration 0044/0187: training loss 0.259; learning rate 0.000076
Epoch 26 iteration 0045/0187: training loss 0.258; learning rate 0.000076
Epoch 26 iteration 0046/0187: training loss 0.257; learning rate 0.000076
Epoch 26 iteration 0047/0187: training loss 0.257; learning rate 0.000075
Epoch 26 iteration 0048/0187: training loss 0.255; learning rate 0.000075
Epoch 26 iteration 0049/0187: training loss 0.255; learning rate 0.000075
Epoch 26 iteration 0050/0187: training loss 0.253; learning rate 0.000075
Epoch 26 iteration 0051/0187: training loss 0.253; learning rate 0.000075
Epoch 26 iteration 0052/0187: training loss 0.253; learning rate 0.000075
Epoch 26 iteration 0053/0187: training loss 0.253; learning rate 0.000075
Epoch 26 iteration 0054/0187: training loss 0.253; learning rate 0.000075
Epoch 26 iteration 0055/0187: training loss 0.255; learning rate 0.000075
Epoch 26 iteration 0056/0187: training loss 0.254; learning rate 0.000075
Epoch 26 iteration 0057/0187: training loss 0.256; learning rate 0.000074
Epoch 26 iteration 0058/0187: training loss 0.256; learning rate 0.000074
Epoch 26 iteration 0059/0187: training loss 0.255; learning rate 0.000074
Epoch 26 iteration 0060/0187: training loss 0.255; learning rate 0.000074
Epoch 26 iteration 0061/0187: training loss 0.254; learning rate 0.000074
Epoch 26 iteration 0062/0187: training loss 0.255; learning rate 0.000074
Epoch 26 iteration 0063/0187: training loss 0.256; learning rate 0.000074
Epoch 26 iteration 0064/0187: training loss 0.255; learning rate 0.000074
Epoch 26 iteration 0065/0187: training loss 0.256; learning rate 0.000074
Epoch 26 iteration 0066/0187: training loss 0.258; learning rate 0.000074
Epoch 26 iteration 0067/0187: training loss 0.259; learning rate 0.000073
Epoch 26 iteration 0068/0187: training loss 0.258; learning rate 0.000073
Epoch 26 iteration 0069/0187: training loss 0.257; learning rate 0.000073
Epoch 26 iteration 0070/0187: training loss 0.259; learning rate 0.000073
Epoch 26 iteration 0071/0187: training loss 0.258; learning rate 0.000073
Epoch 26 iteration 0072/0187: training loss 0.258; learning rate 0.000073
Epoch 26 iteration 0073/0187: training loss 0.258; learning rate 0.000073
Epoch 26 iteration 0074/0187: training loss 0.259; learning rate 0.000073
Epoch 26 iteration 0075/0187: training loss 0.260; learning rate 0.000073
Epoch 26 iteration 0076/0187: training loss 0.259; learning rate 0.000073
Epoch 26 iteration 0077/0187: training loss 0.259; learning rate 0.000072
Epoch 26 iteration 0078/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0079/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0080/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0081/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0082/0187: training loss 0.259; learning rate 0.000072
Epoch 26 iteration 0083/0187: training loss 0.259; learning rate 0.000072
Epoch 26 iteration 0084/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0085/0187: training loss 0.258; learning rate 0.000072
Epoch 26 iteration 0086/0187: training loss 0.259; learning rate 0.000072
Epoch 26 iteration 0087/0187: training loss 0.259; learning rate 0.000071
Epoch 26 iteration 0088/0187: training loss 0.260; learning rate 0.000071
Epoch 26 iteration 0089/0187: training loss 0.259; learning rate 0.000071
Epoch 26 iteration 0090/0187: training loss 0.259; learning rate 0.000071
Epoch 26 iteration 0091/0188: training loss 0.260; learning rate 0.000071
Epoch 26 iteration 0092/0188: training loss 0.259; learning rate 0.000071
Epoch 26 iteration 0093/0188: training loss 0.259; learning rate 0.000071
Epoch 26 iteration 0094/0188: training loss 0.258; learning rate 0.000071
Epoch 26 iteration 0095/0188: training loss 0.258; learning rate 0.000071
Epoch 26 iteration 0096/0188: training loss 0.258; learning rate 0.000071
Epoch 26 iteration 0097/0188: training loss 0.258; learning rate 0.000070
Epoch 26 iteration 0098/0188: training loss 0.258; learning rate 0.000070
Epoch 26 iteration 0099/0188: training loss 0.258; learning rate 0.000070
Epoch 26 iteration 0100/0188: training loss 0.258; learning rate 0.000070
Epoch 26 iteration 0101/0188: training loss 0.257; learning rate 0.000070
Epoch 26 iteration 0102/0188: training loss 0.257; learning rate 0.000070
Epoch 26 iteration 0103/0188: training loss 0.257; learning rate 0.000070
Epoch 26 iteration 0104/0188: training loss 0.257; learning rate 0.000070
Epoch 26 iteration 0105/0188: training loss 0.257; learning rate 0.000070
Epoch 26 iteration 0106/0188: training loss 0.256; learning rate 0.000070
Epoch 26 iteration 0107/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0108/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0109/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0110/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0111/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0112/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0113/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0114/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0115/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0116/0188: training loss 0.255; learning rate 0.000069
Epoch 26 iteration 0117/0188: training loss 0.255; learning rate 0.000068
Epoch 26 iteration 0118/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0119/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0120/0188: training loss 0.257; learning rate 0.000068
Epoch 26 iteration 0121/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0122/0188: training loss 0.257; learning rate 0.000068
Epoch 26 iteration 0123/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0124/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0125/0188: training loss 0.256; learning rate 0.000068
Epoch 26 iteration 0126/0188: training loss 0.257; learning rate 0.000068
Epoch 26 iteration 0127/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0128/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0129/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0130/0188: training loss 0.258; learning rate 0.000067
Epoch 26 iteration 0131/0188: training loss 0.258; learning rate 0.000067
Epoch 26 iteration 0132/0188: training loss 0.258; learning rate 0.000067
Epoch 26 iteration 0133/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0134/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0135/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0136/0188: training loss 0.257; learning rate 0.000067
Epoch 26 iteration 0137/0188: training loss 0.257; learning rate 0.000066
Epoch 26 iteration 0138/0188: training loss 0.256; learning rate 0.000066
Epoch 26 iteration 0139/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0140/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0141/0188: training loss 0.256; learning rate 0.000066
Epoch 26 iteration 0142/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0143/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0144/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0145/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0146/0188: training loss 0.255; learning rate 0.000066
Epoch 26 iteration 0147/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0148/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0149/0188: training loss 0.256; learning rate 0.000065
Epoch 26 iteration 0150/0188: training loss 0.256; learning rate 0.000065
Epoch 26 iteration 0151/0188: training loss 0.256; learning rate 0.000065
Epoch 26 iteration 0152/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0153/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0154/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0155/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0156/0188: training loss 0.255; learning rate 0.000065
Epoch 26 iteration 0157/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0158/0188: training loss 0.255; learning rate 0.000064
Epoch 26 iteration 0159/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0160/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0161/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0162/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0163/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0164/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0165/0188: training loss 0.254; learning rate 0.000064
Epoch 26 iteration 0166/0188: training loss 0.255; learning rate 0.000064
Epoch 26 iteration 0167/0188: training loss 0.255; learning rate 0.000063
Epoch 26 iteration 0168/0188: training loss 0.255; learning rate 0.000063
Epoch 26 iteration 0169/0188: training loss 0.255; learning rate 0.000063
Epoch 26 iteration 0170/0188: training loss 0.255; learning rate 0.000063
Epoch 26 iteration 0171/0188: training loss 0.255; learning rate 0.000063
Epoch 26 iteration 0172/0188: training loss 0.254; learning rate 0.000063
Epoch 26 iteration 0173/0188: training loss 0.254; learning rate 0.000063
Epoch 26 iteration 0174/0188: training loss 0.254; learning rate 0.000063
Epoch 26 iteration 0175/0188: training loss 0.254; learning rate 0.000063
Epoch 26 iteration 0176/0188: training loss 0.253; learning rate 0.000063
Epoch 26 iteration 0177/0188: training loss 0.253; learning rate 0.000062
Epoch 26 iteration 0178/0188: training loss 0.254; learning rate 0.000062
Epoch 26 iteration 0179/0188: training loss 0.254; learning rate 0.000062
Epoch 26 iteration 0180/0188: training loss 0.253; learning rate 0.000062
Epoch 26 iteration 0181/0188: training loss 0.253; learning rate 0.000062
Epoch 26 iteration 0182/0188: training loss 0.254; learning rate 0.000062
Epoch 26 iteration 0183/0188: training loss 0.254; learning rate 0.000062
Epoch 26 iteration 0184/0188: training loss 0.254; learning rate 0.000062
Epoch 26 iteration 0185/0188: training loss 0.253; learning rate 0.000062
Epoch 26 iteration 0186/0188: training loss 0.254; learning rate 0.000062
Epoch 26 validation pixAcc: 0.338, mIoU: 0.204
Epoch 27 iteration 0001/0187: training loss 0.262; learning rate 0.000061
Epoch 27 iteration 0002/0187: training loss 0.256; learning rate 0.000061
Epoch 27 iteration 0003/0187: training loss 0.261; learning rate 0.000061
Epoch 27 iteration 0004/0187: training loss 0.255; learning rate 0.000061
Epoch 27 iteration 0005/0187: training loss 0.254; learning rate 0.000061
Epoch 27 iteration 0006/0187: training loss 0.253; learning rate 0.000061
Epoch 27 iteration 0007/0187: training loss 0.246; learning rate 0.000061
Epoch 27 iteration 0008/0187: training loss 0.247; learning rate 0.000061
Epoch 27 iteration 0009/0187: training loss 0.240; learning rate 0.000061
Epoch 27 iteration 0010/0187: training loss 0.241; learning rate 0.000060
Epoch 27 iteration 0011/0187: training loss 0.245; learning rate 0.000060
Epoch 27 iteration 0012/0187: training loss 0.243; learning rate 0.000060
Epoch 27 iteration 0013/0187: training loss 0.243; learning rate 0.000060
Epoch 27 iteration 0014/0187: training loss 0.245; learning rate 0.000060
Epoch 27 iteration 0015/0187: training loss 0.241; learning rate 0.000060
Epoch 27 iteration 0016/0187: training loss 0.236; learning rate 0.000060
Epoch 27 iteration 0017/0187: training loss 0.234; learning rate 0.000060
Epoch 27 iteration 0018/0187: training loss 0.235; learning rate 0.000060
Epoch 27 iteration 0019/0187: training loss 0.234; learning rate 0.000060
Epoch 27 iteration 0020/0187: training loss 0.232; learning rate 0.000059
Epoch 27 iteration 0021/0187: training loss 0.234; learning rate 0.000059
Epoch 27 iteration 0022/0187: training loss 0.236; learning rate 0.000059
Epoch 27 iteration 0023/0187: training loss 0.235; learning rate 0.000059
Epoch 27 iteration 0024/0187: training loss 0.236; learning rate 0.000059
Epoch 27 iteration 0025/0187: training loss 0.236; learning rate 0.000059
Epoch 27 iteration 0026/0187: training loss 0.234; learning rate 0.000059
Epoch 27 iteration 0027/0187: training loss 0.234; learning rate 0.000059
Epoch 27 iteration 0028/0187: training loss 0.234; learning rate 0.000059
Epoch 27 iteration 0029/0187: training loss 0.235; learning rate 0.000058
Epoch 27 iteration 0030/0187: training loss 0.238; learning rate 0.000058
Epoch 27 iteration 0031/0187: training loss 0.238; learning rate 0.000058
Epoch 27 iteration 0032/0187: training loss 0.237; learning rate 0.000058
Epoch 27 iteration 0033/0187: training loss 0.235; learning rate 0.000058
Epoch 27 iteration 0034/0187: training loss 0.236; learning rate 0.000058
Epoch 27 iteration 0035/0187: training loss 0.237; learning rate 0.000058
Epoch 27 iteration 0036/0187: training loss 0.239; learning rate 0.000058
Epoch 27 iteration 0037/0187: training loss 0.238; learning rate 0.000058
Epoch 27 iteration 0038/0187: training loss 0.239; learning rate 0.000058
Epoch 27 iteration 0039/0187: training loss 0.238; learning rate 0.000057
Epoch 27 iteration 0040/0187: training loss 0.238; learning rate 0.000057
Epoch 27 iteration 0041/0187: training loss 0.239; learning rate 0.000057
Epoch 27 iteration 0042/0187: training loss 0.240; learning rate 0.000057
Epoch 27 iteration 0043/0187: training loss 0.240; learning rate 0.000057
Epoch 27 iteration 0044/0187: training loss 0.241; learning rate 0.000057
Epoch 27 iteration 0045/0187: training loss 0.241; learning rate 0.000057
Epoch 27 iteration 0046/0187: training loss 0.240; learning rate 0.000057
Epoch 27 iteration 0047/0187: training loss 0.240; learning rate 0.000057
Epoch 27 iteration 0048/0187: training loss 0.240; learning rate 0.000057
Epoch 27 iteration 0049/0187: training loss 0.240; learning rate 0.000056
Epoch 27 iteration 0050/0187: training loss 0.238; learning rate 0.000056
Epoch 27 iteration 0051/0187: training loss 0.239; learning rate 0.000056
Epoch 27 iteration 0052/0187: training loss 0.240; learning rate 0.000056
Epoch 27 iteration 0053/0187: training loss 0.239; learning rate 0.000056
Epoch 27 iteration 0054/0187: training loss 0.240; learning rate 0.000056
Epoch 27 iteration 0055/0187: training loss 0.238; learning rate 0.000056
Epoch 27 iteration 0056/0187: training loss 0.239; learning rate 0.000056
Epoch 27 iteration 0057/0187: training loss 0.239; learning rate 0.000056
Epoch 27 iteration 0058/0187: training loss 0.239; learning rate 0.000056
Epoch 27 iteration 0059/0187: training loss 0.239; learning rate 0.000055
Epoch 27 iteration 0060/0187: training loss 0.239; learning rate 0.000055
Epoch 27 iteration 0061/0187: training loss 0.241; learning rate 0.000055
Epoch 27 iteration 0062/0187: training loss 0.241; learning rate 0.000055
Epoch 27 iteration 0063/0187: training loss 0.242; learning rate 0.000055
Epoch 27 iteration 0064/0187: training loss 0.243; learning rate 0.000055
Epoch 27 iteration 0065/0187: training loss 0.244; learning rate 0.000055
Epoch 27 iteration 0066/0187: training loss 0.243; learning rate 0.000055
Epoch 27 iteration 0067/0187: training loss 0.244; learning rate 0.000055
Epoch 27 iteration 0068/0187: training loss 0.243; learning rate 0.000055
Epoch 27 iteration 0069/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0070/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0071/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0072/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0073/0187: training loss 0.246; learning rate 0.000054
Epoch 27 iteration 0074/0187: training loss 0.246; learning rate 0.000054
Epoch 27 iteration 0075/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0076/0187: training loss 0.245; learning rate 0.000054
Epoch 27 iteration 0077/0187: training loss 0.244; learning rate 0.000054
Epoch 27 iteration 0078/0187: training loss 0.245; learning rate 0.000053
Epoch 27 iteration 0079/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0080/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0081/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0082/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0083/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0084/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0085/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0086/0187: training loss 0.244; learning rate 0.000053
Epoch 27 iteration 0087/0187: training loss 0.245; learning rate 0.000053
Epoch 27 iteration 0088/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0089/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0090/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0091/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0092/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0093/0187: training loss 0.244; learning rate 0.000052
Epoch 27 iteration 0094/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0095/0187: training loss 0.245; learning rate 0.000052
Epoch 27 iteration 0096/0187: training loss 0.246; learning rate 0.000052
Epoch 27 iteration 0097/0187: training loss 0.246; learning rate 0.000052
Epoch 27 iteration 0098/0187: training loss 0.246; learning rate 0.000051
Epoch 27 iteration 0099/0187: training loss 0.246; learning rate 0.000051
Epoch 27 iteration 0100/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0101/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0102/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0103/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0104/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0105/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0106/0187: training loss 0.245; learning rate 0.000051
Epoch 27 iteration 0107/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0108/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0109/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0110/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0111/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0112/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0113/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0114/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0115/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0116/0187: training loss 0.244; learning rate 0.000050
Epoch 27 iteration 0117/0187: training loss 0.245; learning rate 0.000049
Epoch 27 iteration 0118/0187: training loss 0.244; learning rate 0.000049
Epoch 27 iteration 0119/0187: training loss 0.244; learning rate 0.000049
Epoch 27 iteration 0120/0187: training loss 0.243; learning rate 0.000049
Epoch 27 iteration 0121/0187: training loss 0.244; learning rate 0.000049
Epoch 27 iteration 0122/0187: training loss 0.244; learning rate 0.000049
Epoch 27 iteration 0123/0187: training loss 0.244; learning rate 0.000049
Epoch 27 iteration 0124/0187: training loss 0.243; learning rate 0.000049
Epoch 27 iteration 0125/0187: training loss 0.243; learning rate 0.000049
Epoch 27 iteration 0126/0187: training loss 0.243; learning rate 0.000049
Epoch 27 iteration 0127/0187: training loss 0.244; learning rate 0.000048
Epoch 27 iteration 0128/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0129/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0130/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0131/0187: training loss 0.246; learning rate 0.000048
Epoch 27 iteration 0132/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0133/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0134/0187: training loss 0.245; learning rate 0.000048
Epoch 27 iteration 0135/0187: training loss 0.246; learning rate 0.000048
Epoch 27 iteration 0136/0187: training loss 0.246; learning rate 0.000047
Epoch 27 iteration 0137/0187: training loss 0.247; learning rate 0.000047
Epoch 27 iteration 0138/0187: training loss 0.246; learning rate 0.000047
Epoch 27 iteration 0139/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0140/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0141/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0142/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0143/0187: training loss 0.246; learning rate 0.000047
Epoch 27 iteration 0144/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0145/0187: training loss 0.245; learning rate 0.000047
Epoch 27 iteration 0146/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0147/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0148/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0149/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0150/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0151/0187: training loss 0.246; learning rate 0.000046
Epoch 27 iteration 0152/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0153/0187: training loss 0.245; learning rate 0.000046
Epoch 27 iteration 0154/0187: training loss 0.244; learning rate 0.000046
Epoch 27 iteration 0155/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0156/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0157/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0158/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0159/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0160/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0161/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0162/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0163/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0164/0187: training loss 0.245; learning rate 0.000045
Epoch 27 iteration 0165/0187: training loss 0.245; learning rate 0.000044
Epoch 27 iteration 0166/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0167/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0168/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0169/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0170/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0171/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0172/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0173/0187: training loss 0.246; learning rate 0.000044
Epoch 27 iteration 0174/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0175/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0176/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0177/0187: training loss 0.246; learning rate 0.000043
Epoch 27 iteration 0178/0187: training loss 0.246; learning rate 0.000043
Epoch 27 iteration 0179/0187: training loss 0.246; learning rate 0.000043
Epoch 27 iteration 0180/0187: training loss 0.246; learning rate 0.000043
Epoch 27 iteration 0181/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0182/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0183/0187: training loss 0.247; learning rate 0.000043
Epoch 27 iteration 0184/0187: training loss 0.247; learning rate 0.000042
Epoch 27 iteration 0185/0187: training loss 0.247; learning rate 0.000042
Epoch 27 iteration 0186/0187: training loss 0.247; learning rate 0.000042
Epoch 27 iteration 0187/0187: training loss 0.247; learning rate 0.000042
Epoch 27 validation pixAcc: 0.338, mIoU: 0.203
Epoch 28 iteration 0001/0187: training loss 0.250; learning rate 0.000042
Epoch 28 iteration 0002/0187: training loss 0.243; learning rate 0.000042
Epoch 28 iteration 0003/0187: training loss 0.256; learning rate 0.000042
Epoch 28 iteration 0004/0187: training loss 0.251; learning rate 0.000042
Epoch 28 iteration 0005/0187: training loss 0.257; learning rate 0.000041
Epoch 28 iteration 0006/0187: training loss 0.255; learning rate 0.000041
Epoch 28 iteration 0007/0187: training loss 0.250; learning rate 0.000041
Epoch 28 iteration 0008/0187: training loss 0.243; learning rate 0.000041
Epoch 28 iteration 0009/0187: training loss 0.245; learning rate 0.000041
Epoch 28 iteration 0010/0187: training loss 0.247; learning rate 0.000041
Epoch 28 iteration 0011/0187: training loss 0.246; learning rate 0.000041
Epoch 28 iteration 0012/0187: training loss 0.248; learning rate 0.000041
Epoch 28 iteration 0013/0187: training loss 0.256; learning rate 0.000041
Epoch 28 iteration 0014/0187: training loss 0.248; learning rate 0.000041
Epoch 28 iteration 0015/0187: training loss 0.246; learning rate 0.000040
Epoch 28 iteration 0016/0187: training loss 0.248; learning rate 0.000040
Epoch 28 iteration 0017/0187: training loss 0.246; learning rate 0.000040
Epoch 28 iteration 0018/0187: training loss 0.246; learning rate 0.000040
Epoch 28 iteration 0019/0187: training loss 0.247; learning rate 0.000040
Epoch 28 iteration 0020/0187: training loss 0.245; learning rate 0.000040
Epoch 28 iteration 0021/0187: training loss 0.248; learning rate 0.000040
Epoch 28 iteration 0022/0187: training loss 0.246; learning rate 0.000040
Epoch 28 iteration 0023/0187: training loss 0.247; learning rate 0.000040
Epoch 28 iteration 0024/0187: training loss 0.246; learning rate 0.000039
Epoch 28 iteration 0025/0187: training loss 0.246; learning rate 0.000039
Epoch 28 iteration 0026/0187: training loss 0.246; learning rate 0.000039
Epoch 28 iteration 0027/0187: training loss 0.244; learning rate 0.000039
Epoch 28 iteration 0028/0187: training loss 0.242; learning rate 0.000039
Epoch 28 iteration 0029/0187: training loss 0.243; learning rate 0.000039
Epoch 28 iteration 0030/0187: training loss 0.243; learning rate 0.000039
Epoch 28 iteration 0031/0187: training loss 0.241; learning rate 0.000039
Epoch 28 iteration 0032/0187: training loss 0.244; learning rate 0.000039
Epoch 28 iteration 0033/0187: training loss 0.244; learning rate 0.000039
Epoch 28 iteration 0034/0187: training loss 0.244; learning rate 0.000038
Epoch 28 iteration 0035/0187: training loss 0.244; learning rate 0.000038
Epoch 28 iteration 0036/0187: training loss 0.244; learning rate 0.000038
Epoch 28 iteration 0037/0187: training loss 0.242; learning rate 0.000038
Epoch 28 iteration 0038/0187: training loss 0.240; learning rate 0.000038
Epoch 28 iteration 0039/0187: training loss 0.242; learning rate 0.000038
Epoch 28 iteration 0040/0187: training loss 0.240; learning rate 0.000038
Epoch 28 iteration 0041/0187: training loss 0.239; learning rate 0.000038
Epoch 28 iteration 0042/0187: training loss 0.240; learning rate 0.000038
Epoch 28 iteration 0043/0187: training loss 0.238; learning rate 0.000037
Epoch 28 iteration 0044/0187: training loss 0.239; learning rate 0.000037
Epoch 28 iteration 0045/0187: training loss 0.239; learning rate 0.000037
Epoch 28 iteration 0046/0187: training loss 0.239; learning rate 0.000037
Epoch 28 iteration 0047/0187: training loss 0.238; learning rate 0.000037
Epoch 28 iteration 0048/0187: training loss 0.237; learning rate 0.000037
Epoch 28 iteration 0049/0187: training loss 0.238; learning rate 0.000037
Epoch 28 iteration 0050/0187: training loss 0.238; learning rate 0.000037
Epoch 28 iteration 0051/0187: training loss 0.239; learning rate 0.000037
Epoch 28 iteration 0052/0187: training loss 0.241; learning rate 0.000036
Epoch 28 iteration 0053/0187: training loss 0.240; learning rate 0.000036
Epoch 28 iteration 0054/0187: training loss 0.239; learning rate 0.000036
Epoch 28 iteration 0055/0187: training loss 0.238; learning rate 0.000036
Epoch 28 iteration 0056/0187: training loss 0.239; learning rate 0.000036
Epoch 28 iteration 0057/0187: training loss 0.242; learning rate 0.000036
Epoch 28 iteration 0058/0187: training loss 0.243; learning rate 0.000036
Epoch 28 iteration 0059/0187: training loss 0.243; learning rate 0.000036
Epoch 28 iteration 0060/0187: training loss 0.243; learning rate 0.000036
Epoch 28 iteration 0061/0187: training loss 0.245; learning rate 0.000036
Epoch 28 iteration 0062/0187: training loss 0.244; learning rate 0.000035
Epoch 28 iteration 0063/0187: training loss 0.243; learning rate 0.000035
Epoch 28 iteration 0064/0187: training loss 0.242; learning rate 0.000035
Epoch 28 iteration 0065/0187: training loss 0.243; learning rate 0.000035
Epoch 28 iteration 0066/0187: training loss 0.243; learning rate 0.000035
Epoch 28 iteration 0067/0187: training loss 0.243; learning rate 0.000035
Epoch 28 iteration 0068/0187: training loss 0.242; learning rate 0.000035
Epoch 28 iteration 0069/0187: training loss 0.241; learning rate 0.000035
Epoch 28 iteration 0070/0187: training loss 0.241; learning rate 0.000035
Epoch 28 iteration 0071/0187: training loss 0.241; learning rate 0.000034
Epoch 28 iteration 0072/0187: training loss 0.241; learning rate 0.000034
Epoch 28 iteration 0073/0187: training loss 0.242; learning rate 0.000034
Epoch 28 iteration 0074/0187: training loss 0.243; learning rate 0.000034
Epoch 28 iteration 0075/0187: training loss 0.244; learning rate 0.000034
Epoch 28 iteration 0076/0187: training loss 0.243; learning rate 0.000034
Epoch 28 iteration 0077/0187: training loss 0.244; learning rate 0.000034
Epoch 28 iteration 0078/0187: training loss 0.243; learning rate 0.000034
Epoch 28 iteration 0079/0187: training loss 0.243; learning rate 0.000034
Epoch 28 iteration 0080/0187: training loss 0.242; learning rate 0.000033
Epoch 28 iteration 0081/0187: training loss 0.242; learning rate 0.000033
Epoch 28 iteration 0082/0187: training loss 0.242; learning rate 0.000033
Epoch 28 iteration 0083/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0084/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0085/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0086/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0087/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0088/0187: training loss 0.243; learning rate 0.000033
Epoch 28 iteration 0089/0187: training loss 0.244; learning rate 0.000032
Epoch 28 iteration 0090/0187: training loss 0.243; learning rate 0.000032
Epoch 28 iteration 0091/0188: training loss 0.243; learning rate 0.000032
Epoch 28 iteration 0092/0188: training loss 0.243; learning rate 0.000032
Epoch 28 iteration 0093/0188: training loss 0.243; learning rate 0.000032
Epoch 28 iteration 0094/0188: training loss 0.244; learning rate 0.000032
Epoch 28 iteration 0095/0188: training loss 0.244; learning rate 0.000032
Epoch 28 iteration 0096/0188: training loss 0.244; learning rate 0.000032
Epoch 28 iteration 0097/0188: training loss 0.245; learning rate 0.000032
Epoch 28 iteration 0098/0188: training loss 0.245; learning rate 0.000032
Epoch 28 iteration 0099/0188: training loss 0.246; learning rate 0.000031
Epoch 28 iteration 0100/0188: training loss 0.245; learning rate 0.000031
Epoch 28 iteration 0101/0188: training loss 0.245; learning rate 0.000031
Epoch 28 iteration 0102/0188: training loss 0.245; learning rate 0.000031
Epoch 28 iteration 0103/0188: training loss 0.245; learning rate 0.000031
Epoch 28 iteration 0104/0188: training loss 0.245; learning rate 0.000031
Epoch 28 iteration 0105/0188: training loss 0.246; learning rate 0.000031
Epoch 28 iteration 0106/0188: training loss 0.246; learning rate 0.000031
Epoch 28 iteration 0107/0188: training loss 0.246; learning rate 0.000031
Epoch 28 iteration 0108/0188: training loss 0.247; learning rate 0.000030
Epoch 28 iteration 0109/0188: training loss 0.247; learning rate 0.000030
Epoch 28 iteration 0110/0188: training loss 0.247; learning rate 0.000030
Epoch 28 iteration 0111/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0112/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0113/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0114/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0115/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0116/0188: training loss 0.248; learning rate 0.000030
Epoch 28 iteration 0117/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0118/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0119/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0120/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0121/0188: training loss 0.247; learning rate 0.000029
Epoch 28 iteration 0122/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0123/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0124/0188: training loss 0.248; learning rate 0.000029
Epoch 28 iteration 0125/0188: training loss 0.247; learning rate 0.000029
Epoch 28 iteration 0126/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0127/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0128/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0129/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0130/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0131/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0132/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0133/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0134/0188: training loss 0.247; learning rate 0.000028
Epoch 28 iteration 0135/0188: training loss 0.247; learning rate 0.000027
Epoch 28 iteration 0136/0188: training loss 0.247; learning rate 0.000027
Epoch 28 iteration 0137/0188: training loss 0.247; learning rate 0.000027
Epoch 28 iteration 0138/0188: training loss 0.248; learning rate 0.000027
Epoch 28 iteration 0139/0188: training loss 0.247; learning rate 0.000027
Epoch 28 iteration 0140/0188: training loss 0.248; learning rate 0.000027
Epoch 28 iteration 0141/0188: training loss 0.248; learning rate 0.000027
Epoch 28 iteration 0142/0188: training loss 0.248; learning rate 0.000027
Epoch 28 iteration 0143/0188: training loss 0.247; learning rate 0.000027
Epoch 28 iteration 0144/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0145/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0146/0188: training loss 0.248; learning rate 0.000026
Epoch 28 iteration 0147/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0148/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0149/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0150/0188: training loss 0.248; learning rate 0.000026
Epoch 28 iteration 0151/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0152/0188: training loss 0.247; learning rate 0.000026
Epoch 28 iteration 0153/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0154/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0155/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0156/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0157/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0158/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0159/0188: training loss 0.248; learning rate 0.000025
Epoch 28 iteration 0160/0188: training loss 0.247; learning rate 0.000025
Epoch 28 iteration 0161/0188: training loss 0.247; learning rate 0.000025
Epoch 28 iteration 0162/0188: training loss 0.247; learning rate 0.000024
Epoch 28 iteration 0163/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0164/0188: training loss 0.247; learning rate 0.000024
Epoch 28 iteration 0165/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0166/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0167/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0168/0188: training loss 0.249; learning rate 0.000024
Epoch 28 iteration 0169/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0170/0188: training loss 0.248; learning rate 0.000024
Epoch 28 iteration 0171/0188: training loss 0.249; learning rate 0.000023
Epoch 28 iteration 0172/0188: training loss 0.248; learning rate 0.000023
Epoch 28 iteration 0173/0188: training loss 0.248; learning rate 0.000023
Epoch 28 iteration 0174/0188: training loss 0.248; learning rate 0.000023
Epoch 28 iteration 0175/0188: training loss 0.249; learning rate 0.000023
Epoch 28 iteration 0176/0188: training loss 0.248; learning rate 0.000023
Epoch 28 iteration 0177/0188: training loss 0.249; learning rate 0.000023
Epoch 28 iteration 0178/0188: training loss 0.249; learning rate 0.000023
Epoch 28 iteration 0179/0188: training loss 0.249; learning rate 0.000023
Epoch 28 iteration 0180/0188: training loss 0.249; learning rate 0.000022
Epoch 28 iteration 0181/0188: training loss 0.248; learning rate 0.000022
Epoch 28 iteration 0182/0188: training loss 0.248; learning rate 0.000022
Epoch 28 iteration 0183/0188: training loss 0.249; learning rate 0.000022
Epoch 28 iteration 0184/0188: training loss 0.248; learning rate 0.000022
Epoch 28 iteration 0185/0188: training loss 0.248; learning rate 0.000022
Epoch 28 iteration 0186/0188: training loss 0.248; learning rate 0.000022
Epoch 28 validation pixAcc: 0.339, mIoU: 0.204
Epoch 29 iteration 0001/0187: training loss 0.305; learning rate 0.000021
Epoch 29 iteration 0002/0187: training loss 0.281; learning rate 0.000021
Epoch 29 iteration 0003/0187: training loss 0.287; learning rate 0.000021
Epoch 29 iteration 0004/0187: training loss 0.287; learning rate 0.000021
Epoch 29 iteration 0005/0187: training loss 0.285; learning rate 0.000021
Epoch 29 iteration 0006/0187: training loss 0.292; learning rate 0.000021
Epoch 29 iteration 0007/0187: training loss 0.284; learning rate 0.000021
Epoch 29 iteration 0008/0187: training loss 0.272; learning rate 0.000021
Epoch 29 iteration 0009/0187: training loss 0.270; learning rate 0.000021
Epoch 29 iteration 0010/0187: training loss 0.272; learning rate 0.000020
Epoch 29 iteration 0011/0187: training loss 0.267; learning rate 0.000020
Epoch 29 iteration 0012/0187: training loss 0.260; learning rate 0.000020
Epoch 29 iteration 0013/0187: training loss 0.266; learning rate 0.000020
Epoch 29 iteration 0014/0187: training loss 0.268; learning rate 0.000020
Epoch 29 iteration 0015/0187: training loss 0.270; learning rate 0.000020
Epoch 29 iteration 0016/0187: training loss 0.267; learning rate 0.000020
Epoch 29 iteration 0017/0187: training loss 0.263; learning rate 0.000020
Epoch 29 iteration 0018/0187: training loss 0.263; learning rate 0.000020
Epoch 29 iteration 0019/0187: training loss 0.261; learning rate 0.000019
Epoch 29 iteration 0020/0187: training loss 0.259; learning rate 0.000019
Epoch 29 iteration 0021/0187: training loss 0.254; learning rate 0.000019
Epoch 29 iteration 0022/0187: training loss 0.258; learning rate 0.000019
Epoch 29 iteration 0023/0187: training loss 0.257; learning rate 0.000019
Epoch 29 iteration 0024/0187: training loss 0.254; learning rate 0.000019
Epoch 29 iteration 0025/0187: training loss 0.255; learning rate 0.000019
Epoch 29 iteration 0026/0187: training loss 0.254; learning rate 0.000019
Epoch 29 iteration 0027/0187: training loss 0.250; learning rate 0.000019
Epoch 29 iteration 0028/0187: training loss 0.251; learning rate 0.000018
Epoch 29 iteration 0029/0187: training loss 0.253; learning rate 0.000018
Epoch 29 iteration 0030/0187: training loss 0.253; learning rate 0.000018
Epoch 29 iteration 0031/0187: training loss 0.253; learning rate 0.000018
Epoch 29 iteration 0032/0187: training loss 0.253; learning rate 0.000018
Epoch 29 iteration 0033/0187: training loss 0.252; learning rate 0.000018
Epoch 29 iteration 0034/0187: training loss 0.251; learning rate 0.000018
Epoch 29 iteration 0035/0187: training loss 0.251; learning rate 0.000018
Epoch 29 iteration 0036/0187: training loss 0.253; learning rate 0.000017
Epoch 29 iteration 0037/0187: training loss 0.253; learning rate 0.000017
Epoch 29 iteration 0038/0187: training loss 0.252; learning rate 0.000017
Epoch 29 iteration 0039/0187: training loss 0.251; learning rate 0.000017
Epoch 29 iteration 0040/0187: training loss 0.251; learning rate 0.000017
Epoch 29 iteration 0041/0187: training loss 0.251; learning rate 0.000017
Epoch 29 iteration 0042/0187: training loss 0.253; learning rate 0.000017
Epoch 29 iteration 0043/0187: training loss 0.255; learning rate 0.000017
Epoch 29 iteration 0044/0187: training loss 0.253; learning rate 0.000017
Epoch 29 iteration 0045/0187: training loss 0.254; learning rate 0.000016
Epoch 29 iteration 0046/0187: training loss 0.257; learning rate 0.000016
Epoch 29 iteration 0047/0187: training loss 0.256; learning rate 0.000016
Epoch 29 iteration 0048/0187: training loss 0.256; learning rate 0.000016
Epoch 29 iteration 0049/0187: training loss 0.257; learning rate 0.000016
Epoch 29 iteration 0050/0187: training loss 0.257; learning rate 0.000016
Epoch 29 iteration 0051/0187: training loss 0.258; learning rate 0.000016
Epoch 29 iteration 0052/0187: training loss 0.258; learning rate 0.000016
Epoch 29 iteration 0053/0187: training loss 0.258; learning rate 0.000015
Epoch 29 iteration 0054/0187: training loss 0.259; learning rate 0.000015
Epoch 29 iteration 0055/0187: training loss 0.257; learning rate 0.000015
Epoch 29 iteration 0056/0187: training loss 0.256; learning rate 0.000015
Epoch 29 iteration 0057/0187: training loss 0.256; learning rate 0.000015
Epoch 29 iteration 0058/0187: training loss 0.254; learning rate 0.000015
Epoch 29 iteration 0059/0187: training loss 0.254; learning rate 0.000015
Epoch 29 iteration 0060/0187: training loss 0.254; learning rate 0.000015
Epoch 29 iteration 0061/0187: training loss 0.254; learning rate 0.000015
Epoch 29 iteration 0062/0187: training loss 0.253; learning rate 0.000014
Epoch 29 iteration 0063/0187: training loss 0.252; learning rate 0.000014
Epoch 29 iteration 0064/0187: training loss 0.251; learning rate 0.000014
Epoch 29 iteration 0065/0187: training loss 0.251; learning rate 0.000014
Epoch 29 iteration 0066/0187: training loss 0.251; learning rate 0.000014
Epoch 29 iteration 0067/0187: training loss 0.251; learning rate 0.000014
Epoch 29 iteration 0068/0187: training loss 0.250; learning rate 0.000014
Epoch 29 iteration 0069/0187: training loss 0.251; learning rate 0.000014
Epoch 29 iteration 0070/0187: training loss 0.251; learning rate 0.000013
Epoch 29 iteration 0071/0187: training loss 0.251; learning rate 0.000013
Epoch 29 iteration 0072/0187: training loss 0.251; learning rate 0.000013
Epoch 29 iteration 0073/0187: training loss 0.250; learning rate 0.000013
Epoch 29 iteration 0074/0187: training loss 0.250; learning rate 0.000013
Epoch 29 iteration 0075/0187: training loss 0.252; learning rate 0.000013
Epoch 29 iteration 0076/0187: training loss 0.252; learning rate 0.000013
Epoch 29 iteration 0077/0187: training loss 0.253; learning rate 0.000013
Epoch 29 iteration 0078/0187: training loss 0.252; learning rate 0.000012
Epoch 29 iteration 0079/0187: training loss 0.252; learning rate 0.000012
Epoch 29 iteration 0080/0187: training loss 0.252; learning rate 0.000012
Epoch 29 iteration 0081/0187: training loss 0.251; learning rate 0.000012
Epoch 29 iteration 0082/0187: training loss 0.251; learning rate 0.000012
Epoch 29 iteration 0083/0187: training loss 0.251; learning rate 0.000012
Epoch 29 iteration 0084/0187: training loss 0.253; learning rate 0.000012
Epoch 29 iteration 0085/0187: training loss 0.252; learning rate 0.000012
Epoch 29 iteration 0086/0187: training loss 0.252; learning rate 0.000012
Epoch 29 iteration 0087/0187: training loss 0.252; learning rate 0.000011
Epoch 29 iteration 0088/0187: training loss 0.252; learning rate 0.000011
Epoch 29 iteration 0089/0187: training loss 0.251; learning rate 0.000011
Epoch 29 iteration 0090/0187: training loss 0.252; learning rate 0.000011
Epoch 29 iteration 0091/0187: training loss 0.253; learning rate 0.000011
Epoch 29 iteration 0092/0187: training loss 0.253; learning rate 0.000011
Epoch 29 iteration 0093/0187: training loss 0.254; learning rate 0.000011
Epoch 29 iteration 0094/0187: training loss 0.254; learning rate 0.000011
Epoch 29 iteration 0095/0187: training loss 0.254; learning rate 0.000010
Epoch 29 iteration 0096/0187: training loss 0.254; learning rate 0.000010
Epoch 29 iteration 0097/0187: training loss 0.253; learning rate 0.000010
Epoch 29 iteration 0098/0187: training loss 0.252; learning rate 0.000010
Epoch 29 iteration 0099/0187: training loss 0.253; learning rate 0.000010
Epoch 29 iteration 0100/0187: training loss 0.253; learning rate 0.000010
Epoch 29 iteration 0101/0187: training loss 0.253; learning rate 0.000010
Epoch 29 iteration 0102/0187: training loss 0.253; learning rate 0.000010
Epoch 29 iteration 0103/0187: training loss 0.253; learning rate 0.000009
Epoch 29 iteration 0104/0187: training loss 0.252; learning rate 0.000009
Epoch 29 iteration 0105/0187: training loss 0.251; learning rate 0.000009
Epoch 29 iteration 0106/0187: training loss 0.251; learning rate 0.000009
Epoch 29 iteration 0107/0187: training loss 0.250; learning rate 0.000009
Epoch 29 iteration 0108/0187: training loss 0.250; learning rate 0.000009
Epoch 29 iteration 0109/0187: training loss 0.251; learning rate 0.000009
Epoch 29 iteration 0110/0187: training loss 0.251; learning rate 0.000009
Epoch 29 iteration 0111/0187: training loss 0.253; learning rate 0.000008
Epoch 29 iteration 0112/0187: training loss 0.252; learning rate 0.000008
Epoch 29 iteration 0113/0187: training loss 0.253; learning rate 0.000008
Epoch 29 iteration 0114/0187: training loss 0.253; learning rate 0.000008
Epoch 29 iteration 0115/0187: training loss 0.252; learning rate 0.000008
Epoch 29 iteration 0116/0187: training loss 0.252; learning rate 0.000008
Epoch 29 iteration 0117/0187: training loss 0.251; learning rate 0.000008
Epoch 29 iteration 0118/0187: training loss 0.252; learning rate 0.000008
Epoch 29 iteration 0119/0187: training loss 0.252; learning rate 0.000007
Epoch 29 iteration 0120/0187: training loss 0.252; learning rate 0.000007
Epoch 29 iteration 0121/0187: training loss 0.251; learning rate 0.000007
Epoch 29 iteration 0122/0187: training loss 0.251; learning rate 0.000007
Epoch 29 iteration 0123/0187: training loss 0.250; learning rate 0.000007
Epoch 29 iteration 0124/0187: training loss 0.250; learning rate 0.000007
Epoch 29 iteration 0125/0187: training loss 0.250; learning rate 0.000007
Epoch 29 iteration 0126/0187: training loss 0.250; learning rate 0.000006
Epoch 29 iteration 0127/0187: training loss 0.250; learning rate 0.000006
Epoch 29 iteration 0128/0187: training loss 0.250; learning rate 0.000006
Epoch 29 iteration 0129/0187: training loss 0.250; learning rate 0.000006
Epoch 29 iteration 0130/0187: training loss 0.249; learning rate 0.000006
Epoch 29 iteration 0131/0187: training loss 0.249; learning rate 0.000006
Epoch 29 iteration 0132/0187: training loss 0.249; learning rate 0.000006
Epoch 29 iteration 0133/0187: training loss 0.249; learning rate 0.000006
Epoch 29 iteration 0134/0187: training loss 0.249; learning rate 0.000005
Epoch 29 iteration 0135/0187: training loss 0.248; learning rate 0.000005
Epoch 29 iteration 0136/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0137/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0138/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0139/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0140/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0141/0187: training loss 0.247; learning rate 0.000005
Epoch 29 iteration 0142/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0143/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0144/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0145/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0146/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0147/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0148/0187: training loss 0.246; learning rate 0.000004
Epoch 29 iteration 0149/0187: training loss 0.246; learning rate 0.000003
Epoch 29 iteration 0150/0187: training loss 0.246; learning rate 0.000003
Epoch 29 iteration 0151/0187: training loss 0.245; learning rate 0.000003
Epoch 29 iteration 0152/0187: training loss 0.245; learning rate 0.000003
Epoch 29 iteration 0153/0187: training loss 0.245; learning rate 0.000003
Epoch 29 iteration 0154/0187: training loss 0.246; learning rate 0.000003
Epoch 29 iteration 0155/0187: training loss 0.245; learning rate 0.000003
Epoch 29 iteration 0156/0187: training loss 0.245; learning rate 0.000002
Epoch 29 iteration 0157/0187: training loss 0.245; learning rate 0.000002
Epoch 29 iteration 0158/0187: training loss 0.246; learning rate 0.000002
Epoch 29 iteration 0159/0187: training loss 0.245; learning rate 0.000002
Epoch 29 iteration 0160/0187: training loss 0.245; learning rate 0.000002
Epoch 29 iteration 0161/0187: training loss 0.246; learning rate 0.000002
Epoch 29 iteration 0162/0187: training loss 0.246; learning rate 0.000002
Epoch 29 iteration 0163/0187: training loss 0.245; learning rate 0.000001
Epoch 29 iteration 0164/0187: training loss 0.245; learning rate 0.000001
Epoch 29 iteration 0165/0187: training loss 0.245; learning rate 0.000001
Epoch 29 iteration 0166/0187: training loss 0.246; learning rate 0.000001
Epoch 29 iteration 0167/0187: training loss 0.246; learning rate 0.000001
Epoch 29 iteration 0168/0187: training loss 0.246; learning rate 0.000001
Epoch 29 iteration 0169/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0170/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0171/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0172/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0173/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0174/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0175/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0176/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0177/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0178/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0179/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0180/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0181/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0182/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0183/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0184/0187: training loss 0.246; learning rate 0.000000
Epoch 29 iteration 0185/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0186/0187: training loss 0.245; learning rate 0.000000
Epoch 29 iteration 0187/0187: training loss 0.245; learning rate 0.000000
Epoch 29 validation pixAcc: 0.339, mIoU: 0.205
