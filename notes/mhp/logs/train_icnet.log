Namespace(aux=False, aux_weight=0.5, backbone='resnet50', base_size=768, batch_size=16, checkname='icnet_resnet50_mhp', clip_grad=0, crop_size=768, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7)], dataset='mhp', dtype='float32', epochs=30, eval=False, kvstore='device', log_interval=1, logging_file='train.log', lr=0.0005, mode=None, model='icnet', model_zoo=None, momentum=0.9, ngpus=8, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 8}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, resume=None, save_dir='runs/mhp/icnet/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', weight_decay=0.0001, workers=48)
Starting Epoch: 0
Total Epochs: 30
Epoch 0 iteration 0001/0187: training loss 3.278; learning rate 0.000500
Epoch 0 iteration 0002/0187: training loss 3.176; learning rate 0.000500
Epoch 0 iteration 0003/0187: training loss 3.194; learning rate 0.000500
Epoch 0 iteration 0004/0187: training loss 2.927; learning rate 0.000500
Epoch 0 iteration 0005/0187: training loss 2.797; learning rate 0.000500
Epoch 0 iteration 0006/0187: training loss 2.735; learning rate 0.000499
Epoch 0 iteration 0007/0187: training loss 2.674; learning rate 0.000499
Epoch 0 iteration 0008/0187: training loss 2.688; learning rate 0.000499
Epoch 0 iteration 0009/0187: training loss 2.663; learning rate 0.000499
Epoch 0 iteration 0010/0187: training loss 2.653; learning rate 0.000499
Epoch 0 iteration 0011/0187: training loss 2.566; learning rate 0.000499
Epoch 0 iteration 0012/0187: training loss 2.540; learning rate 0.000499
Epoch 0 iteration 0013/0187: training loss 2.493; learning rate 0.000499
Epoch 0 iteration 0014/0187: training loss 2.491; learning rate 0.000499
Epoch 0 iteration 0015/0187: training loss 2.458; learning rate 0.000499
Epoch 0 iteration 0016/0187: training loss 2.413; learning rate 0.000499
Epoch 0 iteration 0017/0187: training loss 2.395; learning rate 0.000499
Epoch 0 iteration 0018/0187: training loss 2.358; learning rate 0.000498
Epoch 0 iteration 0019/0187: training loss 2.342; learning rate 0.000498
Epoch 0 iteration 0020/0187: training loss 2.367; learning rate 0.000498
Epoch 0 iteration 0021/0187: training loss 2.322; learning rate 0.000498
Epoch 0 iteration 0022/0187: training loss 2.280; learning rate 0.000498
Epoch 0 iteration 0023/0187: training loss 2.266; learning rate 0.000498
Epoch 0 iteration 0024/0187: training loss 2.260; learning rate 0.000498
Epoch 0 iteration 0025/0187: training loss 2.258; learning rate 0.000498
Epoch 0 iteration 0026/0187: training loss 2.241; learning rate 0.000498
Epoch 0 iteration 0027/0187: training loss 2.244; learning rate 0.000498
Epoch 0 iteration 0028/0187: training loss 2.254; learning rate 0.000498
Epoch 0 iteration 0029/0187: training loss 2.236; learning rate 0.000498
Epoch 0 iteration 0030/0187: training loss 2.223; learning rate 0.000498
Epoch 0 iteration 0031/0187: training loss 2.206; learning rate 0.000497
Epoch 0 iteration 0032/0187: training loss 2.195; learning rate 0.000497
Epoch 0 iteration 0033/0187: training loss 2.182; learning rate 0.000497
Epoch 0 iteration 0034/0187: training loss 2.152; learning rate 0.000497
Epoch 0 iteration 0035/0187: training loss 2.142; learning rate 0.000497
Epoch 0 iteration 0036/0187: training loss 2.124; learning rate 0.000497
Epoch 0 iteration 0037/0187: training loss 2.114; learning rate 0.000497
Epoch 0 iteration 0038/0187: training loss 2.106; learning rate 0.000497
Epoch 0 iteration 0039/0187: training loss 2.105; learning rate 0.000497
Epoch 0 iteration 0040/0187: training loss 2.084; learning rate 0.000497
Epoch 0 iteration 0041/0187: training loss 2.074; learning rate 0.000497
Epoch 0 iteration 0042/0187: training loss 2.073; learning rate 0.000497
Epoch 0 iteration 0043/0187: training loss 2.056; learning rate 0.000496
Epoch 0 iteration 0044/0187: training loss 2.072; learning rate 0.000496
Epoch 0 iteration 0045/0187: training loss 2.059; learning rate 0.000496
Epoch 0 iteration 0046/0187: training loss 2.054; learning rate 0.000496
Epoch 0 iteration 0047/0187: training loss 2.040; learning rate 0.000496
Epoch 0 iteration 0048/0187: training loss 2.044; learning rate 0.000496
Epoch 0 iteration 0049/0187: training loss 2.028; learning rate 0.000496
Epoch 0 iteration 0050/0187: training loss 2.029; learning rate 0.000496
Epoch 0 iteration 0051/0187: training loss 2.020; learning rate 0.000496
Epoch 0 iteration 0052/0187: training loss 2.020; learning rate 0.000496
Epoch 0 iteration 0053/0187: training loss 2.008; learning rate 0.000496
Epoch 0 iteration 0054/0187: training loss 1.998; learning rate 0.000496
Epoch 0 iteration 0055/0187: training loss 1.996; learning rate 0.000496
Epoch 0 iteration 0056/0187: training loss 1.979; learning rate 0.000495
Epoch 0 iteration 0057/0187: training loss 1.969; learning rate 0.000495
Epoch 0 iteration 0058/0187: training loss 1.962; learning rate 0.000495
Epoch 0 iteration 0059/0187: training loss 1.962; learning rate 0.000495
Epoch 0 iteration 0060/0187: training loss 1.952; learning rate 0.000495
Epoch 0 iteration 0061/0187: training loss 1.944; learning rate 0.000495
Epoch 0 iteration 0062/0187: training loss 1.936; learning rate 0.000495
Epoch 0 iteration 0063/0187: training loss 1.931; learning rate 0.000495
Epoch 0 iteration 0064/0187: training loss 1.916; learning rate 0.000495
Epoch 0 iteration 0065/0187: training loss 1.907; learning rate 0.000495
Epoch 0 iteration 0066/0187: training loss 1.899; learning rate 0.000495
Epoch 0 iteration 0067/0187: training loss 1.902; learning rate 0.000495
Epoch 0 iteration 0068/0187: training loss 1.891; learning rate 0.000494
Epoch 0 iteration 0069/0187: training loss 1.886; learning rate 0.000494
Epoch 0 iteration 0070/0187: training loss 1.880; learning rate 0.000494
Epoch 0 iteration 0071/0187: training loss 1.878; learning rate 0.000494
Epoch 0 iteration 0072/0187: training loss 1.868; learning rate 0.000494
Epoch 0 iteration 0073/0187: training loss 1.857; learning rate 0.000494
Epoch 0 iteration 0074/0187: training loss 1.848; learning rate 0.000494
Epoch 0 iteration 0075/0187: training loss 1.848; learning rate 0.000494
Epoch 0 iteration 0076/0187: training loss 1.844; learning rate 0.000494
Epoch 0 iteration 0077/0187: training loss 1.837; learning rate 0.000494
Epoch 0 iteration 0078/0187: training loss 1.825; learning rate 0.000494
Epoch 0 iteration 0079/0187: training loss 1.828; learning rate 0.000494
Epoch 0 iteration 0080/0187: training loss 1.823; learning rate 0.000493
Epoch 0 iteration 0081/0187: training loss 1.819; learning rate 0.000493
Epoch 0 iteration 0082/0187: training loss 1.815; learning rate 0.000493
Epoch 0 iteration 0083/0187: training loss 1.811; learning rate 0.000493
Epoch 0 iteration 0084/0187: training loss 1.807; learning rate 0.000493
Epoch 0 iteration 0085/0187: training loss 1.804; learning rate 0.000493
Epoch 0 iteration 0086/0187: training loss 1.799; learning rate 0.000493
Epoch 0 iteration 0087/0187: training loss 1.798; learning rate 0.000493
Epoch 0 iteration 0088/0187: training loss 1.788; learning rate 0.000493
Epoch 0 iteration 0089/0187: training loss 1.786; learning rate 0.000493
Epoch 0 iteration 0090/0187: training loss 1.782; learning rate 0.000493
Epoch 0 iteration 0091/0188: training loss 1.777; learning rate 0.000493
Epoch 0 iteration 0092/0188: training loss 1.769; learning rate 0.000493
Epoch 0 iteration 0093/0188: training loss 1.764; learning rate 0.000492
Epoch 0 iteration 0094/0188: training loss 1.758; learning rate 0.000492
Epoch 0 iteration 0095/0188: training loss 1.759; learning rate 0.000492
Epoch 0 iteration 0096/0188: training loss 1.751; learning rate 0.000492
Epoch 0 iteration 0097/0188: training loss 1.750; learning rate 0.000492
Epoch 0 iteration 0098/0188: training loss 1.749; learning rate 0.000492
Epoch 0 iteration 0099/0188: training loss 1.745; learning rate 0.000492
Epoch 0 iteration 0100/0188: training loss 1.739; learning rate 0.000492
Epoch 0 iteration 0101/0188: training loss 1.733; learning rate 0.000492
Epoch 0 iteration 0102/0188: training loss 1.732; learning rate 0.000492
Epoch 0 iteration 0103/0188: training loss 1.730; learning rate 0.000492
Epoch 0 iteration 0104/0188: training loss 1.725; learning rate 0.000492
Epoch 0 iteration 0105/0188: training loss 1.721; learning rate 0.000491
Epoch 0 iteration 0106/0188: training loss 1.714; learning rate 0.000491
Epoch 0 iteration 0107/0188: training loss 1.709; learning rate 0.000491
Epoch 0 iteration 0108/0188: training loss 1.705; learning rate 0.000491
Epoch 0 iteration 0109/0188: training loss 1.699; learning rate 0.000491
Epoch 0 iteration 0110/0188: training loss 1.695; learning rate 0.000491
Epoch 0 iteration 0111/0188: training loss 1.690; learning rate 0.000491
Epoch 0 iteration 0112/0188: training loss 1.685; learning rate 0.000491
Epoch 0 iteration 0113/0188: training loss 1.679; learning rate 0.000491
Epoch 0 iteration 0114/0188: training loss 1.673; learning rate 0.000491
Epoch 0 iteration 0115/0188: training loss 1.671; learning rate 0.000491
Epoch 0 iteration 0116/0188: training loss 1.668; learning rate 0.000491
Epoch 0 iteration 0117/0188: training loss 1.665; learning rate 0.000491
Epoch 0 iteration 0118/0188: training loss 1.657; learning rate 0.000490
Epoch 0 iteration 0119/0188: training loss 1.653; learning rate 0.000490
Epoch 0 iteration 0120/0188: training loss 1.648; learning rate 0.000490
Epoch 0 iteration 0121/0188: training loss 1.646; learning rate 0.000490
Epoch 0 iteration 0122/0188: training loss 1.648; learning rate 0.000490
Epoch 0 iteration 0123/0188: training loss 1.644; learning rate 0.000490
Epoch 0 iteration 0124/0188: training loss 1.639; learning rate 0.000490
Epoch 0 iteration 0125/0188: training loss 1.636; learning rate 0.000490
Epoch 0 iteration 0126/0188: training loss 1.633; learning rate 0.000490
Epoch 0 iteration 0127/0188: training loss 1.630; learning rate 0.000490
Epoch 0 iteration 0128/0188: training loss 1.629; learning rate 0.000490
Epoch 0 iteration 0129/0188: training loss 1.625; learning rate 0.000490
Epoch 0 iteration 0130/0188: training loss 1.621; learning rate 0.000489
Epoch 0 iteration 0131/0188: training loss 1.617; learning rate 0.000489
Epoch 0 iteration 0132/0188: training loss 1.612; learning rate 0.000489
Epoch 0 iteration 0133/0188: training loss 1.609; learning rate 0.000489
Epoch 0 iteration 0134/0188: training loss 1.608; learning rate 0.000489
Epoch 0 iteration 0135/0188: training loss 1.604; learning rate 0.000489
Epoch 0 iteration 0136/0188: training loss 1.600; learning rate 0.000489
Epoch 0 iteration 0137/0188: training loss 1.596; learning rate 0.000489
Epoch 0 iteration 0138/0188: training loss 1.594; learning rate 0.000489
Epoch 0 iteration 0139/0188: training loss 1.594; learning rate 0.000489
Epoch 0 iteration 0140/0188: training loss 1.593; learning rate 0.000489
Epoch 0 iteration 0141/0188: training loss 1.593; learning rate 0.000489
Epoch 0 iteration 0142/0188: training loss 1.588; learning rate 0.000489
Epoch 0 iteration 0143/0188: training loss 1.586; learning rate 0.000488
Epoch 0 iteration 0144/0188: training loss 1.582; learning rate 0.000488
Epoch 0 iteration 0145/0188: training loss 1.579; learning rate 0.000488
Epoch 0 iteration 0146/0188: training loss 1.574; learning rate 0.000488
Epoch 0 iteration 0147/0188: training loss 1.572; learning rate 0.000488
Epoch 0 iteration 0148/0188: training loss 1.572; learning rate 0.000488
Epoch 0 iteration 0149/0188: training loss 1.570; learning rate 0.000488
Epoch 0 iteration 0150/0188: training loss 1.566; learning rate 0.000488
Epoch 0 iteration 0151/0188: training loss 1.563; learning rate 0.000488
Epoch 0 iteration 0152/0188: training loss 1.565; learning rate 0.000488
Epoch 0 iteration 0153/0188: training loss 1.560; learning rate 0.000488
Epoch 0 iteration 0154/0188: training loss 1.559; learning rate 0.000488
Epoch 0 iteration 0155/0188: training loss 1.555; learning rate 0.000487
Epoch 0 iteration 0156/0188: training loss 1.550; learning rate 0.000487
Epoch 0 iteration 0157/0188: training loss 1.546; learning rate 0.000487
Epoch 0 iteration 0158/0188: training loss 1.544; learning rate 0.000487
Epoch 0 iteration 0159/0188: training loss 1.541; learning rate 0.000487
Epoch 0 iteration 0160/0188: training loss 1.537; learning rate 0.000487
Epoch 0 iteration 0161/0188: training loss 1.535; learning rate 0.000487
Epoch 0 iteration 0162/0188: training loss 1.532; learning rate 0.000487
Epoch 0 iteration 0163/0188: training loss 1.529; learning rate 0.000487
Epoch 0 iteration 0164/0188: training loss 1.524; learning rate 0.000487
Epoch 0 iteration 0165/0188: training loss 1.522; learning rate 0.000487
Epoch 0 iteration 0166/0188: training loss 1.520; learning rate 0.000487
Epoch 0 iteration 0167/0188: training loss 1.517; learning rate 0.000487
Epoch 0 iteration 0168/0188: training loss 1.513; learning rate 0.000486
Epoch 0 iteration 0169/0188: training loss 1.512; learning rate 0.000486
Epoch 0 iteration 0170/0188: training loss 1.508; learning rate 0.000486
Epoch 0 iteration 0171/0188: training loss 1.506; learning rate 0.000486
Epoch 0 iteration 0172/0188: training loss 1.502; learning rate 0.000486
Epoch 0 iteration 0173/0188: training loss 1.502; learning rate 0.000486
Epoch 0 iteration 0174/0188: training loss 1.501; learning rate 0.000486
Epoch 0 iteration 0175/0188: training loss 1.498; learning rate 0.000486
Epoch 0 iteration 0176/0188: training loss 1.495; learning rate 0.000486
Epoch 0 iteration 0177/0188: training loss 1.493; learning rate 0.000486
Epoch 0 iteration 0178/0188: training loss 1.490; learning rate 0.000486
Epoch 0 iteration 0179/0188: training loss 1.488; learning rate 0.000486
Epoch 0 iteration 0180/0188: training loss 1.485; learning rate 0.000485
Epoch 0 iteration 0181/0188: training loss 1.484; learning rate 0.000485
Epoch 0 iteration 0182/0188: training loss 1.482; learning rate 0.000485
Epoch 0 iteration 0183/0188: training loss 1.479; learning rate 0.000485
Epoch 0 iteration 0184/0188: training loss 1.477; learning rate 0.000485
Epoch 0 iteration 0185/0188: training loss 1.473; learning rate 0.000485
Epoch 0 iteration 0186/0188: training loss 1.472; learning rate 0.000485
Epoch 0 validation pixAcc: 0.303, mIoU: 0.125
Epoch 1 iteration 0001/0187: training loss 1.195; learning rate 0.000485
Epoch 1 iteration 0002/0187: training loss 1.027; learning rate 0.000485
Epoch 1 iteration 0003/0187: training loss 1.097; learning rate 0.000485
Epoch 1 iteration 0004/0187: training loss 1.076; learning rate 0.000485
Epoch 1 iteration 0005/0187: training loss 1.115; learning rate 0.000484
Epoch 1 iteration 0006/0187: training loss 1.098; learning rate 0.000484
Epoch 1 iteration 0007/0187: training loss 1.084; learning rate 0.000484
Epoch 1 iteration 0008/0187: training loss 1.095; learning rate 0.000484
Epoch 1 iteration 0009/0187: training loss 1.151; learning rate 0.000484
Epoch 1 iteration 0010/0187: training loss 1.148; learning rate 0.000484
Epoch 1 iteration 0011/0187: training loss 1.149; learning rate 0.000484
Epoch 1 iteration 0012/0187: training loss 1.144; learning rate 0.000484
Epoch 1 iteration 0013/0187: training loss 1.155; learning rate 0.000484
Epoch 1 iteration 0014/0187: training loss 1.134; learning rate 0.000484
Epoch 1 iteration 0015/0187: training loss 1.125; learning rate 0.000484
Epoch 1 iteration 0016/0187: training loss 1.113; learning rate 0.000484
Epoch 1 iteration 0017/0187: training loss 1.109; learning rate 0.000484
Epoch 1 iteration 0018/0187: training loss 1.104; learning rate 0.000483
Epoch 1 iteration 0019/0187: training loss 1.093; learning rate 0.000483
Epoch 1 iteration 0020/0187: training loss 1.082; learning rate 0.000483
Epoch 1 iteration 0021/0187: training loss 1.081; learning rate 0.000483
Epoch 1 iteration 0022/0187: training loss 1.086; learning rate 0.000483
Epoch 1 iteration 0023/0187: training loss 1.070; learning rate 0.000483
Epoch 1 iteration 0024/0187: training loss 1.073; learning rate 0.000483
Epoch 1 iteration 0025/0187: training loss 1.084; learning rate 0.000483
Epoch 1 iteration 0026/0187: training loss 1.101; learning rate 0.000483
Epoch 1 iteration 0027/0187: training loss 1.099; learning rate 0.000483
Epoch 1 iteration 0028/0187: training loss 1.091; learning rate 0.000483
Epoch 1 iteration 0029/0187: training loss 1.090; learning rate 0.000483
Epoch 1 iteration 0030/0187: training loss 1.094; learning rate 0.000482
Epoch 1 iteration 0031/0187: training loss 1.096; learning rate 0.000482
Epoch 1 iteration 0032/0187: training loss 1.088; learning rate 0.000482
Epoch 1 iteration 0033/0187: training loss 1.085; learning rate 0.000482
Epoch 1 iteration 0034/0187: training loss 1.080; learning rate 0.000482
Epoch 1 iteration 0035/0187: training loss 1.077; learning rate 0.000482
Epoch 1 iteration 0036/0187: training loss 1.077; learning rate 0.000482
Epoch 1 iteration 0037/0187: training loss 1.071; learning rate 0.000482
Epoch 1 iteration 0038/0187: training loss 1.072; learning rate 0.000482
Epoch 1 iteration 0039/0187: training loss 1.066; learning rate 0.000482
Epoch 1 iteration 0040/0187: training loss 1.078; learning rate 0.000482
Epoch 1 iteration 0041/0187: training loss 1.079; learning rate 0.000482
Epoch 1 iteration 0042/0187: training loss 1.079; learning rate 0.000482
Epoch 1 iteration 0043/0187: training loss 1.074; learning rate 0.000481
Epoch 1 iteration 0044/0187: training loss 1.072; learning rate 0.000481
Epoch 1 iteration 0045/0187: training loss 1.071; learning rate 0.000481
Epoch 1 iteration 0046/0187: training loss 1.072; learning rate 0.000481
Epoch 1 iteration 0047/0187: training loss 1.066; learning rate 0.000481
Epoch 1 iteration 0048/0187: training loss 1.063; learning rate 0.000481
Epoch 1 iteration 0049/0187: training loss 1.059; learning rate 0.000481
Epoch 1 iteration 0050/0187: training loss 1.054; learning rate 0.000481
Epoch 1 iteration 0051/0187: training loss 1.052; learning rate 0.000481
Epoch 1 iteration 0052/0187: training loss 1.049; learning rate 0.000481
Epoch 1 iteration 0053/0187: training loss 1.056; learning rate 0.000481
Epoch 1 iteration 0054/0187: training loss 1.056; learning rate 0.000481
Epoch 1 iteration 0055/0187: training loss 1.055; learning rate 0.000480
Epoch 1 iteration 0056/0187: training loss 1.055; learning rate 0.000480
Epoch 1 iteration 0057/0187: training loss 1.056; learning rate 0.000480
Epoch 1 iteration 0058/0187: training loss 1.052; learning rate 0.000480
Epoch 1 iteration 0059/0187: training loss 1.049; learning rate 0.000480
Epoch 1 iteration 0060/0187: training loss 1.051; learning rate 0.000480
Epoch 1 iteration 0061/0187: training loss 1.045; learning rate 0.000480
Epoch 1 iteration 0062/0187: training loss 1.043; learning rate 0.000480
Epoch 1 iteration 0063/0187: training loss 1.041; learning rate 0.000480
Epoch 1 iteration 0064/0187: training loss 1.039; learning rate 0.000480
Epoch 1 iteration 0065/0187: training loss 1.041; learning rate 0.000480
Epoch 1 iteration 0066/0187: training loss 1.039; learning rate 0.000480
Epoch 1 iteration 0067/0187: training loss 1.033; learning rate 0.000479
Epoch 1 iteration 0068/0187: training loss 1.030; learning rate 0.000479
Epoch 1 iteration 0069/0187: training loss 1.027; learning rate 0.000479
Epoch 1 iteration 0070/0187: training loss 1.029; learning rate 0.000479
Epoch 1 iteration 0071/0187: training loss 1.029; learning rate 0.000479
Epoch 1 iteration 0072/0187: training loss 1.027; learning rate 0.000479
Epoch 1 iteration 0073/0187: training loss 1.031; learning rate 0.000479
Epoch 1 iteration 0074/0187: training loss 1.030; learning rate 0.000479
Epoch 1 iteration 0075/0187: training loss 1.034; learning rate 0.000479
Epoch 1 iteration 0076/0187: training loss 1.035; learning rate 0.000479
Epoch 1 iteration 0077/0187: training loss 1.034; learning rate 0.000479
Epoch 1 iteration 0078/0187: training loss 1.041; learning rate 0.000479
Epoch 1 iteration 0079/0187: training loss 1.038; learning rate 0.000479
Epoch 1 iteration 0080/0187: training loss 1.038; learning rate 0.000478
Epoch 1 iteration 0081/0187: training loss 1.037; learning rate 0.000478
Epoch 1 iteration 0082/0187: training loss 1.036; learning rate 0.000478
Epoch 1 iteration 0083/0187: training loss 1.037; learning rate 0.000478
Epoch 1 iteration 0084/0187: training loss 1.043; learning rate 0.000478
Epoch 1 iteration 0085/0187: training loss 1.043; learning rate 0.000478
Epoch 1 iteration 0086/0187: training loss 1.039; learning rate 0.000478
Epoch 1 iteration 0087/0187: training loss 1.037; learning rate 0.000478
Epoch 1 iteration 0088/0187: training loss 1.036; learning rate 0.000478
Epoch 1 iteration 0089/0187: training loss 1.034; learning rate 0.000478
Epoch 1 iteration 0090/0187: training loss 1.033; learning rate 0.000478
Epoch 1 iteration 0091/0187: training loss 1.035; learning rate 0.000478
Epoch 1 iteration 0092/0187: training loss 1.034; learning rate 0.000477
Epoch 1 iteration 0093/0187: training loss 1.035; learning rate 0.000477
Epoch 1 iteration 0094/0187: training loss 1.033; learning rate 0.000477
Epoch 1 iteration 0095/0187: training loss 1.033; learning rate 0.000477
Epoch 1 iteration 0096/0187: training loss 1.039; learning rate 0.000477
Epoch 1 iteration 0097/0187: training loss 1.042; learning rate 0.000477
Epoch 1 iteration 0098/0187: training loss 1.041; learning rate 0.000477
Epoch 1 iteration 0099/0187: training loss 1.037; learning rate 0.000477
Epoch 1 iteration 0100/0187: training loss 1.040; learning rate 0.000477
Epoch 1 iteration 0101/0187: training loss 1.038; learning rate 0.000477
Epoch 1 iteration 0102/0187: training loss 1.038; learning rate 0.000477
Epoch 1 iteration 0103/0187: training loss 1.035; learning rate 0.000477
Epoch 1 iteration 0104/0187: training loss 1.037; learning rate 0.000477
Epoch 1 iteration 0105/0187: training loss 1.040; learning rate 0.000476
Epoch 1 iteration 0106/0187: training loss 1.039; learning rate 0.000476
Epoch 1 iteration 0107/0187: training loss 1.042; learning rate 0.000476
Epoch 1 iteration 0108/0187: training loss 1.042; learning rate 0.000476
Epoch 1 iteration 0109/0187: training loss 1.040; learning rate 0.000476
Epoch 1 iteration 0110/0187: training loss 1.039; learning rate 0.000476
Epoch 1 iteration 0111/0187: training loss 1.038; learning rate 0.000476
Epoch 1 iteration 0112/0187: training loss 1.040; learning rate 0.000476
Epoch 1 iteration 0113/0187: training loss 1.039; learning rate 0.000476
Epoch 1 iteration 0114/0187: training loss 1.040; learning rate 0.000476
Epoch 1 iteration 0115/0187: training loss 1.038; learning rate 0.000476
Epoch 1 iteration 0116/0187: training loss 1.034; learning rate 0.000476
Epoch 1 iteration 0117/0187: training loss 1.033; learning rate 0.000475
Epoch 1 iteration 0118/0187: training loss 1.032; learning rate 0.000475
Epoch 1 iteration 0119/0187: training loss 1.029; learning rate 0.000475
Epoch 1 iteration 0120/0187: training loss 1.027; learning rate 0.000475
Epoch 1 iteration 0121/0187: training loss 1.030; learning rate 0.000475
Epoch 1 iteration 0122/0187: training loss 1.027; learning rate 0.000475
Epoch 1 iteration 0123/0187: training loss 1.026; learning rate 0.000475
Epoch 1 iteration 0124/0187: training loss 1.027; learning rate 0.000475
Epoch 1 iteration 0125/0187: training loss 1.026; learning rate 0.000475
Epoch 1 iteration 0126/0187: training loss 1.027; learning rate 0.000475
Epoch 1 iteration 0127/0187: training loss 1.027; learning rate 0.000475
Epoch 1 iteration 0128/0187: training loss 1.025; learning rate 0.000475
Epoch 1 iteration 0129/0187: training loss 1.027; learning rate 0.000474
Epoch 1 iteration 0130/0187: training loss 1.027; learning rate 0.000474
Epoch 1 iteration 0131/0187: training loss 1.025; learning rate 0.000474
Epoch 1 iteration 0132/0187: training loss 1.026; learning rate 0.000474
Epoch 1 iteration 0133/0187: training loss 1.024; learning rate 0.000474
Epoch 1 iteration 0134/0187: training loss 1.022; learning rate 0.000474
Epoch 1 iteration 0135/0187: training loss 1.019; learning rate 0.000474
Epoch 1 iteration 0136/0187: training loss 1.019; learning rate 0.000474
Epoch 1 iteration 0137/0187: training loss 1.021; learning rate 0.000474
Epoch 1 iteration 0138/0187: training loss 1.019; learning rate 0.000474
Epoch 1 iteration 0139/0187: training loss 1.019; learning rate 0.000474
Epoch 1 iteration 0140/0187: training loss 1.018; learning rate 0.000474
Epoch 1 iteration 0141/0187: training loss 1.019; learning rate 0.000474
Epoch 1 iteration 0142/0187: training loss 1.016; learning rate 0.000473
Epoch 1 iteration 0143/0187: training loss 1.015; learning rate 0.000473
Epoch 1 iteration 0144/0187: training loss 1.017; learning rate 0.000473
Epoch 1 iteration 0145/0187: training loss 1.015; learning rate 0.000473
Epoch 1 iteration 0146/0187: training loss 1.015; learning rate 0.000473
Epoch 1 iteration 0147/0187: training loss 1.017; learning rate 0.000473
Epoch 1 iteration 0148/0187: training loss 1.015; learning rate 0.000473
Epoch 1 iteration 0149/0187: training loss 1.013; learning rate 0.000473
Epoch 1 iteration 0150/0187: training loss 1.012; learning rate 0.000473
Epoch 1 iteration 0151/0187: training loss 1.013; learning rate 0.000473
Epoch 1 iteration 0152/0187: training loss 1.013; learning rate 0.000473
Epoch 1 iteration 0153/0187: training loss 1.012; learning rate 0.000473
Epoch 1 iteration 0154/0187: training loss 1.012; learning rate 0.000472
Epoch 1 iteration 0155/0187: training loss 1.012; learning rate 0.000472
Epoch 1 iteration 0156/0187: training loss 1.010; learning rate 0.000472
Epoch 1 iteration 0157/0187: training loss 1.009; learning rate 0.000472
Epoch 1 iteration 0158/0187: training loss 1.010; learning rate 0.000472
Epoch 1 iteration 0159/0187: training loss 1.010; learning rate 0.000472
Epoch 1 iteration 0160/0187: training loss 1.012; learning rate 0.000472
Epoch 1 iteration 0161/0187: training loss 1.013; learning rate 0.000472
Epoch 1 iteration 0162/0187: training loss 1.013; learning rate 0.000472
Epoch 1 iteration 0163/0187: training loss 1.014; learning rate 0.000472
Epoch 1 iteration 0164/0187: training loss 1.015; learning rate 0.000472
Epoch 1 iteration 0165/0187: training loss 1.014; learning rate 0.000472
Epoch 1 iteration 0166/0187: training loss 1.014; learning rate 0.000472
Epoch 1 iteration 0167/0187: training loss 1.014; learning rate 0.000471
Epoch 1 iteration 0168/0187: training loss 1.011; learning rate 0.000471
Epoch 1 iteration 0169/0187: training loss 1.011; learning rate 0.000471
Epoch 1 iteration 0170/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0171/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0172/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0173/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0174/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0175/0187: training loss 1.011; learning rate 0.000471
Epoch 1 iteration 0176/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0177/0187: training loss 1.012; learning rate 0.000471
Epoch 1 iteration 0178/0187: training loss 1.010; learning rate 0.000471
Epoch 1 iteration 0179/0187: training loss 1.009; learning rate 0.000470
Epoch 1 iteration 0180/0187: training loss 1.009; learning rate 0.000470
Epoch 1 iteration 0181/0187: training loss 1.008; learning rate 0.000470
Epoch 1 iteration 0182/0187: training loss 1.009; learning rate 0.000470
Epoch 1 iteration 0183/0187: training loss 1.009; learning rate 0.000470
Epoch 1 iteration 0184/0187: training loss 1.008; learning rate 0.000470
Epoch 1 iteration 0185/0187: training loss 1.007; learning rate 0.000470
Epoch 1 iteration 0186/0187: training loss 1.007; learning rate 0.000470
Epoch 1 iteration 0187/0187: training loss 1.006; learning rate 0.000470
Epoch 1 validation pixAcc: 0.319, mIoU: 0.140
Epoch 2 iteration 0001/0187: training loss 0.859; learning rate 0.000470
Epoch 2 iteration 0002/0187: training loss 0.892; learning rate 0.000470
Epoch 2 iteration 0003/0187: training loss 0.903; learning rate 0.000469
Epoch 2 iteration 0004/0187: training loss 0.878; learning rate 0.000469
Epoch 2 iteration 0005/0187: training loss 0.869; learning rate 0.000469
Epoch 2 iteration 0006/0187: training loss 0.850; learning rate 0.000469
Epoch 2 iteration 0007/0187: training loss 0.840; learning rate 0.000469
Epoch 2 iteration 0008/0187: training loss 0.826; learning rate 0.000469
Epoch 2 iteration 0009/0187: training loss 0.825; learning rate 0.000469
Epoch 2 iteration 0010/0187: training loss 0.864; learning rate 0.000469
Epoch 2 iteration 0011/0187: training loss 0.874; learning rate 0.000469
Epoch 2 iteration 0012/0187: training loss 0.885; learning rate 0.000469
Epoch 2 iteration 0013/0187: training loss 0.878; learning rate 0.000469
Epoch 2 iteration 0014/0187: training loss 0.882; learning rate 0.000469
Epoch 2 iteration 0015/0187: training loss 0.884; learning rate 0.000469
Epoch 2 iteration 0016/0187: training loss 0.883; learning rate 0.000468
Epoch 2 iteration 0017/0187: training loss 0.871; learning rate 0.000468
Epoch 2 iteration 0018/0187: training loss 0.855; learning rate 0.000468
Epoch 2 iteration 0019/0187: training loss 0.869; learning rate 0.000468
Epoch 2 iteration 0020/0187: training loss 0.869; learning rate 0.000468
Epoch 2 iteration 0021/0187: training loss 0.859; learning rate 0.000468
Epoch 2 iteration 0022/0187: training loss 0.855; learning rate 0.000468
Epoch 2 iteration 0023/0187: training loss 0.871; learning rate 0.000468
Epoch 2 iteration 0024/0187: training loss 0.882; learning rate 0.000468
Epoch 2 iteration 0025/0187: training loss 0.877; learning rate 0.000468
Epoch 2 iteration 0026/0187: training loss 0.880; learning rate 0.000468
Epoch 2 iteration 0027/0187: training loss 0.881; learning rate 0.000468
Epoch 2 iteration 0028/0187: training loss 0.875; learning rate 0.000467
Epoch 2 iteration 0029/0187: training loss 0.882; learning rate 0.000467
Epoch 2 iteration 0030/0187: training loss 0.883; learning rate 0.000467
Epoch 2 iteration 0031/0187: training loss 0.879; learning rate 0.000467
Epoch 2 iteration 0032/0187: training loss 0.877; learning rate 0.000467
Epoch 2 iteration 0033/0187: training loss 0.874; learning rate 0.000467
Epoch 2 iteration 0034/0187: training loss 0.878; learning rate 0.000467
Epoch 2 iteration 0035/0187: training loss 0.878; learning rate 0.000467
Epoch 2 iteration 0036/0187: training loss 0.876; learning rate 0.000467
Epoch 2 iteration 0037/0187: training loss 0.877; learning rate 0.000467
Epoch 2 iteration 0038/0187: training loss 0.879; learning rate 0.000467
Epoch 2 iteration 0039/0187: training loss 0.883; learning rate 0.000467
Epoch 2 iteration 0040/0187: training loss 0.885; learning rate 0.000466
Epoch 2 iteration 0041/0187: training loss 0.881; learning rate 0.000466
Epoch 2 iteration 0042/0187: training loss 0.882; learning rate 0.000466
Epoch 2 iteration 0043/0187: training loss 0.883; learning rate 0.000466
Epoch 2 iteration 0044/0187: training loss 0.880; learning rate 0.000466
Epoch 2 iteration 0045/0187: training loss 0.885; learning rate 0.000466
Epoch 2 iteration 0046/0187: training loss 0.888; learning rate 0.000466
Epoch 2 iteration 0047/0187: training loss 0.897; learning rate 0.000466
Epoch 2 iteration 0048/0187: training loss 0.897; learning rate 0.000466
Epoch 2 iteration 0049/0187: training loss 0.904; learning rate 0.000466
Epoch 2 iteration 0050/0187: training loss 0.905; learning rate 0.000466
Epoch 2 iteration 0051/0187: training loss 0.906; learning rate 0.000466
Epoch 2 iteration 0052/0187: training loss 0.915; learning rate 0.000466
Epoch 2 iteration 0053/0187: training loss 0.913; learning rate 0.000465
Epoch 2 iteration 0054/0187: training loss 0.909; learning rate 0.000465
Epoch 2 iteration 0055/0187: training loss 0.909; learning rate 0.000465
Epoch 2 iteration 0056/0187: training loss 0.907; learning rate 0.000465
Epoch 2 iteration 0057/0187: training loss 0.906; learning rate 0.000465
Epoch 2 iteration 0058/0187: training loss 0.907; learning rate 0.000465
Epoch 2 iteration 0059/0187: training loss 0.905; learning rate 0.000465
Epoch 2 iteration 0060/0187: training loss 0.906; learning rate 0.000465
Epoch 2 iteration 0061/0187: training loss 0.908; learning rate 0.000465
Epoch 2 iteration 0062/0187: training loss 0.907; learning rate 0.000465
Epoch 2 iteration 0063/0187: training loss 0.905; learning rate 0.000465
Epoch 2 iteration 0064/0187: training loss 0.904; learning rate 0.000465
Epoch 2 iteration 0065/0187: training loss 0.902; learning rate 0.000464
Epoch 2 iteration 0066/0187: training loss 0.899; learning rate 0.000464
Epoch 2 iteration 0067/0187: training loss 0.900; learning rate 0.000464
Epoch 2 iteration 0068/0187: training loss 0.898; learning rate 0.000464
Epoch 2 iteration 0069/0187: training loss 0.898; learning rate 0.000464
Epoch 2 iteration 0070/0187: training loss 0.895; learning rate 0.000464
Epoch 2 iteration 0071/0187: training loss 0.897; learning rate 0.000464
Epoch 2 iteration 0072/0187: training loss 0.893; learning rate 0.000464
Epoch 2 iteration 0073/0187: training loss 0.896; learning rate 0.000464
Epoch 2 iteration 0074/0187: training loss 0.893; learning rate 0.000464
Epoch 2 iteration 0075/0187: training loss 0.892; learning rate 0.000464
Epoch 2 iteration 0076/0187: training loss 0.889; learning rate 0.000464
Epoch 2 iteration 0077/0187: training loss 0.888; learning rate 0.000464
Epoch 2 iteration 0078/0187: training loss 0.887; learning rate 0.000463
Epoch 2 iteration 0079/0187: training loss 0.885; learning rate 0.000463
Epoch 2 iteration 0080/0187: training loss 0.880; learning rate 0.000463
Epoch 2 iteration 0081/0187: training loss 0.879; learning rate 0.000463
Epoch 2 iteration 0082/0187: training loss 0.883; learning rate 0.000463
Epoch 2 iteration 0083/0187: training loss 0.884; learning rate 0.000463
Epoch 2 iteration 0084/0187: training loss 0.886; learning rate 0.000463
Epoch 2 iteration 0085/0187: training loss 0.885; learning rate 0.000463
Epoch 2 iteration 0086/0187: training loss 0.885; learning rate 0.000463
Epoch 2 iteration 0087/0187: training loss 0.884; learning rate 0.000463
Epoch 2 iteration 0088/0187: training loss 0.882; learning rate 0.000463
Epoch 2 iteration 0089/0187: training loss 0.883; learning rate 0.000463
Epoch 2 iteration 0090/0187: training loss 0.885; learning rate 0.000462
Epoch 2 iteration 0091/0188: training loss 0.886; learning rate 0.000462
Epoch 2 iteration 0092/0188: training loss 0.886; learning rate 0.000462
Epoch 2 iteration 0093/0188: training loss 0.883; learning rate 0.000462
Epoch 2 iteration 0094/0188: training loss 0.883; learning rate 0.000462
Epoch 2 iteration 0095/0188: training loss 0.882; learning rate 0.000462
Epoch 2 iteration 0096/0188: training loss 0.881; learning rate 0.000462
Epoch 2 iteration 0097/0188: training loss 0.880; learning rate 0.000462
Epoch 2 iteration 0098/0188: training loss 0.882; learning rate 0.000462
Epoch 2 iteration 0099/0188: training loss 0.884; learning rate 0.000462
Epoch 2 iteration 0100/0188: training loss 0.885; learning rate 0.000462
Epoch 2 iteration 0101/0188: training loss 0.886; learning rate 0.000462
Epoch 2 iteration 0102/0188: training loss 0.884; learning rate 0.000461
Epoch 2 iteration 0103/0188: training loss 0.886; learning rate 0.000461
Epoch 2 iteration 0104/0188: training loss 0.885; learning rate 0.000461
Epoch 2 iteration 0105/0188: training loss 0.884; learning rate 0.000461
Epoch 2 iteration 0106/0188: training loss 0.884; learning rate 0.000461
Epoch 2 iteration 0107/0188: training loss 0.885; learning rate 0.000461
Epoch 2 iteration 0108/0188: training loss 0.885; learning rate 0.000461
Epoch 2 iteration 0109/0188: training loss 0.886; learning rate 0.000461
Epoch 2 iteration 0110/0188: training loss 0.886; learning rate 0.000461
Epoch 2 iteration 0111/0188: training loss 0.885; learning rate 0.000461
Epoch 2 iteration 0112/0188: training loss 0.883; learning rate 0.000461
Epoch 2 iteration 0113/0188: training loss 0.885; learning rate 0.000461
Epoch 2 iteration 0114/0188: training loss 0.886; learning rate 0.000461
Epoch 2 iteration 0115/0188: training loss 0.884; learning rate 0.000460
Epoch 2 iteration 0116/0188: training loss 0.884; learning rate 0.000460
Epoch 2 iteration 0117/0188: training loss 0.882; learning rate 0.000460
Epoch 2 iteration 0118/0188: training loss 0.883; learning rate 0.000460
Epoch 2 iteration 0119/0188: training loss 0.885; learning rate 0.000460
Epoch 2 iteration 0120/0188: training loss 0.883; learning rate 0.000460
Epoch 2 iteration 0121/0188: training loss 0.882; learning rate 0.000460
Epoch 2 iteration 0122/0188: training loss 0.882; learning rate 0.000460
Epoch 2 iteration 0123/0188: training loss 0.879; learning rate 0.000460
Epoch 2 iteration 0124/0188: training loss 0.881; learning rate 0.000460
Epoch 2 iteration 0125/0188: training loss 0.881; learning rate 0.000460
Epoch 2 iteration 0126/0188: training loss 0.881; learning rate 0.000460
Epoch 2 iteration 0127/0188: training loss 0.883; learning rate 0.000459
Epoch 2 iteration 0128/0188: training loss 0.883; learning rate 0.000459
Epoch 2 iteration 0129/0188: training loss 0.884; learning rate 0.000459
Epoch 2 iteration 0130/0188: training loss 0.885; learning rate 0.000459
Epoch 2 iteration 0131/0188: training loss 0.885; learning rate 0.000459
Epoch 2 iteration 0132/0188: training loss 0.884; learning rate 0.000459
Epoch 2 iteration 0133/0188: training loss 0.885; learning rate 0.000459
Epoch 2 iteration 0134/0188: training loss 0.885; learning rate 0.000459
Epoch 2 iteration 0135/0188: training loss 0.886; learning rate 0.000459
Epoch 2 iteration 0136/0188: training loss 0.886; learning rate 0.000459
Epoch 2 iteration 0137/0188: training loss 0.886; learning rate 0.000459
Epoch 2 iteration 0138/0188: training loss 0.886; learning rate 0.000459
Epoch 2 iteration 0139/0188: training loss 0.885; learning rate 0.000458
Epoch 2 iteration 0140/0188: training loss 0.886; learning rate 0.000458
Epoch 2 iteration 0141/0188: training loss 0.886; learning rate 0.000458
Epoch 2 iteration 0142/0188: training loss 0.884; learning rate 0.000458
Epoch 2 iteration 0143/0188: training loss 0.884; learning rate 0.000458
Epoch 2 iteration 0144/0188: training loss 0.884; learning rate 0.000458
Epoch 2 iteration 0145/0188: training loss 0.882; learning rate 0.000458
Epoch 2 iteration 0146/0188: training loss 0.883; learning rate 0.000458
Epoch 2 iteration 0147/0188: training loss 0.882; learning rate 0.000458
Epoch 2 iteration 0148/0188: training loss 0.883; learning rate 0.000458
Epoch 2 iteration 0149/0188: training loss 0.883; learning rate 0.000458
Epoch 2 iteration 0150/0188: training loss 0.882; learning rate 0.000458
Epoch 2 iteration 0151/0188: training loss 0.882; learning rate 0.000458
Epoch 2 iteration 0152/0188: training loss 0.883; learning rate 0.000457
Epoch 2 iteration 0153/0188: training loss 0.882; learning rate 0.000457
Epoch 2 iteration 0154/0188: training loss 0.882; learning rate 0.000457
Epoch 2 iteration 0155/0188: training loss 0.883; learning rate 0.000457
Epoch 2 iteration 0156/0188: training loss 0.883; learning rate 0.000457
Epoch 2 iteration 0157/0188: training loss 0.883; learning rate 0.000457
Epoch 2 iteration 0158/0188: training loss 0.886; learning rate 0.000457
Epoch 2 iteration 0159/0188: training loss 0.886; learning rate 0.000457
Epoch 2 iteration 0160/0188: training loss 0.884; learning rate 0.000457
Epoch 2 iteration 0161/0188: training loss 0.883; learning rate 0.000457
Epoch 2 iteration 0162/0188: training loss 0.882; learning rate 0.000457
Epoch 2 iteration 0163/0188: training loss 0.881; learning rate 0.000457
Epoch 2 iteration 0164/0188: training loss 0.880; learning rate 0.000456
Epoch 2 iteration 0165/0188: training loss 0.879; learning rate 0.000456
Epoch 2 iteration 0166/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0167/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0168/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0169/0188: training loss 0.884; learning rate 0.000456
Epoch 2 iteration 0170/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0171/0188: training loss 0.882; learning rate 0.000456
Epoch 2 iteration 0172/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0173/0188: training loss 0.882; learning rate 0.000456
Epoch 2 iteration 0174/0188: training loss 0.883; learning rate 0.000456
Epoch 2 iteration 0175/0188: training loss 0.884; learning rate 0.000456
Epoch 2 iteration 0176/0188: training loss 0.884; learning rate 0.000455
Epoch 2 iteration 0177/0188: training loss 0.884; learning rate 0.000455
Epoch 2 iteration 0178/0188: training loss 0.883; learning rate 0.000455
Epoch 2 iteration 0179/0188: training loss 0.882; learning rate 0.000455
Epoch 2 iteration 0180/0188: training loss 0.883; learning rate 0.000455
Epoch 2 iteration 0181/0188: training loss 0.883; learning rate 0.000455
Epoch 2 iteration 0182/0188: training loss 0.882; learning rate 0.000455
Epoch 2 iteration 0183/0188: training loss 0.883; learning rate 0.000455
Epoch 2 iteration 0184/0188: training loss 0.882; learning rate 0.000455
Epoch 2 iteration 0185/0188: training loss 0.881; learning rate 0.000455
Epoch 2 iteration 0186/0188: training loss 0.882; learning rate 0.000455
Epoch 2 validation pixAcc: 0.328, mIoU: 0.149
Epoch 3 iteration 0001/0187: training loss 1.014; learning rate 0.000455
Epoch 3 iteration 0002/0187: training loss 0.987; learning rate 0.000454
Epoch 3 iteration 0003/0187: training loss 0.895; learning rate 0.000454
Epoch 3 iteration 0004/0187: training loss 0.874; learning rate 0.000454
Epoch 3 iteration 0005/0187: training loss 0.855; learning rate 0.000454
Epoch 3 iteration 0006/0187: training loss 0.857; learning rate 0.000454
Epoch 3 iteration 0007/0187: training loss 0.853; learning rate 0.000454
Epoch 3 iteration 0008/0187: training loss 0.837; learning rate 0.000454
Epoch 3 iteration 0009/0187: training loss 0.831; learning rate 0.000454
Epoch 3 iteration 0010/0187: training loss 0.831; learning rate 0.000454
Epoch 3 iteration 0011/0187: training loss 0.851; learning rate 0.000454
Epoch 3 iteration 0012/0187: training loss 0.837; learning rate 0.000454
Epoch 3 iteration 0013/0187: training loss 0.838; learning rate 0.000454
Epoch 3 iteration 0014/0187: training loss 0.824; learning rate 0.000453
Epoch 3 iteration 0015/0187: training loss 0.814; learning rate 0.000453
Epoch 3 iteration 0016/0187: training loss 0.805; learning rate 0.000453
Epoch 3 iteration 0017/0187: training loss 0.805; learning rate 0.000453
Epoch 3 iteration 0018/0187: training loss 0.810; learning rate 0.000453
Epoch 3 iteration 0019/0187: training loss 0.806; learning rate 0.000453
Epoch 3 iteration 0020/0187: training loss 0.813; learning rate 0.000453
Epoch 3 iteration 0021/0187: training loss 0.825; learning rate 0.000453
Epoch 3 iteration 0022/0187: training loss 0.830; learning rate 0.000453
Epoch 3 iteration 0023/0187: training loss 0.829; learning rate 0.000453
Epoch 3 iteration 0024/0187: training loss 0.828; learning rate 0.000453
Epoch 3 iteration 0025/0187: training loss 0.838; learning rate 0.000453
Epoch 3 iteration 0026/0187: training loss 0.839; learning rate 0.000452
Epoch 3 iteration 0027/0187: training loss 0.836; learning rate 0.000452
Epoch 3 iteration 0028/0187: training loss 0.841; learning rate 0.000452
Epoch 3 iteration 0029/0187: training loss 0.850; learning rate 0.000452
Epoch 3 iteration 0030/0187: training loss 0.848; learning rate 0.000452
Epoch 3 iteration 0031/0187: training loss 0.862; learning rate 0.000452
Epoch 3 iteration 0032/0187: training loss 0.857; learning rate 0.000452
Epoch 3 iteration 0033/0187: training loss 0.864; learning rate 0.000452
Epoch 3 iteration 0034/0187: training loss 0.862; learning rate 0.000452
Epoch 3 iteration 0035/0187: training loss 0.863; learning rate 0.000452
Epoch 3 iteration 0036/0187: training loss 0.868; learning rate 0.000452
Epoch 3 iteration 0037/0187: training loss 0.862; learning rate 0.000452
Epoch 3 iteration 0038/0187: training loss 0.857; learning rate 0.000452
Epoch 3 iteration 0039/0187: training loss 0.856; learning rate 0.000451
Epoch 3 iteration 0040/0187: training loss 0.853; learning rate 0.000451
Epoch 3 iteration 0041/0187: training loss 0.847; learning rate 0.000451
Epoch 3 iteration 0042/0187: training loss 0.847; learning rate 0.000451
Epoch 3 iteration 0043/0187: training loss 0.849; learning rate 0.000451
Epoch 3 iteration 0044/0187: training loss 0.850; learning rate 0.000451
Epoch 3 iteration 0045/0187: training loss 0.852; learning rate 0.000451
Epoch 3 iteration 0046/0187: training loss 0.852; learning rate 0.000451
Epoch 3 iteration 0047/0187: training loss 0.850; learning rate 0.000451
Epoch 3 iteration 0048/0187: training loss 0.846; learning rate 0.000451
Epoch 3 iteration 0049/0187: training loss 0.845; learning rate 0.000451
Epoch 3 iteration 0050/0187: training loss 0.844; learning rate 0.000451
Epoch 3 iteration 0051/0187: training loss 0.844; learning rate 0.000450
Epoch 3 iteration 0052/0187: training loss 0.839; learning rate 0.000450
Epoch 3 iteration 0053/0187: training loss 0.838; learning rate 0.000450
Epoch 3 iteration 0054/0187: training loss 0.839; learning rate 0.000450
Epoch 3 iteration 0055/0187: training loss 0.841; learning rate 0.000450
Epoch 3 iteration 0056/0187: training loss 0.840; learning rate 0.000450
Epoch 3 iteration 0057/0187: training loss 0.842; learning rate 0.000450
Epoch 3 iteration 0058/0187: training loss 0.843; learning rate 0.000450
Epoch 3 iteration 0059/0187: training loss 0.840; learning rate 0.000450
Epoch 3 iteration 0060/0187: training loss 0.842; learning rate 0.000450
Epoch 3 iteration 0061/0187: training loss 0.844; learning rate 0.000450
Epoch 3 iteration 0062/0187: training loss 0.846; learning rate 0.000450
Epoch 3 iteration 0063/0187: training loss 0.849; learning rate 0.000449
Epoch 3 iteration 0064/0187: training loss 0.849; learning rate 0.000449
Epoch 3 iteration 0065/0187: training loss 0.852; learning rate 0.000449
Epoch 3 iteration 0066/0187: training loss 0.849; learning rate 0.000449
Epoch 3 iteration 0067/0187: training loss 0.852; learning rate 0.000449
Epoch 3 iteration 0068/0187: training loss 0.857; learning rate 0.000449
Epoch 3 iteration 0069/0187: training loss 0.859; learning rate 0.000449
Epoch 3 iteration 0070/0187: training loss 0.860; learning rate 0.000449
Epoch 3 iteration 0071/0187: training loss 0.860; learning rate 0.000449
Epoch 3 iteration 0072/0187: training loss 0.865; learning rate 0.000449
Epoch 3 iteration 0073/0187: training loss 0.866; learning rate 0.000449
Epoch 3 iteration 0074/0187: training loss 0.866; learning rate 0.000449
Epoch 3 iteration 0075/0187: training loss 0.866; learning rate 0.000449
Epoch 3 iteration 0076/0187: training loss 0.863; learning rate 0.000448
Epoch 3 iteration 0077/0187: training loss 0.859; learning rate 0.000448
Epoch 3 iteration 0078/0187: training loss 0.858; learning rate 0.000448
Epoch 3 iteration 0079/0187: training loss 0.864; learning rate 0.000448
Epoch 3 iteration 0080/0187: training loss 0.863; learning rate 0.000448
Epoch 3 iteration 0081/0187: training loss 0.860; learning rate 0.000448
Epoch 3 iteration 0082/0187: training loss 0.860; learning rate 0.000448
Epoch 3 iteration 0083/0187: training loss 0.860; learning rate 0.000448
Epoch 3 iteration 0084/0187: training loss 0.861; learning rate 0.000448
Epoch 3 iteration 0085/0187: training loss 0.858; learning rate 0.000448
Epoch 3 iteration 0086/0187: training loss 0.858; learning rate 0.000448
Epoch 3 iteration 0087/0187: training loss 0.862; learning rate 0.000448
Epoch 3 iteration 0088/0187: training loss 0.864; learning rate 0.000447
Epoch 3 iteration 0089/0187: training loss 0.864; learning rate 0.000447
Epoch 3 iteration 0090/0187: training loss 0.863; learning rate 0.000447
Epoch 3 iteration 0091/0187: training loss 0.860; learning rate 0.000447
Epoch 3 iteration 0092/0187: training loss 0.862; learning rate 0.000447
Epoch 3 iteration 0093/0187: training loss 0.861; learning rate 0.000447
Epoch 3 iteration 0094/0187: training loss 0.859; learning rate 0.000447
Epoch 3 iteration 0095/0187: training loss 0.861; learning rate 0.000447
Epoch 3 iteration 0096/0187: training loss 0.861; learning rate 0.000447
Epoch 3 iteration 0097/0187: training loss 0.860; learning rate 0.000447
Epoch 3 iteration 0098/0187: training loss 0.860; learning rate 0.000447
Epoch 3 iteration 0099/0187: training loss 0.862; learning rate 0.000447
Epoch 3 iteration 0100/0187: training loss 0.859; learning rate 0.000446
Epoch 3 iteration 0101/0187: training loss 0.858; learning rate 0.000446
Epoch 3 iteration 0102/0187: training loss 0.857; learning rate 0.000446
Epoch 3 iteration 0103/0187: training loss 0.856; learning rate 0.000446
Epoch 3 iteration 0104/0187: training loss 0.857; learning rate 0.000446
Epoch 3 iteration 0105/0187: training loss 0.858; learning rate 0.000446
Epoch 3 iteration 0106/0187: training loss 0.858; learning rate 0.000446
Epoch 3 iteration 0107/0187: training loss 0.859; learning rate 0.000446
Epoch 3 iteration 0108/0187: training loss 0.858; learning rate 0.000446
Epoch 3 iteration 0109/0187: training loss 0.857; learning rate 0.000446
Epoch 3 iteration 0110/0187: training loss 0.854; learning rate 0.000446
Epoch 3 iteration 0111/0187: training loss 0.854; learning rate 0.000446
Epoch 3 iteration 0112/0187: training loss 0.855; learning rate 0.000446
Epoch 3 iteration 0113/0187: training loss 0.853; learning rate 0.000445
Epoch 3 iteration 0114/0187: training loss 0.854; learning rate 0.000445
Epoch 3 iteration 0115/0187: training loss 0.853; learning rate 0.000445
Epoch 3 iteration 0116/0187: training loss 0.853; learning rate 0.000445
Epoch 3 iteration 0117/0187: training loss 0.852; learning rate 0.000445
Epoch 3 iteration 0118/0187: training loss 0.850; learning rate 0.000445
Epoch 3 iteration 0119/0187: training loss 0.850; learning rate 0.000445
Epoch 3 iteration 0120/0187: training loss 0.850; learning rate 0.000445
Epoch 3 iteration 0121/0187: training loss 0.851; learning rate 0.000445
Epoch 3 iteration 0122/0187: training loss 0.851; learning rate 0.000445
Epoch 3 iteration 0123/0187: training loss 0.851; learning rate 0.000445
Epoch 3 iteration 0124/0187: training loss 0.852; learning rate 0.000445
Epoch 3 iteration 0125/0187: training loss 0.852; learning rate 0.000444
Epoch 3 iteration 0126/0187: training loss 0.852; learning rate 0.000444
Epoch 3 iteration 0127/0187: training loss 0.854; learning rate 0.000444
Epoch 3 iteration 0128/0187: training loss 0.853; learning rate 0.000444
Epoch 3 iteration 0129/0187: training loss 0.855; learning rate 0.000444
Epoch 3 iteration 0130/0187: training loss 0.854; learning rate 0.000444
Epoch 3 iteration 0131/0187: training loss 0.853; learning rate 0.000444
Epoch 3 iteration 0132/0187: training loss 0.852; learning rate 0.000444
Epoch 3 iteration 0133/0187: training loss 0.853; learning rate 0.000444
Epoch 3 iteration 0134/0187: training loss 0.852; learning rate 0.000444
Epoch 3 iteration 0135/0187: training loss 0.852; learning rate 0.000444
Epoch 3 iteration 0136/0187: training loss 0.853; learning rate 0.000444
Epoch 3 iteration 0137/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0138/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0139/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0140/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0141/0187: training loss 0.854; learning rate 0.000443
Epoch 3 iteration 0142/0187: training loss 0.855; learning rate 0.000443
Epoch 3 iteration 0143/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0144/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0145/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0146/0187: training loss 0.853; learning rate 0.000443
Epoch 3 iteration 0147/0187: training loss 0.852; learning rate 0.000443
Epoch 3 iteration 0148/0187: training loss 0.852; learning rate 0.000443
Epoch 3 iteration 0149/0187: training loss 0.851; learning rate 0.000442
Epoch 3 iteration 0150/0187: training loss 0.850; learning rate 0.000442
Epoch 3 iteration 0151/0187: training loss 0.849; learning rate 0.000442
Epoch 3 iteration 0152/0187: training loss 0.847; learning rate 0.000442
Epoch 3 iteration 0153/0187: training loss 0.845; learning rate 0.000442
Epoch 3 iteration 0154/0187: training loss 0.845; learning rate 0.000442
Epoch 3 iteration 0155/0187: training loss 0.847; learning rate 0.000442
Epoch 3 iteration 0156/0187: training loss 0.846; learning rate 0.000442
Epoch 3 iteration 0157/0187: training loss 0.847; learning rate 0.000442
Epoch 3 iteration 0158/0187: training loss 0.848; learning rate 0.000442
Epoch 3 iteration 0159/0187: training loss 0.847; learning rate 0.000442
Epoch 3 iteration 0160/0187: training loss 0.846; learning rate 0.000442
Epoch 3 iteration 0161/0187: training loss 0.846; learning rate 0.000442
Epoch 3 iteration 0162/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0163/0187: training loss 0.845; learning rate 0.000441
Epoch 3 iteration 0164/0187: training loss 0.845; learning rate 0.000441
Epoch 3 iteration 0165/0187: training loss 0.844; learning rate 0.000441
Epoch 3 iteration 0166/0187: training loss 0.845; learning rate 0.000441
Epoch 3 iteration 0167/0187: training loss 0.845; learning rate 0.000441
Epoch 3 iteration 0168/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0169/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0170/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0171/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0172/0187: training loss 0.847; learning rate 0.000441
Epoch 3 iteration 0173/0187: training loss 0.846; learning rate 0.000441
Epoch 3 iteration 0174/0187: training loss 0.845; learning rate 0.000440
Epoch 3 iteration 0175/0187: training loss 0.844; learning rate 0.000440
Epoch 3 iteration 0176/0187: training loss 0.845; learning rate 0.000440
Epoch 3 iteration 0177/0187: training loss 0.844; learning rate 0.000440
Epoch 3 iteration 0178/0187: training loss 0.844; learning rate 0.000440
Epoch 3 iteration 0179/0187: training loss 0.842; learning rate 0.000440
Epoch 3 iteration 0180/0187: training loss 0.843; learning rate 0.000440
Epoch 3 iteration 0181/0187: training loss 0.843; learning rate 0.000440
Epoch 3 iteration 0182/0187: training loss 0.843; learning rate 0.000440
Epoch 3 iteration 0183/0187: training loss 0.843; learning rate 0.000440
Epoch 3 iteration 0184/0187: training loss 0.844; learning rate 0.000440
Epoch 3 iteration 0185/0187: training loss 0.845; learning rate 0.000440
Epoch 3 iteration 0186/0187: training loss 0.844; learning rate 0.000439
Epoch 3 iteration 0187/0187: training loss 0.845; learning rate 0.000439
Epoch 3 validation pixAcc: 0.332, mIoU: 0.158
Epoch 4 iteration 0001/0187: training loss 0.921; learning rate 0.000439
Epoch 4 iteration 0002/0187: training loss 0.908; learning rate 0.000439
Epoch 4 iteration 0003/0187: training loss 0.872; learning rate 0.000439
Epoch 4 iteration 0004/0187: training loss 0.879; learning rate 0.000439
Epoch 4 iteration 0005/0187: training loss 0.921; learning rate 0.000439
Epoch 4 iteration 0006/0187: training loss 0.875; learning rate 0.000439
Epoch 4 iteration 0007/0187: training loss 0.883; learning rate 0.000439
Epoch 4 iteration 0008/0187: training loss 0.883; learning rate 0.000439
Epoch 4 iteration 0009/0187: training loss 0.862; learning rate 0.000439
Epoch 4 iteration 0010/0187: training loss 0.860; learning rate 0.000439
Epoch 4 iteration 0011/0187: training loss 0.859; learning rate 0.000438
Epoch 4 iteration 0012/0187: training loss 0.832; learning rate 0.000438
Epoch 4 iteration 0013/0187: training loss 0.831; learning rate 0.000438
Epoch 4 iteration 0014/0187: training loss 0.818; learning rate 0.000438
Epoch 4 iteration 0015/0187: training loss 0.814; learning rate 0.000438
Epoch 4 iteration 0016/0187: training loss 0.814; learning rate 0.000438
Epoch 4 iteration 0017/0187: training loss 0.821; learning rate 0.000438
Epoch 4 iteration 0018/0187: training loss 0.813; learning rate 0.000438
Epoch 4 iteration 0019/0187: training loss 0.815; learning rate 0.000438
Epoch 4 iteration 0020/0187: training loss 0.811; learning rate 0.000438
Epoch 4 iteration 0021/0187: training loss 0.811; learning rate 0.000438
Epoch 4 iteration 0022/0187: training loss 0.813; learning rate 0.000438
Epoch 4 iteration 0023/0187: training loss 0.825; learning rate 0.000437
Epoch 4 iteration 0024/0187: training loss 0.825; learning rate 0.000437
Epoch 4 iteration 0025/0187: training loss 0.822; learning rate 0.000437
Epoch 4 iteration 0026/0187: training loss 0.821; learning rate 0.000437
Epoch 4 iteration 0027/0187: training loss 0.830; learning rate 0.000437
Epoch 4 iteration 0028/0187: training loss 0.827; learning rate 0.000437
Epoch 4 iteration 0029/0187: training loss 0.831; learning rate 0.000437
Epoch 4 iteration 0030/0187: training loss 0.835; learning rate 0.000437
Epoch 4 iteration 0031/0187: training loss 0.834; learning rate 0.000437
Epoch 4 iteration 0032/0187: training loss 0.829; learning rate 0.000437
Epoch 4 iteration 0033/0187: training loss 0.828; learning rate 0.000437
Epoch 4 iteration 0034/0187: training loss 0.831; learning rate 0.000437
Epoch 4 iteration 0035/0187: training loss 0.825; learning rate 0.000436
Epoch 4 iteration 0036/0187: training loss 0.821; learning rate 0.000436
Epoch 4 iteration 0037/0187: training loss 0.827; learning rate 0.000436
Epoch 4 iteration 0038/0187: training loss 0.835; learning rate 0.000436
Epoch 4 iteration 0039/0187: training loss 0.831; learning rate 0.000436
Epoch 4 iteration 0040/0187: training loss 0.831; learning rate 0.000436
Epoch 4 iteration 0041/0187: training loss 0.834; learning rate 0.000436
Epoch 4 iteration 0042/0187: training loss 0.831; learning rate 0.000436
Epoch 4 iteration 0043/0187: training loss 0.834; learning rate 0.000436
Epoch 4 iteration 0044/0187: training loss 0.831; learning rate 0.000436
Epoch 4 iteration 0045/0187: training loss 0.830; learning rate 0.000436
Epoch 4 iteration 0046/0187: training loss 0.833; learning rate 0.000436
Epoch 4 iteration 0047/0187: training loss 0.837; learning rate 0.000435
Epoch 4 iteration 0048/0187: training loss 0.834; learning rate 0.000435
Epoch 4 iteration 0049/0187: training loss 0.832; learning rate 0.000435
Epoch 4 iteration 0050/0187: training loss 0.830; learning rate 0.000435
Epoch 4 iteration 0051/0187: training loss 0.829; learning rate 0.000435
Epoch 4 iteration 0052/0187: training loss 0.833; learning rate 0.000435
Epoch 4 iteration 0053/0187: training loss 0.830; learning rate 0.000435
Epoch 4 iteration 0054/0187: training loss 0.829; learning rate 0.000435
Epoch 4 iteration 0055/0187: training loss 0.832; learning rate 0.000435
Epoch 4 iteration 0056/0187: training loss 0.830; learning rate 0.000435
Epoch 4 iteration 0057/0187: training loss 0.830; learning rate 0.000435
Epoch 4 iteration 0058/0187: training loss 0.830; learning rate 0.000435
Epoch 4 iteration 0059/0187: training loss 0.829; learning rate 0.000435
Epoch 4 iteration 0060/0187: training loss 0.830; learning rate 0.000434
Epoch 4 iteration 0061/0187: training loss 0.828; learning rate 0.000434
Epoch 4 iteration 0062/0187: training loss 0.830; learning rate 0.000434
Epoch 4 iteration 0063/0187: training loss 0.831; learning rate 0.000434
Epoch 4 iteration 0064/0187: training loss 0.831; learning rate 0.000434
Epoch 4 iteration 0065/0187: training loss 0.830; learning rate 0.000434
Epoch 4 iteration 0066/0187: training loss 0.827; learning rate 0.000434
Epoch 4 iteration 0067/0187: training loss 0.826; learning rate 0.000434
Epoch 4 iteration 0068/0187: training loss 0.827; learning rate 0.000434
Epoch 4 iteration 0069/0187: training loss 0.830; learning rate 0.000434
Epoch 4 iteration 0070/0187: training loss 0.830; learning rate 0.000434
Epoch 4 iteration 0071/0187: training loss 0.826; learning rate 0.000434
Epoch 4 iteration 0072/0187: training loss 0.827; learning rate 0.000433
Epoch 4 iteration 0073/0187: training loss 0.823; learning rate 0.000433
Epoch 4 iteration 0074/0187: training loss 0.823; learning rate 0.000433
Epoch 4 iteration 0075/0187: training loss 0.822; learning rate 0.000433
Epoch 4 iteration 0076/0187: training loss 0.821; learning rate 0.000433
Epoch 4 iteration 0077/0187: training loss 0.818; learning rate 0.000433
Epoch 4 iteration 0078/0187: training loss 0.817; learning rate 0.000433
Epoch 4 iteration 0079/0187: training loss 0.816; learning rate 0.000433
Epoch 4 iteration 0080/0187: training loss 0.814; learning rate 0.000433
Epoch 4 iteration 0081/0187: training loss 0.814; learning rate 0.000433
Epoch 4 iteration 0082/0187: training loss 0.813; learning rate 0.000433
Epoch 4 iteration 0083/0187: training loss 0.811; learning rate 0.000433
Epoch 4 iteration 0084/0187: training loss 0.810; learning rate 0.000432
Epoch 4 iteration 0085/0187: training loss 0.810; learning rate 0.000432
Epoch 4 iteration 0086/0187: training loss 0.809; learning rate 0.000432
Epoch 4 iteration 0087/0187: training loss 0.808; learning rate 0.000432
Epoch 4 iteration 0088/0187: training loss 0.809; learning rate 0.000432
Epoch 4 iteration 0089/0187: training loss 0.809; learning rate 0.000432
Epoch 4 iteration 0090/0187: training loss 0.808; learning rate 0.000432
Epoch 4 iteration 0091/0188: training loss 0.807; learning rate 0.000432
Epoch 4 iteration 0092/0188: training loss 0.809; learning rate 0.000432
Epoch 4 iteration 0093/0188: training loss 0.811; learning rate 0.000432
Epoch 4 iteration 0094/0188: training loss 0.809; learning rate 0.000432
Epoch 4 iteration 0095/0188: training loss 0.813; learning rate 0.000432
Epoch 4 iteration 0096/0188: training loss 0.811; learning rate 0.000432
Epoch 4 iteration 0097/0188: training loss 0.810; learning rate 0.000431
Epoch 4 iteration 0098/0188: training loss 0.808; learning rate 0.000431
Epoch 4 iteration 0099/0188: training loss 0.810; learning rate 0.000431
Epoch 4 iteration 0100/0188: training loss 0.810; learning rate 0.000431
Epoch 4 iteration 0101/0188: training loss 0.812; learning rate 0.000431
Epoch 4 iteration 0102/0188: training loss 0.812; learning rate 0.000431
Epoch 4 iteration 0103/0188: training loss 0.814; learning rate 0.000431
Epoch 4 iteration 0104/0188: training loss 0.815; learning rate 0.000431
Epoch 4 iteration 0105/0188: training loss 0.814; learning rate 0.000431
Epoch 4 iteration 0106/0188: training loss 0.813; learning rate 0.000431
Epoch 4 iteration 0107/0188: training loss 0.812; learning rate 0.000431
Epoch 4 iteration 0108/0188: training loss 0.811; learning rate 0.000431
Epoch 4 iteration 0109/0188: training loss 0.812; learning rate 0.000430
Epoch 4 iteration 0110/0188: training loss 0.810; learning rate 0.000430
Epoch 4 iteration 0111/0188: training loss 0.810; learning rate 0.000430
Epoch 4 iteration 0112/0188: training loss 0.809; learning rate 0.000430
Epoch 4 iteration 0113/0188: training loss 0.807; learning rate 0.000430
Epoch 4 iteration 0114/0188: training loss 0.810; learning rate 0.000430
Epoch 4 iteration 0115/0188: training loss 0.813; learning rate 0.000430
Epoch 4 iteration 0116/0188: training loss 0.813; learning rate 0.000430
Epoch 4 iteration 0117/0188: training loss 0.813; learning rate 0.000430
Epoch 4 iteration 0118/0188: training loss 0.812; learning rate 0.000430
Epoch 4 iteration 0119/0188: training loss 0.814; learning rate 0.000430
Epoch 4 iteration 0120/0188: training loss 0.813; learning rate 0.000430
Epoch 4 iteration 0121/0188: training loss 0.813; learning rate 0.000429
Epoch 4 iteration 0122/0188: training loss 0.814; learning rate 0.000429
Epoch 4 iteration 0123/0188: training loss 0.815; learning rate 0.000429
Epoch 4 iteration 0124/0188: training loss 0.815; learning rate 0.000429
Epoch 4 iteration 0125/0188: training loss 0.817; learning rate 0.000429
Epoch 4 iteration 0126/0188: training loss 0.818; learning rate 0.000429
Epoch 4 iteration 0127/0188: training loss 0.818; learning rate 0.000429
Epoch 4 iteration 0128/0188: training loss 0.817; learning rate 0.000429
Epoch 4 iteration 0129/0188: training loss 0.820; learning rate 0.000429
Epoch 4 iteration 0130/0188: training loss 0.820; learning rate 0.000429
Epoch 4 iteration 0131/0188: training loss 0.820; learning rate 0.000429
Epoch 4 iteration 0132/0188: training loss 0.820; learning rate 0.000429
Epoch 4 iteration 0133/0188: training loss 0.820; learning rate 0.000428
Epoch 4 iteration 0134/0188: training loss 0.820; learning rate 0.000428
Epoch 4 iteration 0135/0188: training loss 0.820; learning rate 0.000428
Epoch 4 iteration 0136/0188: training loss 0.823; learning rate 0.000428
Epoch 4 iteration 0137/0188: training loss 0.822; learning rate 0.000428
Epoch 4 iteration 0138/0188: training loss 0.822; learning rate 0.000428
Epoch 4 iteration 0139/0188: training loss 0.823; learning rate 0.000428
Epoch 4 iteration 0140/0188: training loss 0.826; learning rate 0.000428
Epoch 4 iteration 0141/0188: training loss 0.826; learning rate 0.000428
Epoch 4 iteration 0142/0188: training loss 0.823; learning rate 0.000428
Epoch 4 iteration 0143/0188: training loss 0.824; learning rate 0.000428
Epoch 4 iteration 0144/0188: training loss 0.824; learning rate 0.000428
Epoch 4 iteration 0145/0188: training loss 0.825; learning rate 0.000428
Epoch 4 iteration 0146/0188: training loss 0.824; learning rate 0.000427
Epoch 4 iteration 0147/0188: training loss 0.824; learning rate 0.000427
Epoch 4 iteration 0148/0188: training loss 0.823; learning rate 0.000427
Epoch 4 iteration 0149/0188: training loss 0.824; learning rate 0.000427
Epoch 4 iteration 0150/0188: training loss 0.822; learning rate 0.000427
Epoch 4 iteration 0151/0188: training loss 0.821; learning rate 0.000427
Epoch 4 iteration 0152/0188: training loss 0.821; learning rate 0.000427
Epoch 4 iteration 0153/0188: training loss 0.820; learning rate 0.000427
Epoch 4 iteration 0154/0188: training loss 0.819; learning rate 0.000427
Epoch 4 iteration 0155/0188: training loss 0.820; learning rate 0.000427
Epoch 4 iteration 0156/0188: training loss 0.821; learning rate 0.000427
Epoch 4 iteration 0157/0188: training loss 0.820; learning rate 0.000427
Epoch 4 iteration 0158/0188: training loss 0.822; learning rate 0.000426
Epoch 4 iteration 0159/0188: training loss 0.820; learning rate 0.000426
Epoch 4 iteration 0160/0188: training loss 0.820; learning rate 0.000426
Epoch 4 iteration 0161/0188: training loss 0.820; learning rate 0.000426
Epoch 4 iteration 0162/0188: training loss 0.820; learning rate 0.000426
Epoch 4 iteration 0163/0188: training loss 0.819; learning rate 0.000426
Epoch 4 iteration 0164/0188: training loss 0.820; learning rate 0.000426
Epoch 4 iteration 0165/0188: training loss 0.818; learning rate 0.000426
Epoch 4 iteration 0166/0188: training loss 0.819; learning rate 0.000426
Epoch 4 iteration 0167/0188: training loss 0.817; learning rate 0.000426
Epoch 4 iteration 0168/0188: training loss 0.818; learning rate 0.000426
Epoch 4 iteration 0169/0188: training loss 0.819; learning rate 0.000426
Epoch 4 iteration 0170/0188: training loss 0.818; learning rate 0.000425
Epoch 4 iteration 0171/0188: training loss 0.819; learning rate 0.000425
Epoch 4 iteration 0172/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0173/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0174/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0175/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0176/0188: training loss 0.821; learning rate 0.000425
Epoch 4 iteration 0177/0188: training loss 0.821; learning rate 0.000425
Epoch 4 iteration 0178/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0179/0188: training loss 0.818; learning rate 0.000425
Epoch 4 iteration 0180/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0181/0188: training loss 0.820; learning rate 0.000425
Epoch 4 iteration 0182/0188: training loss 0.820; learning rate 0.000424
Epoch 4 iteration 0183/0188: training loss 0.821; learning rate 0.000424
Epoch 4 iteration 0184/0188: training loss 0.821; learning rate 0.000424
Epoch 4 iteration 0185/0188: training loss 0.824; learning rate 0.000424
Epoch 4 iteration 0186/0188: training loss 0.824; learning rate 0.000424
Epoch 4 validation pixAcc: 0.336, mIoU: 0.164
Epoch 5 iteration 0001/0187: training loss 0.692; learning rate 0.000424
Epoch 5 iteration 0002/0187: training loss 0.702; learning rate 0.000424
Epoch 5 iteration 0003/0187: training loss 0.661; learning rate 0.000424
Epoch 5 iteration 0004/0187: training loss 0.655; learning rate 0.000424
Epoch 5 iteration 0005/0187: training loss 0.708; learning rate 0.000424
Epoch 5 iteration 0006/0187: training loss 0.720; learning rate 0.000424
Epoch 5 iteration 0007/0187: training loss 0.724; learning rate 0.000424
Epoch 5 iteration 0008/0187: training loss 0.726; learning rate 0.000423
Epoch 5 iteration 0009/0187: training loss 0.718; learning rate 0.000423
Epoch 5 iteration 0010/0187: training loss 0.721; learning rate 0.000423
Epoch 5 iteration 0011/0187: training loss 0.730; learning rate 0.000423
Epoch 5 iteration 0012/0187: training loss 0.729; learning rate 0.000423
Epoch 5 iteration 0013/0187: training loss 0.715; learning rate 0.000423
Epoch 5 iteration 0014/0187: training loss 0.715; learning rate 0.000423
Epoch 5 iteration 0015/0187: training loss 0.723; learning rate 0.000423
Epoch 5 iteration 0016/0187: training loss 0.718; learning rate 0.000423
Epoch 5 iteration 0017/0187: training loss 0.704; learning rate 0.000423
Epoch 5 iteration 0018/0187: training loss 0.721; learning rate 0.000423
Epoch 5 iteration 0019/0187: training loss 0.717; learning rate 0.000423
Epoch 5 iteration 0020/0187: training loss 0.722; learning rate 0.000422
Epoch 5 iteration 0021/0187: training loss 0.716; learning rate 0.000422
Epoch 5 iteration 0022/0187: training loss 0.709; learning rate 0.000422
Epoch 5 iteration 0023/0187: training loss 0.718; learning rate 0.000422
Epoch 5 iteration 0024/0187: training loss 0.719; learning rate 0.000422
Epoch 5 iteration 0025/0187: training loss 0.720; learning rate 0.000422
Epoch 5 iteration 0026/0187: training loss 0.723; learning rate 0.000422
Epoch 5 iteration 0027/0187: training loss 0.729; learning rate 0.000422
Epoch 5 iteration 0028/0187: training loss 0.736; learning rate 0.000422
Epoch 5 iteration 0029/0187: training loss 0.736; learning rate 0.000422
Epoch 5 iteration 0030/0187: training loss 0.739; learning rate 0.000422
Epoch 5 iteration 0031/0187: training loss 0.740; learning rate 0.000422
Epoch 5 iteration 0032/0187: training loss 0.739; learning rate 0.000421
Epoch 5 iteration 0033/0187: training loss 0.736; learning rate 0.000421
Epoch 5 iteration 0034/0187: training loss 0.734; learning rate 0.000421
Epoch 5 iteration 0035/0187: training loss 0.735; learning rate 0.000421
Epoch 5 iteration 0036/0187: training loss 0.737; learning rate 0.000421
Epoch 5 iteration 0037/0187: training loss 0.740; learning rate 0.000421
Epoch 5 iteration 0038/0187: training loss 0.737; learning rate 0.000421
Epoch 5 iteration 0039/0187: training loss 0.740; learning rate 0.000421
Epoch 5 iteration 0040/0187: training loss 0.741; learning rate 0.000421
Epoch 5 iteration 0041/0187: training loss 0.740; learning rate 0.000421
Epoch 5 iteration 0042/0187: training loss 0.742; learning rate 0.000421
Epoch 5 iteration 0043/0187: training loss 0.741; learning rate 0.000421
Epoch 5 iteration 0044/0187: training loss 0.742; learning rate 0.000420
Epoch 5 iteration 0045/0187: training loss 0.743; learning rate 0.000420
Epoch 5 iteration 0046/0187: training loss 0.745; learning rate 0.000420
Epoch 5 iteration 0047/0187: training loss 0.749; learning rate 0.000420
Epoch 5 iteration 0048/0187: training loss 0.750; learning rate 0.000420
Epoch 5 iteration 0049/0187: training loss 0.749; learning rate 0.000420
Epoch 5 iteration 0050/0187: training loss 0.753; learning rate 0.000420
Epoch 5 iteration 0051/0187: training loss 0.754; learning rate 0.000420
Epoch 5 iteration 0052/0187: training loss 0.754; learning rate 0.000420
Epoch 5 iteration 0053/0187: training loss 0.751; learning rate 0.000420
Epoch 5 iteration 0054/0187: training loss 0.753; learning rate 0.000420
Epoch 5 iteration 0055/0187: training loss 0.753; learning rate 0.000420
Epoch 5 iteration 0056/0187: training loss 0.751; learning rate 0.000419
Epoch 5 iteration 0057/0187: training loss 0.748; learning rate 0.000419
Epoch 5 iteration 0058/0187: training loss 0.748; learning rate 0.000419
Epoch 5 iteration 0059/0187: training loss 0.748; learning rate 0.000419
Epoch 5 iteration 0060/0187: training loss 0.748; learning rate 0.000419
Epoch 5 iteration 0061/0187: training loss 0.747; learning rate 0.000419
Epoch 5 iteration 0062/0187: training loss 0.750; learning rate 0.000419
Epoch 5 iteration 0063/0187: training loss 0.748; learning rate 0.000419
Epoch 5 iteration 0064/0187: training loss 0.749; learning rate 0.000419
Epoch 5 iteration 0065/0187: training loss 0.745; learning rate 0.000419
Epoch 5 iteration 0066/0187: training loss 0.741; learning rate 0.000419
Epoch 5 iteration 0067/0187: training loss 0.743; learning rate 0.000419
Epoch 5 iteration 0068/0187: training loss 0.743; learning rate 0.000419
Epoch 5 iteration 0069/0187: training loss 0.745; learning rate 0.000418
Epoch 5 iteration 0070/0187: training loss 0.747; learning rate 0.000418
Epoch 5 iteration 0071/0187: training loss 0.749; learning rate 0.000418
Epoch 5 iteration 0072/0187: training loss 0.749; learning rate 0.000418
Epoch 5 iteration 0073/0187: training loss 0.749; learning rate 0.000418
Epoch 5 iteration 0074/0187: training loss 0.747; learning rate 0.000418
Epoch 5 iteration 0075/0187: training loss 0.748; learning rate 0.000418
Epoch 5 iteration 0076/0187: training loss 0.745; learning rate 0.000418
Epoch 5 iteration 0077/0187: training loss 0.745; learning rate 0.000418
Epoch 5 iteration 0078/0187: training loss 0.746; learning rate 0.000418
Epoch 5 iteration 0079/0187: training loss 0.743; learning rate 0.000418
Epoch 5 iteration 0080/0187: training loss 0.746; learning rate 0.000418
Epoch 5 iteration 0081/0187: training loss 0.746; learning rate 0.000417
Epoch 5 iteration 0082/0187: training loss 0.747; learning rate 0.000417
Epoch 5 iteration 0083/0187: training loss 0.748; learning rate 0.000417
Epoch 5 iteration 0084/0187: training loss 0.747; learning rate 0.000417
Epoch 5 iteration 0085/0187: training loss 0.748; learning rate 0.000417
Epoch 5 iteration 0086/0187: training loss 0.751; learning rate 0.000417
Epoch 5 iteration 0087/0187: training loss 0.749; learning rate 0.000417
Epoch 5 iteration 0088/0187: training loss 0.753; learning rate 0.000417
Epoch 5 iteration 0089/0187: training loss 0.754; learning rate 0.000417
Epoch 5 iteration 0090/0187: training loss 0.752; learning rate 0.000417
Epoch 5 iteration 0091/0187: training loss 0.751; learning rate 0.000417
Epoch 5 iteration 0092/0187: training loss 0.753; learning rate 0.000417
Epoch 5 iteration 0093/0187: training loss 0.757; learning rate 0.000416
Epoch 5 iteration 0094/0187: training loss 0.755; learning rate 0.000416
Epoch 5 iteration 0095/0187: training loss 0.757; learning rate 0.000416
Epoch 5 iteration 0096/0187: training loss 0.757; learning rate 0.000416
Epoch 5 iteration 0097/0187: training loss 0.755; learning rate 0.000416
Epoch 5 iteration 0098/0187: training loss 0.756; learning rate 0.000416
Epoch 5 iteration 0099/0187: training loss 0.755; learning rate 0.000416
Epoch 5 iteration 0100/0187: training loss 0.757; learning rate 0.000416
Epoch 5 iteration 0101/0187: training loss 0.757; learning rate 0.000416
Epoch 5 iteration 0102/0187: training loss 0.755; learning rate 0.000416
Epoch 5 iteration 0103/0187: training loss 0.755; learning rate 0.000416
Epoch 5 iteration 0104/0187: training loss 0.756; learning rate 0.000416
Epoch 5 iteration 0105/0187: training loss 0.758; learning rate 0.000415
Epoch 5 iteration 0106/0187: training loss 0.758; learning rate 0.000415
Epoch 5 iteration 0107/0187: training loss 0.757; learning rate 0.000415
Epoch 5 iteration 0108/0187: training loss 0.759; learning rate 0.000415
Epoch 5 iteration 0109/0187: training loss 0.759; learning rate 0.000415
Epoch 5 iteration 0110/0187: training loss 0.760; learning rate 0.000415
Epoch 5 iteration 0111/0187: training loss 0.759; learning rate 0.000415
Epoch 5 iteration 0112/0187: training loss 0.760; learning rate 0.000415
Epoch 5 iteration 0113/0187: training loss 0.760; learning rate 0.000415
Epoch 5 iteration 0114/0187: training loss 0.759; learning rate 0.000415
Epoch 5 iteration 0115/0187: training loss 0.759; learning rate 0.000415
Epoch 5 iteration 0116/0187: training loss 0.761; learning rate 0.000415
Epoch 5 iteration 0117/0187: training loss 0.761; learning rate 0.000415
Epoch 5 iteration 0118/0187: training loss 0.760; learning rate 0.000414
Epoch 5 iteration 0119/0187: training loss 0.759; learning rate 0.000414
Epoch 5 iteration 0120/0187: training loss 0.758; learning rate 0.000414
Epoch 5 iteration 0121/0187: training loss 0.757; learning rate 0.000414
Epoch 5 iteration 0122/0187: training loss 0.758; learning rate 0.000414
Epoch 5 iteration 0123/0187: training loss 0.759; learning rate 0.000414
Epoch 5 iteration 0124/0187: training loss 0.758; learning rate 0.000414
Epoch 5 iteration 0125/0187: training loss 0.758; learning rate 0.000414
Epoch 5 iteration 0126/0187: training loss 0.762; learning rate 0.000414
Epoch 5 iteration 0127/0187: training loss 0.763; learning rate 0.000414
Epoch 5 iteration 0128/0187: training loss 0.764; learning rate 0.000414
Epoch 5 iteration 0129/0187: training loss 0.763; learning rate 0.000414
Epoch 5 iteration 0130/0187: training loss 0.764; learning rate 0.000413
Epoch 5 iteration 0131/0187: training loss 0.764; learning rate 0.000413
Epoch 5 iteration 0132/0187: training loss 0.762; learning rate 0.000413
Epoch 5 iteration 0133/0187: training loss 0.762; learning rate 0.000413
Epoch 5 iteration 0134/0187: training loss 0.762; learning rate 0.000413
Epoch 5 iteration 0135/0187: training loss 0.760; learning rate 0.000413
Epoch 5 iteration 0136/0187: training loss 0.761; learning rate 0.000413
Epoch 5 iteration 0137/0187: training loss 0.759; learning rate 0.000413
Epoch 5 iteration 0138/0187: training loss 0.761; learning rate 0.000413
Epoch 5 iteration 0139/0187: training loss 0.760; learning rate 0.000413
Epoch 5 iteration 0140/0187: training loss 0.760; learning rate 0.000413
Epoch 5 iteration 0141/0187: training loss 0.759; learning rate 0.000413
Epoch 5 iteration 0142/0187: training loss 0.760; learning rate 0.000412
Epoch 5 iteration 0143/0187: training loss 0.759; learning rate 0.000412
Epoch 5 iteration 0144/0187: training loss 0.758; learning rate 0.000412
Epoch 5 iteration 0145/0187: training loss 0.758; learning rate 0.000412
Epoch 5 iteration 0146/0187: training loss 0.759; learning rate 0.000412
Epoch 5 iteration 0147/0187: training loss 0.759; learning rate 0.000412
Epoch 5 iteration 0148/0187: training loss 0.760; learning rate 0.000412
Epoch 5 iteration 0149/0187: training loss 0.761; learning rate 0.000412
Epoch 5 iteration 0150/0187: training loss 0.761; learning rate 0.000412
Epoch 5 iteration 0151/0187: training loss 0.760; learning rate 0.000412
Epoch 5 iteration 0152/0187: training loss 0.759; learning rate 0.000412
Epoch 5 iteration 0153/0187: training loss 0.758; learning rate 0.000412
Epoch 5 iteration 0154/0187: training loss 0.758; learning rate 0.000411
Epoch 5 iteration 0155/0187: training loss 0.757; learning rate 0.000411
Epoch 5 iteration 0156/0187: training loss 0.759; learning rate 0.000411
Epoch 5 iteration 0157/0187: training loss 0.759; learning rate 0.000411
Epoch 5 iteration 0158/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0159/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0160/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0161/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0162/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0163/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0164/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0165/0187: training loss 0.760; learning rate 0.000411
Epoch 5 iteration 0166/0187: training loss 0.760; learning rate 0.000410
Epoch 5 iteration 0167/0187: training loss 0.760; learning rate 0.000410
Epoch 5 iteration 0168/0187: training loss 0.760; learning rate 0.000410
Epoch 5 iteration 0169/0187: training loss 0.760; learning rate 0.000410
Epoch 5 iteration 0170/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0171/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0172/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0173/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0174/0187: training loss 0.759; learning rate 0.000410
Epoch 5 iteration 0175/0187: training loss 0.759; learning rate 0.000410
Epoch 5 iteration 0176/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0177/0187: training loss 0.758; learning rate 0.000410
Epoch 5 iteration 0178/0187: training loss 0.760; learning rate 0.000410
Epoch 5 iteration 0179/0187: training loss 0.758; learning rate 0.000409
Epoch 5 iteration 0180/0187: training loss 0.757; learning rate 0.000409
Epoch 5 iteration 0181/0187: training loss 0.759; learning rate 0.000409
Epoch 5 iteration 0182/0187: training loss 0.758; learning rate 0.000409
Epoch 5 iteration 0183/0187: training loss 0.760; learning rate 0.000409
Epoch 5 iteration 0184/0187: training loss 0.761; learning rate 0.000409
Epoch 5 iteration 0185/0187: training loss 0.761; learning rate 0.000409
Epoch 5 iteration 0186/0187: training loss 0.762; learning rate 0.000409
Epoch 5 iteration 0187/0187: training loss 0.761; learning rate 0.000409
Epoch 5 validation pixAcc: 0.337, mIoU: 0.163
Epoch 6 iteration 0001/0187: training loss 0.750; learning rate 0.000409
Epoch 6 iteration 0002/0187: training loss 0.778; learning rate 0.000409
Epoch 6 iteration 0003/0187: training loss 0.759; learning rate 0.000408
Epoch 6 iteration 0004/0187: training loss 0.792; learning rate 0.000408
Epoch 6 iteration 0005/0187: training loss 0.816; learning rate 0.000408
Epoch 6 iteration 0006/0187: training loss 0.813; learning rate 0.000408
Epoch 6 iteration 0007/0187: training loss 0.788; learning rate 0.000408
Epoch 6 iteration 0008/0187: training loss 0.786; learning rate 0.000408
Epoch 6 iteration 0009/0187: training loss 0.787; learning rate 0.000408
Epoch 6 iteration 0010/0187: training loss 0.774; learning rate 0.000408
Epoch 6 iteration 0011/0187: training loss 0.782; learning rate 0.000408
Epoch 6 iteration 0012/0187: training loss 0.792; learning rate 0.000408
Epoch 6 iteration 0013/0187: training loss 0.801; learning rate 0.000408
Epoch 6 iteration 0014/0187: training loss 0.811; learning rate 0.000408
Epoch 6 iteration 0015/0187: training loss 0.797; learning rate 0.000407
Epoch 6 iteration 0016/0187: training loss 0.788; learning rate 0.000407
Epoch 6 iteration 0017/0187: training loss 0.779; learning rate 0.000407
Epoch 6 iteration 0018/0187: training loss 0.784; learning rate 0.000407
Epoch 6 iteration 0019/0187: training loss 0.783; learning rate 0.000407
Epoch 6 iteration 0020/0187: training loss 0.789; learning rate 0.000407
Epoch 6 iteration 0021/0187: training loss 0.789; learning rate 0.000407
Epoch 6 iteration 0022/0187: training loss 0.788; learning rate 0.000407
Epoch 6 iteration 0023/0187: training loss 0.782; learning rate 0.000407
Epoch 6 iteration 0024/0187: training loss 0.790; learning rate 0.000407
Epoch 6 iteration 0025/0187: training loss 0.781; learning rate 0.000407
Epoch 6 iteration 0026/0187: training loss 0.782; learning rate 0.000407
Epoch 6 iteration 0027/0187: training loss 0.789; learning rate 0.000406
Epoch 6 iteration 0028/0187: training loss 0.784; learning rate 0.000406
Epoch 6 iteration 0029/0187: training loss 0.780; learning rate 0.000406
Epoch 6 iteration 0030/0187: training loss 0.771; learning rate 0.000406
Epoch 6 iteration 0031/0187: training loss 0.772; learning rate 0.000406
Epoch 6 iteration 0032/0187: training loss 0.772; learning rate 0.000406
Epoch 6 iteration 0033/0187: training loss 0.781; learning rate 0.000406
Epoch 6 iteration 0034/0187: training loss 0.781; learning rate 0.000406
Epoch 6 iteration 0035/0187: training loss 0.781; learning rate 0.000406
Epoch 6 iteration 0036/0187: training loss 0.779; learning rate 0.000406
Epoch 6 iteration 0037/0187: training loss 0.777; learning rate 0.000406
Epoch 6 iteration 0038/0187: training loss 0.774; learning rate 0.000406
Epoch 6 iteration 0039/0187: training loss 0.771; learning rate 0.000405
Epoch 6 iteration 0040/0187: training loss 0.764; learning rate 0.000405
Epoch 6 iteration 0041/0187: training loss 0.764; learning rate 0.000405
Epoch 6 iteration 0042/0187: training loss 0.765; learning rate 0.000405
Epoch 6 iteration 0043/0187: training loss 0.762; learning rate 0.000405
Epoch 6 iteration 0044/0187: training loss 0.763; learning rate 0.000405
Epoch 6 iteration 0045/0187: training loss 0.761; learning rate 0.000405
Epoch 6 iteration 0046/0187: training loss 0.760; learning rate 0.000405
Epoch 6 iteration 0047/0187: training loss 0.765; learning rate 0.000405
Epoch 6 iteration 0048/0187: training loss 0.765; learning rate 0.000405
Epoch 6 iteration 0049/0187: training loss 0.766; learning rate 0.000405
Epoch 6 iteration 0050/0187: training loss 0.767; learning rate 0.000405
Epoch 6 iteration 0051/0187: training loss 0.765; learning rate 0.000404
Epoch 6 iteration 0052/0187: training loss 0.762; learning rate 0.000404
Epoch 6 iteration 0053/0187: training loss 0.763; learning rate 0.000404
Epoch 6 iteration 0054/0187: training loss 0.765; learning rate 0.000404
Epoch 6 iteration 0055/0187: training loss 0.763; learning rate 0.000404
Epoch 6 iteration 0056/0187: training loss 0.763; learning rate 0.000404
Epoch 6 iteration 0057/0187: training loss 0.767; learning rate 0.000404
Epoch 6 iteration 0058/0187: training loss 0.767; learning rate 0.000404
Epoch 6 iteration 0059/0187: training loss 0.767; learning rate 0.000404
Epoch 6 iteration 0060/0187: training loss 0.766; learning rate 0.000404
Epoch 6 iteration 0061/0187: training loss 0.767; learning rate 0.000404
Epoch 6 iteration 0062/0187: training loss 0.764; learning rate 0.000404
Epoch 6 iteration 0063/0187: training loss 0.764; learning rate 0.000404
Epoch 6 iteration 0064/0187: training loss 0.763; learning rate 0.000403
Epoch 6 iteration 0065/0187: training loss 0.763; learning rate 0.000403
Epoch 6 iteration 0066/0187: training loss 0.763; learning rate 0.000403
Epoch 6 iteration 0067/0187: training loss 0.767; learning rate 0.000403
Epoch 6 iteration 0068/0187: training loss 0.764; learning rate 0.000403
Epoch 6 iteration 0069/0187: training loss 0.765; learning rate 0.000403
Epoch 6 iteration 0070/0187: training loss 0.764; learning rate 0.000403
Epoch 6 iteration 0071/0187: training loss 0.761; learning rate 0.000403
Epoch 6 iteration 0072/0187: training loss 0.757; learning rate 0.000403
Epoch 6 iteration 0073/0187: training loss 0.758; learning rate 0.000403
Epoch 6 iteration 0074/0187: training loss 0.757; learning rate 0.000403
Epoch 6 iteration 0075/0187: training loss 0.757; learning rate 0.000403
Epoch 6 iteration 0076/0187: training loss 0.752; learning rate 0.000402
Epoch 6 iteration 0077/0187: training loss 0.755; learning rate 0.000402
Epoch 6 iteration 0078/0187: training loss 0.753; learning rate 0.000402
Epoch 6 iteration 0079/0187: training loss 0.756; learning rate 0.000402
Epoch 6 iteration 0080/0187: training loss 0.756; learning rate 0.000402
Epoch 6 iteration 0081/0187: training loss 0.758; learning rate 0.000402
Epoch 6 iteration 0082/0187: training loss 0.757; learning rate 0.000402
Epoch 6 iteration 0083/0187: training loss 0.756; learning rate 0.000402
Epoch 6 iteration 0084/0187: training loss 0.753; learning rate 0.000402
Epoch 6 iteration 0085/0187: training loss 0.752; learning rate 0.000402
Epoch 6 iteration 0086/0187: training loss 0.751; learning rate 0.000402
Epoch 6 iteration 0087/0187: training loss 0.750; learning rate 0.000402
Epoch 6 iteration 0088/0187: training loss 0.749; learning rate 0.000401
Epoch 6 iteration 0089/0187: training loss 0.751; learning rate 0.000401
Epoch 6 iteration 0090/0187: training loss 0.749; learning rate 0.000401
Epoch 6 iteration 0091/0188: training loss 0.750; learning rate 0.000401
Epoch 6 iteration 0092/0188: training loss 0.749; learning rate 0.000401
Epoch 6 iteration 0093/0188: training loss 0.750; learning rate 0.000401
Epoch 6 iteration 0094/0188: training loss 0.750; learning rate 0.000401
Epoch 6 iteration 0095/0188: training loss 0.750; learning rate 0.000401
Epoch 6 iteration 0096/0188: training loss 0.749; learning rate 0.000401
Epoch 6 iteration 0097/0188: training loss 0.750; learning rate 0.000401
Epoch 6 iteration 0098/0188: training loss 0.751; learning rate 0.000401
Epoch 6 iteration 0099/0188: training loss 0.753; learning rate 0.000401
Epoch 6 iteration 0100/0188: training loss 0.753; learning rate 0.000400
Epoch 6 iteration 0101/0188: training loss 0.754; learning rate 0.000400
Epoch 6 iteration 0102/0188: training loss 0.755; learning rate 0.000400
Epoch 6 iteration 0103/0188: training loss 0.755; learning rate 0.000400
Epoch 6 iteration 0104/0188: training loss 0.752; learning rate 0.000400
Epoch 6 iteration 0105/0188: training loss 0.753; learning rate 0.000400
Epoch 6 iteration 0106/0188: training loss 0.755; learning rate 0.000400
Epoch 6 iteration 0107/0188: training loss 0.757; learning rate 0.000400
Epoch 6 iteration 0108/0188: training loss 0.759; learning rate 0.000400
Epoch 6 iteration 0109/0188: training loss 0.759; learning rate 0.000400
Epoch 6 iteration 0110/0188: training loss 0.762; learning rate 0.000400
Epoch 6 iteration 0111/0188: training loss 0.762; learning rate 0.000400
Epoch 6 iteration 0112/0188: training loss 0.760; learning rate 0.000399
Epoch 6 iteration 0113/0188: training loss 0.761; learning rate 0.000399
Epoch 6 iteration 0114/0188: training loss 0.761; learning rate 0.000399
Epoch 6 iteration 0115/0188: training loss 0.760; learning rate 0.000399
Epoch 6 iteration 0116/0188: training loss 0.761; learning rate 0.000399
Epoch 6 iteration 0117/0188: training loss 0.761; learning rate 0.000399
Epoch 6 iteration 0118/0188: training loss 0.760; learning rate 0.000399
Epoch 6 iteration 0119/0188: training loss 0.764; learning rate 0.000399
Epoch 6 iteration 0120/0188: training loss 0.764; learning rate 0.000399
Epoch 6 iteration 0121/0188: training loss 0.763; learning rate 0.000399
Epoch 6 iteration 0122/0188: training loss 0.765; learning rate 0.000399
Epoch 6 iteration 0123/0188: training loss 0.765; learning rate 0.000399
Epoch 6 iteration 0124/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0125/0188: training loss 0.766; learning rate 0.000398
Epoch 6 iteration 0126/0188: training loss 0.766; learning rate 0.000398
Epoch 6 iteration 0127/0188: training loss 0.767; learning rate 0.000398
Epoch 6 iteration 0128/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0129/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0130/0188: training loss 0.764; learning rate 0.000398
Epoch 6 iteration 0131/0188: training loss 0.764; learning rate 0.000398
Epoch 6 iteration 0132/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0133/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0134/0188: training loss 0.766; learning rate 0.000398
Epoch 6 iteration 0135/0188: training loss 0.765; learning rate 0.000398
Epoch 6 iteration 0136/0188: training loss 0.764; learning rate 0.000398
Epoch 6 iteration 0137/0188: training loss 0.765; learning rate 0.000397
Epoch 6 iteration 0138/0188: training loss 0.766; learning rate 0.000397
Epoch 6 iteration 0139/0188: training loss 0.764; learning rate 0.000397
Epoch 6 iteration 0140/0188: training loss 0.765; learning rate 0.000397
Epoch 6 iteration 0141/0188: training loss 0.763; learning rate 0.000397
Epoch 6 iteration 0142/0188: training loss 0.764; learning rate 0.000397
Epoch 6 iteration 0143/0188: training loss 0.763; learning rate 0.000397
Epoch 6 iteration 0144/0188: training loss 0.762; learning rate 0.000397
Epoch 6 iteration 0145/0188: training loss 0.760; learning rate 0.000397
Epoch 6 iteration 0146/0188: training loss 0.761; learning rate 0.000397
Epoch 6 iteration 0147/0188: training loss 0.764; learning rate 0.000397
Epoch 6 iteration 0148/0188: training loss 0.763; learning rate 0.000397
Epoch 6 iteration 0149/0188: training loss 0.762; learning rate 0.000396
Epoch 6 iteration 0150/0188: training loss 0.763; learning rate 0.000396
Epoch 6 iteration 0151/0188: training loss 0.761; learning rate 0.000396
Epoch 6 iteration 0152/0188: training loss 0.762; learning rate 0.000396
Epoch 6 iteration 0153/0188: training loss 0.762; learning rate 0.000396
Epoch 6 iteration 0154/0188: training loss 0.761; learning rate 0.000396
Epoch 6 iteration 0155/0188: training loss 0.760; learning rate 0.000396
Epoch 6 iteration 0156/0188: training loss 0.759; learning rate 0.000396
Epoch 6 iteration 0157/0188: training loss 0.759; learning rate 0.000396
Epoch 6 iteration 0158/0188: training loss 0.759; learning rate 0.000396
Epoch 6 iteration 0159/0188: training loss 0.760; learning rate 0.000396
Epoch 6 iteration 0160/0188: training loss 0.758; learning rate 0.000396
Epoch 6 iteration 0161/0188: training loss 0.757; learning rate 0.000395
Epoch 6 iteration 0162/0188: training loss 0.757; learning rate 0.000395
Epoch 6 iteration 0163/0188: training loss 0.757; learning rate 0.000395
Epoch 6 iteration 0164/0188: training loss 0.758; learning rate 0.000395
Epoch 6 iteration 0165/0188: training loss 0.758; learning rate 0.000395
Epoch 6 iteration 0166/0188: training loss 0.759; learning rate 0.000395
Epoch 6 iteration 0167/0188: training loss 0.759; learning rate 0.000395
Epoch 6 iteration 0168/0188: training loss 0.759; learning rate 0.000395
Epoch 6 iteration 0169/0188: training loss 0.759; learning rate 0.000395
Epoch 6 iteration 0170/0188: training loss 0.758; learning rate 0.000395
Epoch 6 iteration 0171/0188: training loss 0.758; learning rate 0.000395
Epoch 6 iteration 0172/0188: training loss 0.758; learning rate 0.000395
Epoch 6 iteration 0173/0188: training loss 0.757; learning rate 0.000394
Epoch 6 iteration 0174/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0175/0188: training loss 0.757; learning rate 0.000394
Epoch 6 iteration 0176/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0177/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0178/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0179/0188: training loss 0.755; learning rate 0.000394
Epoch 6 iteration 0180/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0181/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0182/0188: training loss 0.756; learning rate 0.000394
Epoch 6 iteration 0183/0188: training loss 0.755; learning rate 0.000394
Epoch 6 iteration 0184/0188: training loss 0.755; learning rate 0.000394
Epoch 6 iteration 0185/0188: training loss 0.754; learning rate 0.000393
Epoch 6 iteration 0186/0188: training loss 0.755; learning rate 0.000393
Epoch 6 validation pixAcc: 0.338, mIoU: 0.161
Epoch 7 iteration 0001/0187: training loss 0.541; learning rate 0.000393
Epoch 7 iteration 0002/0187: training loss 0.572; learning rate 0.000393
Epoch 7 iteration 0003/0187: training loss 0.592; learning rate 0.000393
Epoch 7 iteration 0004/0187: training loss 0.614; learning rate 0.000393
Epoch 7 iteration 0005/0187: training loss 0.618; learning rate 0.000393
Epoch 7 iteration 0006/0187: training loss 0.650; learning rate 0.000393
Epoch 7 iteration 0007/0187: training loss 0.631; learning rate 0.000393
Epoch 7 iteration 0008/0187: training loss 0.632; learning rate 0.000393
Epoch 7 iteration 0009/0187: training loss 0.650; learning rate 0.000393
Epoch 7 iteration 0010/0187: training loss 0.678; learning rate 0.000392
Epoch 7 iteration 0011/0187: training loss 0.681; learning rate 0.000392
Epoch 7 iteration 0012/0187: training loss 0.695; learning rate 0.000392
Epoch 7 iteration 0013/0187: training loss 0.685; learning rate 0.000392
Epoch 7 iteration 0014/0187: training loss 0.677; learning rate 0.000392
Epoch 7 iteration 0015/0187: training loss 0.686; learning rate 0.000392
Epoch 7 iteration 0016/0187: training loss 0.694; learning rate 0.000392
Epoch 7 iteration 0017/0187: training loss 0.698; learning rate 0.000392
Epoch 7 iteration 0018/0187: training loss 0.700; learning rate 0.000392
Epoch 7 iteration 0019/0187: training loss 0.714; learning rate 0.000392
Epoch 7 iteration 0020/0187: training loss 0.717; learning rate 0.000392
Epoch 7 iteration 0021/0187: training loss 0.729; learning rate 0.000392
Epoch 7 iteration 0022/0187: training loss 0.741; learning rate 0.000391
Epoch 7 iteration 0023/0187: training loss 0.742; learning rate 0.000391
Epoch 7 iteration 0024/0187: training loss 0.744; learning rate 0.000391
Epoch 7 iteration 0025/0187: training loss 0.752; learning rate 0.000391
Epoch 7 iteration 0026/0187: training loss 0.749; learning rate 0.000391
Epoch 7 iteration 0027/0187: training loss 0.745; learning rate 0.000391
Epoch 7 iteration 0028/0187: training loss 0.745; learning rate 0.000391
Epoch 7 iteration 0029/0187: training loss 0.743; learning rate 0.000391
Epoch 7 iteration 0030/0187: training loss 0.736; learning rate 0.000391
Epoch 7 iteration 0031/0187: training loss 0.735; learning rate 0.000391
Epoch 7 iteration 0032/0187: training loss 0.736; learning rate 0.000391
Epoch 7 iteration 0033/0187: training loss 0.734; learning rate 0.000391
Epoch 7 iteration 0034/0187: training loss 0.734; learning rate 0.000391
Epoch 7 iteration 0035/0187: training loss 0.727; learning rate 0.000390
Epoch 7 iteration 0036/0187: training loss 0.727; learning rate 0.000390
Epoch 7 iteration 0037/0187: training loss 0.725; learning rate 0.000390
Epoch 7 iteration 0038/0187: training loss 0.732; learning rate 0.000390
Epoch 7 iteration 0039/0187: training loss 0.735; learning rate 0.000390
Epoch 7 iteration 0040/0187: training loss 0.740; learning rate 0.000390
Epoch 7 iteration 0041/0187: training loss 0.749; learning rate 0.000390
Epoch 7 iteration 0042/0187: training loss 0.753; learning rate 0.000390
Epoch 7 iteration 0043/0187: training loss 0.754; learning rate 0.000390
Epoch 7 iteration 0044/0187: training loss 0.751; learning rate 0.000390
Epoch 7 iteration 0045/0187: training loss 0.750; learning rate 0.000390
Epoch 7 iteration 0046/0187: training loss 0.750; learning rate 0.000390
Epoch 7 iteration 0047/0187: training loss 0.751; learning rate 0.000389
Epoch 7 iteration 0048/0187: training loss 0.752; learning rate 0.000389
Epoch 7 iteration 0049/0187: training loss 0.749; learning rate 0.000389
Epoch 7 iteration 0050/0187: training loss 0.744; learning rate 0.000389
Epoch 7 iteration 0051/0187: training loss 0.744; learning rate 0.000389
Epoch 7 iteration 0052/0187: training loss 0.745; learning rate 0.000389
Epoch 7 iteration 0053/0187: training loss 0.745; learning rate 0.000389
Epoch 7 iteration 0054/0187: training loss 0.743; learning rate 0.000389
Epoch 7 iteration 0055/0187: training loss 0.742; learning rate 0.000389
Epoch 7 iteration 0056/0187: training loss 0.742; learning rate 0.000389
Epoch 7 iteration 0057/0187: training loss 0.745; learning rate 0.000389
Epoch 7 iteration 0058/0187: training loss 0.745; learning rate 0.000389
Epoch 7 iteration 0059/0187: training loss 0.748; learning rate 0.000388
Epoch 7 iteration 0060/0187: training loss 0.747; learning rate 0.000388
Epoch 7 iteration 0061/0187: training loss 0.747; learning rate 0.000388
Epoch 7 iteration 0062/0187: training loss 0.743; learning rate 0.000388
Epoch 7 iteration 0063/0187: training loss 0.741; learning rate 0.000388
Epoch 7 iteration 0064/0187: training loss 0.739; learning rate 0.000388
Epoch 7 iteration 0065/0187: training loss 0.738; learning rate 0.000388
Epoch 7 iteration 0066/0187: training loss 0.739; learning rate 0.000388
Epoch 7 iteration 0067/0187: training loss 0.740; learning rate 0.000388
Epoch 7 iteration 0068/0187: training loss 0.742; learning rate 0.000388
Epoch 7 iteration 0069/0187: training loss 0.741; learning rate 0.000388
Epoch 7 iteration 0070/0187: training loss 0.739; learning rate 0.000388
Epoch 7 iteration 0071/0187: training loss 0.737; learning rate 0.000387
Epoch 7 iteration 0072/0187: training loss 0.736; learning rate 0.000387
Epoch 7 iteration 0073/0187: training loss 0.733; learning rate 0.000387
Epoch 7 iteration 0074/0187: training loss 0.738; learning rate 0.000387
Epoch 7 iteration 0075/0187: training loss 0.736; learning rate 0.000387
Epoch 7 iteration 0076/0187: training loss 0.739; learning rate 0.000387
Epoch 7 iteration 0077/0187: training loss 0.737; learning rate 0.000387
Epoch 7 iteration 0078/0187: training loss 0.736; learning rate 0.000387
Epoch 7 iteration 0079/0187: training loss 0.741; learning rate 0.000387
Epoch 7 iteration 0080/0187: training loss 0.743; learning rate 0.000387
Epoch 7 iteration 0081/0187: training loss 0.743; learning rate 0.000387
Epoch 7 iteration 0082/0187: training loss 0.744; learning rate 0.000387
Epoch 7 iteration 0083/0187: training loss 0.744; learning rate 0.000386
Epoch 7 iteration 0084/0187: training loss 0.741; learning rate 0.000386
Epoch 7 iteration 0085/0187: training loss 0.739; learning rate 0.000386
Epoch 7 iteration 0086/0187: training loss 0.742; learning rate 0.000386
Epoch 7 iteration 0087/0187: training loss 0.741; learning rate 0.000386
Epoch 7 iteration 0088/0187: training loss 0.741; learning rate 0.000386
Epoch 7 iteration 0089/0187: training loss 0.738; learning rate 0.000386
Epoch 7 iteration 0090/0187: training loss 0.738; learning rate 0.000386
Epoch 7 iteration 0091/0187: training loss 0.741; learning rate 0.000386
Epoch 7 iteration 0092/0187: training loss 0.741; learning rate 0.000386
Epoch 7 iteration 0093/0187: training loss 0.740; learning rate 0.000386
Epoch 7 iteration 0094/0187: training loss 0.742; learning rate 0.000386
Epoch 7 iteration 0095/0187: training loss 0.744; learning rate 0.000385
Epoch 7 iteration 0096/0187: training loss 0.744; learning rate 0.000385
Epoch 7 iteration 0097/0187: training loss 0.746; learning rate 0.000385
Epoch 7 iteration 0098/0187: training loss 0.743; learning rate 0.000385
Epoch 7 iteration 0099/0187: training loss 0.748; learning rate 0.000385
Epoch 7 iteration 0100/0187: training loss 0.747; learning rate 0.000385
Epoch 7 iteration 0101/0187: training loss 0.748; learning rate 0.000385
Epoch 7 iteration 0102/0187: training loss 0.746; learning rate 0.000385
Epoch 7 iteration 0103/0187: training loss 0.745; learning rate 0.000385
Epoch 7 iteration 0104/0187: training loss 0.744; learning rate 0.000385
Epoch 7 iteration 0105/0187: training loss 0.743; learning rate 0.000385
Epoch 7 iteration 0106/0187: training loss 0.744; learning rate 0.000385
Epoch 7 iteration 0107/0187: training loss 0.744; learning rate 0.000384
Epoch 7 iteration 0108/0187: training loss 0.742; learning rate 0.000384
Epoch 7 iteration 0109/0187: training loss 0.742; learning rate 0.000384
Epoch 7 iteration 0110/0187: training loss 0.746; learning rate 0.000384
Epoch 7 iteration 0111/0187: training loss 0.743; learning rate 0.000384
Epoch 7 iteration 0112/0187: training loss 0.743; learning rate 0.000384
Epoch 7 iteration 0113/0187: training loss 0.741; learning rate 0.000384
Epoch 7 iteration 0114/0187: training loss 0.740; learning rate 0.000384
Epoch 7 iteration 0115/0187: training loss 0.740; learning rate 0.000384
Epoch 7 iteration 0116/0187: training loss 0.739; learning rate 0.000384
Epoch 7 iteration 0117/0187: training loss 0.737; learning rate 0.000384
Epoch 7 iteration 0118/0187: training loss 0.736; learning rate 0.000384
Epoch 7 iteration 0119/0187: training loss 0.734; learning rate 0.000383
Epoch 7 iteration 0120/0187: training loss 0.736; learning rate 0.000383
Epoch 7 iteration 0121/0187: training loss 0.737; learning rate 0.000383
Epoch 7 iteration 0122/0187: training loss 0.736; learning rate 0.000383
Epoch 7 iteration 0123/0187: training loss 0.735; learning rate 0.000383
Epoch 7 iteration 0124/0187: training loss 0.736; learning rate 0.000383
Epoch 7 iteration 0125/0187: training loss 0.735; learning rate 0.000383
Epoch 7 iteration 0126/0187: training loss 0.738; learning rate 0.000383
Epoch 7 iteration 0127/0187: training loss 0.739; learning rate 0.000383
Epoch 7 iteration 0128/0187: training loss 0.738; learning rate 0.000383
Epoch 7 iteration 0129/0187: training loss 0.737; learning rate 0.000383
Epoch 7 iteration 0130/0187: training loss 0.741; learning rate 0.000383
Epoch 7 iteration 0131/0187: training loss 0.743; learning rate 0.000382
Epoch 7 iteration 0132/0187: training loss 0.743; learning rate 0.000382
Epoch 7 iteration 0133/0187: training loss 0.743; learning rate 0.000382
Epoch 7 iteration 0134/0187: training loss 0.744; learning rate 0.000382
Epoch 7 iteration 0135/0187: training loss 0.743; learning rate 0.000382
Epoch 7 iteration 0136/0187: training loss 0.742; learning rate 0.000382
Epoch 7 iteration 0137/0187: training loss 0.740; learning rate 0.000382
Epoch 7 iteration 0138/0187: training loss 0.740; learning rate 0.000382
Epoch 7 iteration 0139/0187: training loss 0.740; learning rate 0.000382
Epoch 7 iteration 0140/0187: training loss 0.739; learning rate 0.000382
Epoch 7 iteration 0141/0187: training loss 0.738; learning rate 0.000382
Epoch 7 iteration 0142/0187: training loss 0.736; learning rate 0.000382
Epoch 7 iteration 0143/0187: training loss 0.736; learning rate 0.000382
Epoch 7 iteration 0144/0187: training loss 0.735; learning rate 0.000381
Epoch 7 iteration 0145/0187: training loss 0.736; learning rate 0.000381
Epoch 7 iteration 0146/0187: training loss 0.736; learning rate 0.000381
Epoch 7 iteration 0147/0187: training loss 0.735; learning rate 0.000381
Epoch 7 iteration 0148/0187: training loss 0.736; learning rate 0.000381
Epoch 7 iteration 0149/0187: training loss 0.738; learning rate 0.000381
Epoch 7 iteration 0150/0187: training loss 0.738; learning rate 0.000381
Epoch 7 iteration 0151/0187: training loss 0.738; learning rate 0.000381
Epoch 7 iteration 0152/0187: training loss 0.739; learning rate 0.000381
Epoch 7 iteration 0153/0187: training loss 0.740; learning rate 0.000381
Epoch 7 iteration 0154/0187: training loss 0.741; learning rate 0.000381
Epoch 7 iteration 0155/0187: training loss 0.743; learning rate 0.000381
Epoch 7 iteration 0156/0187: training loss 0.742; learning rate 0.000380
Epoch 7 iteration 0157/0187: training loss 0.741; learning rate 0.000380
Epoch 7 iteration 0158/0187: training loss 0.742; learning rate 0.000380
Epoch 7 iteration 0159/0187: training loss 0.743; learning rate 0.000380
Epoch 7 iteration 0160/0187: training loss 0.741; learning rate 0.000380
Epoch 7 iteration 0161/0187: training loss 0.741; learning rate 0.000380
Epoch 7 iteration 0162/0187: training loss 0.741; learning rate 0.000380
Epoch 7 iteration 0163/0187: training loss 0.740; learning rate 0.000380
Epoch 7 iteration 0164/0187: training loss 0.740; learning rate 0.000380
Epoch 7 iteration 0165/0187: training loss 0.742; learning rate 0.000380
Epoch 7 iteration 0166/0187: training loss 0.741; learning rate 0.000380
Epoch 7 iteration 0167/0187: training loss 0.742; learning rate 0.000380
Epoch 7 iteration 0168/0187: training loss 0.741; learning rate 0.000379
Epoch 7 iteration 0169/0187: training loss 0.740; learning rate 0.000379
Epoch 7 iteration 0170/0187: training loss 0.739; learning rate 0.000379
Epoch 7 iteration 0171/0187: training loss 0.739; learning rate 0.000379
Epoch 7 iteration 0172/0187: training loss 0.739; learning rate 0.000379
Epoch 7 iteration 0173/0187: training loss 0.740; learning rate 0.000379
Epoch 7 iteration 0174/0187: training loss 0.740; learning rate 0.000379
Epoch 7 iteration 0175/0187: training loss 0.739; learning rate 0.000379
Epoch 7 iteration 0176/0187: training loss 0.740; learning rate 0.000379
Epoch 7 iteration 0177/0187: training loss 0.743; learning rate 0.000379
Epoch 7 iteration 0178/0187: training loss 0.743; learning rate 0.000379
Epoch 7 iteration 0179/0187: training loss 0.743; learning rate 0.000379
Epoch 7 iteration 0180/0187: training loss 0.743; learning rate 0.000378
Epoch 7 iteration 0181/0187: training loss 0.743; learning rate 0.000378
Epoch 7 iteration 0182/0187: training loss 0.743; learning rate 0.000378
Epoch 7 iteration 0183/0187: training loss 0.743; learning rate 0.000378
Epoch 7 iteration 0184/0187: training loss 0.741; learning rate 0.000378
Epoch 7 iteration 0185/0187: training loss 0.741; learning rate 0.000378
Epoch 7 iteration 0186/0187: training loss 0.741; learning rate 0.000378
Epoch 7 iteration 0187/0187: training loss 0.740; learning rate 0.000378
Epoch 7 validation pixAcc: 0.340, mIoU: 0.163
Epoch 8 iteration 0001/0187: training loss 0.712; learning rate 0.000378
Epoch 8 iteration 0002/0187: training loss 0.715; learning rate 0.000378
Epoch 8 iteration 0003/0187: training loss 0.694; learning rate 0.000378
Epoch 8 iteration 0004/0187: training loss 0.675; learning rate 0.000377
Epoch 8 iteration 0005/0187: training loss 0.735; learning rate 0.000377
Epoch 8 iteration 0006/0187: training loss 0.768; learning rate 0.000377
Epoch 8 iteration 0007/0187: training loss 0.752; learning rate 0.000377
Epoch 8 iteration 0008/0187: training loss 0.718; learning rate 0.000377
Epoch 8 iteration 0009/0187: training loss 0.747; learning rate 0.000377
Epoch 8 iteration 0010/0187: training loss 0.737; learning rate 0.000377
Epoch 8 iteration 0011/0187: training loss 0.735; learning rate 0.000377
Epoch 8 iteration 0012/0187: training loss 0.721; learning rate 0.000377
Epoch 8 iteration 0013/0187: training loss 0.731; learning rate 0.000377
Epoch 8 iteration 0014/0187: training loss 0.724; learning rate 0.000377
Epoch 8 iteration 0015/0187: training loss 0.719; learning rate 0.000377
Epoch 8 iteration 0016/0187: training loss 0.705; learning rate 0.000376
Epoch 8 iteration 0017/0187: training loss 0.706; learning rate 0.000376
Epoch 8 iteration 0018/0187: training loss 0.701; learning rate 0.000376
Epoch 8 iteration 0019/0187: training loss 0.699; learning rate 0.000376
Epoch 8 iteration 0020/0187: training loss 0.689; learning rate 0.000376
Epoch 8 iteration 0021/0187: training loss 0.692; learning rate 0.000376
Epoch 8 iteration 0022/0187: training loss 0.695; learning rate 0.000376
Epoch 8 iteration 0023/0187: training loss 0.693; learning rate 0.000376
Epoch 8 iteration 0024/0187: training loss 0.691; learning rate 0.000376
Epoch 8 iteration 0025/0187: training loss 0.693; learning rate 0.000376
Epoch 8 iteration 0026/0187: training loss 0.689; learning rate 0.000376
Epoch 8 iteration 0027/0187: training loss 0.692; learning rate 0.000376
Epoch 8 iteration 0028/0187: training loss 0.690; learning rate 0.000375
Epoch 8 iteration 0029/0187: training loss 0.690; learning rate 0.000375
Epoch 8 iteration 0030/0187: training loss 0.693; learning rate 0.000375
Epoch 8 iteration 0031/0187: training loss 0.692; learning rate 0.000375
Epoch 8 iteration 0032/0187: training loss 0.690; learning rate 0.000375
Epoch 8 iteration 0033/0187: training loss 0.691; learning rate 0.000375
Epoch 8 iteration 0034/0187: training loss 0.692; learning rate 0.000375
Epoch 8 iteration 0035/0187: training loss 0.691; learning rate 0.000375
Epoch 8 iteration 0036/0187: training loss 0.696; learning rate 0.000375
Epoch 8 iteration 0037/0187: training loss 0.688; learning rate 0.000375
Epoch 8 iteration 0038/0187: training loss 0.690; learning rate 0.000375
Epoch 8 iteration 0039/0187: training loss 0.703; learning rate 0.000375
Epoch 8 iteration 0040/0187: training loss 0.703; learning rate 0.000374
Epoch 8 iteration 0041/0187: training loss 0.702; learning rate 0.000374
Epoch 8 iteration 0042/0187: training loss 0.705; learning rate 0.000374
Epoch 8 iteration 0043/0187: training loss 0.702; learning rate 0.000374
Epoch 8 iteration 0044/0187: training loss 0.710; learning rate 0.000374
Epoch 8 iteration 0045/0187: training loss 0.725; learning rate 0.000374
Epoch 8 iteration 0046/0187: training loss 0.727; learning rate 0.000374
Epoch 8 iteration 0047/0187: training loss 0.726; learning rate 0.000374
Epoch 8 iteration 0048/0187: training loss 0.724; learning rate 0.000374
Epoch 8 iteration 0049/0187: training loss 0.725; learning rate 0.000374
Epoch 8 iteration 0050/0187: training loss 0.724; learning rate 0.000374
Epoch 8 iteration 0051/0187: training loss 0.729; learning rate 0.000374
Epoch 8 iteration 0052/0187: training loss 0.731; learning rate 0.000373
Epoch 8 iteration 0053/0187: training loss 0.728; learning rate 0.000373
Epoch 8 iteration 0054/0187: training loss 0.726; learning rate 0.000373
Epoch 8 iteration 0055/0187: training loss 0.729; learning rate 0.000373
Epoch 8 iteration 0056/0187: training loss 0.726; learning rate 0.000373
Epoch 8 iteration 0057/0187: training loss 0.726; learning rate 0.000373
Epoch 8 iteration 0058/0187: training loss 0.726; learning rate 0.000373
Epoch 8 iteration 0059/0187: training loss 0.725; learning rate 0.000373
Epoch 8 iteration 0060/0187: training loss 0.722; learning rate 0.000373
Epoch 8 iteration 0061/0187: training loss 0.719; learning rate 0.000373
Epoch 8 iteration 0062/0187: training loss 0.719; learning rate 0.000373
Epoch 8 iteration 0063/0187: training loss 0.721; learning rate 0.000373
Epoch 8 iteration 0064/0187: training loss 0.721; learning rate 0.000372
Epoch 8 iteration 0065/0187: training loss 0.722; learning rate 0.000372
Epoch 8 iteration 0066/0187: training loss 0.721; learning rate 0.000372
Epoch 8 iteration 0067/0187: training loss 0.720; learning rate 0.000372
Epoch 8 iteration 0068/0187: training loss 0.722; learning rate 0.000372
Epoch 8 iteration 0069/0187: training loss 0.720; learning rate 0.000372
Epoch 8 iteration 0070/0187: training loss 0.720; learning rate 0.000372
Epoch 8 iteration 0071/0187: training loss 0.719; learning rate 0.000372
Epoch 8 iteration 0072/0187: training loss 0.720; learning rate 0.000372
Epoch 8 iteration 0073/0187: training loss 0.721; learning rate 0.000372
Epoch 8 iteration 0074/0187: training loss 0.725; learning rate 0.000372
Epoch 8 iteration 0075/0187: training loss 0.728; learning rate 0.000372
Epoch 8 iteration 0076/0187: training loss 0.730; learning rate 0.000371
Epoch 8 iteration 0077/0187: training loss 0.727; learning rate 0.000371
Epoch 8 iteration 0078/0187: training loss 0.725; learning rate 0.000371
Epoch 8 iteration 0079/0187: training loss 0.725; learning rate 0.000371
Epoch 8 iteration 0080/0187: training loss 0.726; learning rate 0.000371
Epoch 8 iteration 0081/0187: training loss 0.725; learning rate 0.000371
Epoch 8 iteration 0082/0187: training loss 0.726; learning rate 0.000371
Epoch 8 iteration 0083/0187: training loss 0.726; learning rate 0.000371
Epoch 8 iteration 0084/0187: training loss 0.725; learning rate 0.000371
Epoch 8 iteration 0085/0187: training loss 0.724; learning rate 0.000371
Epoch 8 iteration 0086/0187: training loss 0.722; learning rate 0.000371
Epoch 8 iteration 0087/0187: training loss 0.722; learning rate 0.000371
Epoch 8 iteration 0088/0187: training loss 0.724; learning rate 0.000370
Epoch 8 iteration 0089/0187: training loss 0.723; learning rate 0.000370
Epoch 8 iteration 0090/0187: training loss 0.726; learning rate 0.000370
Epoch 8 iteration 0091/0188: training loss 0.726; learning rate 0.000370
Epoch 8 iteration 0092/0188: training loss 0.726; learning rate 0.000370
Epoch 8 iteration 0093/0188: training loss 0.723; learning rate 0.000370
Epoch 8 iteration 0094/0188: training loss 0.724; learning rate 0.000370
Epoch 8 iteration 0095/0188: training loss 0.725; learning rate 0.000370
Epoch 8 iteration 0096/0188: training loss 0.724; learning rate 0.000370
Epoch 8 iteration 0097/0188: training loss 0.725; learning rate 0.000370
Epoch 8 iteration 0098/0188: training loss 0.722; learning rate 0.000370
Epoch 8 iteration 0099/0188: training loss 0.719; learning rate 0.000370
Epoch 8 iteration 0100/0188: training loss 0.718; learning rate 0.000369
Epoch 8 iteration 0101/0188: training loss 0.718; learning rate 0.000369
Epoch 8 iteration 0102/0188: training loss 0.722; learning rate 0.000369
Epoch 8 iteration 0103/0188: training loss 0.720; learning rate 0.000369
Epoch 8 iteration 0104/0188: training loss 0.718; learning rate 0.000369
Epoch 8 iteration 0105/0188: training loss 0.719; learning rate 0.000369
Epoch 8 iteration 0106/0188: training loss 0.719; learning rate 0.000369
Epoch 8 iteration 0107/0188: training loss 0.719; learning rate 0.000369
Epoch 8 iteration 0108/0188: training loss 0.721; learning rate 0.000369
Epoch 8 iteration 0109/0188: training loss 0.722; learning rate 0.000369
Epoch 8 iteration 0110/0188: training loss 0.722; learning rate 0.000369
Epoch 8 iteration 0111/0188: training loss 0.722; learning rate 0.000369
Epoch 8 iteration 0112/0188: training loss 0.722; learning rate 0.000368
Epoch 8 iteration 0113/0188: training loss 0.723; learning rate 0.000368
Epoch 8 iteration 0114/0188: training loss 0.723; learning rate 0.000368
Epoch 8 iteration 0115/0188: training loss 0.725; learning rate 0.000368
Epoch 8 iteration 0116/0188: training loss 0.725; learning rate 0.000368
Epoch 8 iteration 0117/0188: training loss 0.725; learning rate 0.000368
Epoch 8 iteration 0118/0188: training loss 0.725; learning rate 0.000368
Epoch 8 iteration 0119/0188: training loss 0.724; learning rate 0.000368
Epoch 8 iteration 0120/0188: training loss 0.725; learning rate 0.000368
Epoch 8 iteration 0121/0188: training loss 0.724; learning rate 0.000368
Epoch 8 iteration 0122/0188: training loss 0.723; learning rate 0.000368
Epoch 8 iteration 0123/0188: training loss 0.722; learning rate 0.000368
Epoch 8 iteration 0124/0188: training loss 0.722; learning rate 0.000368
Epoch 8 iteration 0125/0188: training loss 0.721; learning rate 0.000367
Epoch 8 iteration 0126/0188: training loss 0.721; learning rate 0.000367
Epoch 8 iteration 0127/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0128/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0129/0188: training loss 0.721; learning rate 0.000367
Epoch 8 iteration 0130/0188: training loss 0.723; learning rate 0.000367
Epoch 8 iteration 0131/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0132/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0133/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0134/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0135/0188: training loss 0.723; learning rate 0.000367
Epoch 8 iteration 0136/0188: training loss 0.722; learning rate 0.000367
Epoch 8 iteration 0137/0188: training loss 0.723; learning rate 0.000366
Epoch 8 iteration 0138/0188: training loss 0.722; learning rate 0.000366
Epoch 8 iteration 0139/0188: training loss 0.722; learning rate 0.000366
Epoch 8 iteration 0140/0188: training loss 0.722; learning rate 0.000366
Epoch 8 iteration 0141/0188: training loss 0.724; learning rate 0.000366
Epoch 8 iteration 0142/0188: training loss 0.724; learning rate 0.000366
Epoch 8 iteration 0143/0188: training loss 0.726; learning rate 0.000366
Epoch 8 iteration 0144/0188: training loss 0.725; learning rate 0.000366
Epoch 8 iteration 0145/0188: training loss 0.724; learning rate 0.000366
Epoch 8 iteration 0146/0188: training loss 0.722; learning rate 0.000366
Epoch 8 iteration 0147/0188: training loss 0.721; learning rate 0.000366
Epoch 8 iteration 0148/0188: training loss 0.720; learning rate 0.000366
Epoch 8 iteration 0149/0188: training loss 0.719; learning rate 0.000365
Epoch 8 iteration 0150/0188: training loss 0.718; learning rate 0.000365
Epoch 8 iteration 0151/0188: training loss 0.718; learning rate 0.000365
Epoch 8 iteration 0152/0188: training loss 0.718; learning rate 0.000365
Epoch 8 iteration 0153/0188: training loss 0.720; learning rate 0.000365
Epoch 8 iteration 0154/0188: training loss 0.722; learning rate 0.000365
Epoch 8 iteration 0155/0188: training loss 0.723; learning rate 0.000365
Epoch 8 iteration 0156/0188: training loss 0.722; learning rate 0.000365
Epoch 8 iteration 0157/0188: training loss 0.722; learning rate 0.000365
Epoch 8 iteration 0158/0188: training loss 0.721; learning rate 0.000365
Epoch 8 iteration 0159/0188: training loss 0.721; learning rate 0.000365
Epoch 8 iteration 0160/0188: training loss 0.721; learning rate 0.000365
Epoch 8 iteration 0161/0188: training loss 0.720; learning rate 0.000364
Epoch 8 iteration 0162/0188: training loss 0.722; learning rate 0.000364
Epoch 8 iteration 0163/0188: training loss 0.720; learning rate 0.000364
Epoch 8 iteration 0164/0188: training loss 0.720; learning rate 0.000364
Epoch 8 iteration 0165/0188: training loss 0.720; learning rate 0.000364
Epoch 8 iteration 0166/0188: training loss 0.720; learning rate 0.000364
Epoch 8 iteration 0167/0188: training loss 0.721; learning rate 0.000364
Epoch 8 iteration 0168/0188: training loss 0.722; learning rate 0.000364
Epoch 8 iteration 0169/0188: training loss 0.724; learning rate 0.000364
Epoch 8 iteration 0170/0188: training loss 0.724; learning rate 0.000364
Epoch 8 iteration 0171/0188: training loss 0.722; learning rate 0.000364
Epoch 8 iteration 0172/0188: training loss 0.722; learning rate 0.000364
Epoch 8 iteration 0173/0188: training loss 0.721; learning rate 0.000363
Epoch 8 iteration 0174/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0175/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0176/0188: training loss 0.723; learning rate 0.000363
Epoch 8 iteration 0177/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0178/0188: training loss 0.724; learning rate 0.000363
Epoch 8 iteration 0179/0188: training loss 0.723; learning rate 0.000363
Epoch 8 iteration 0180/0188: training loss 0.723; learning rate 0.000363
Epoch 8 iteration 0181/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0182/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0183/0188: training loss 0.723; learning rate 0.000363
Epoch 8 iteration 0184/0188: training loss 0.722; learning rate 0.000363
Epoch 8 iteration 0185/0188: training loss 0.722; learning rate 0.000362
Epoch 8 iteration 0186/0188: training loss 0.721; learning rate 0.000362
Epoch 8 validation pixAcc: 0.339, mIoU: 0.172
Epoch 9 iteration 0001/0187: training loss 0.808; learning rate 0.000362
Epoch 9 iteration 0002/0187: training loss 0.872; learning rate 0.000362
Epoch 9 iteration 0003/0187: training loss 0.914; learning rate 0.000362
Epoch 9 iteration 0004/0187: training loss 0.874; learning rate 0.000362
Epoch 9 iteration 0005/0187: training loss 0.873; learning rate 0.000362
Epoch 9 iteration 0006/0187: training loss 0.853; learning rate 0.000362
Epoch 9 iteration 0007/0187: training loss 0.836; learning rate 0.000362
Epoch 9 iteration 0008/0187: training loss 0.820; learning rate 0.000362
Epoch 9 iteration 0009/0187: training loss 0.796; learning rate 0.000362
Epoch 9 iteration 0010/0187: training loss 0.794; learning rate 0.000361
Epoch 9 iteration 0011/0187: training loss 0.785; learning rate 0.000361
Epoch 9 iteration 0012/0187: training loss 0.770; learning rate 0.000361
Epoch 9 iteration 0013/0187: training loss 0.767; learning rate 0.000361
Epoch 9 iteration 0014/0187: training loss 0.770; learning rate 0.000361
Epoch 9 iteration 0015/0187: training loss 0.758; learning rate 0.000361
Epoch 9 iteration 0016/0187: training loss 0.755; learning rate 0.000361
Epoch 9 iteration 0017/0187: training loss 0.745; learning rate 0.000361
Epoch 9 iteration 0018/0187: training loss 0.730; learning rate 0.000361
Epoch 9 iteration 0019/0187: training loss 0.726; learning rate 0.000361
Epoch 9 iteration 0020/0187: training loss 0.715; learning rate 0.000361
Epoch 9 iteration 0021/0187: training loss 0.705; learning rate 0.000361
Epoch 9 iteration 0022/0187: training loss 0.698; learning rate 0.000360
Epoch 9 iteration 0023/0187: training loss 0.701; learning rate 0.000360
Epoch 9 iteration 0024/0187: training loss 0.696; learning rate 0.000360
Epoch 9 iteration 0025/0187: training loss 0.692; learning rate 0.000360
Epoch 9 iteration 0026/0187: training loss 0.686; learning rate 0.000360
Epoch 9 iteration 0027/0187: training loss 0.694; learning rate 0.000360
Epoch 9 iteration 0028/0187: training loss 0.697; learning rate 0.000360
Epoch 9 iteration 0029/0187: training loss 0.698; learning rate 0.000360
Epoch 9 iteration 0030/0187: training loss 0.703; learning rate 0.000360
Epoch 9 iteration 0031/0187: training loss 0.699; learning rate 0.000360
Epoch 9 iteration 0032/0187: training loss 0.698; learning rate 0.000360
Epoch 9 iteration 0033/0187: training loss 0.696; learning rate 0.000360
Epoch 9 iteration 0034/0187: training loss 0.703; learning rate 0.000359
Epoch 9 iteration 0035/0187: training loss 0.703; learning rate 0.000359
Epoch 9 iteration 0036/0187: training loss 0.703; learning rate 0.000359
Epoch 9 iteration 0037/0187: training loss 0.700; learning rate 0.000359
Epoch 9 iteration 0038/0187: training loss 0.697; learning rate 0.000359
Epoch 9 iteration 0039/0187: training loss 0.697; learning rate 0.000359
Epoch 9 iteration 0040/0187: training loss 0.698; learning rate 0.000359
Epoch 9 iteration 0041/0187: training loss 0.698; learning rate 0.000359
Epoch 9 iteration 0042/0187: training loss 0.702; learning rate 0.000359
Epoch 9 iteration 0043/0187: training loss 0.700; learning rate 0.000359
Epoch 9 iteration 0044/0187: training loss 0.697; learning rate 0.000359
Epoch 9 iteration 0045/0187: training loss 0.698; learning rate 0.000359
Epoch 9 iteration 0046/0187: training loss 0.701; learning rate 0.000358
Epoch 9 iteration 0047/0187: training loss 0.705; learning rate 0.000358
Epoch 9 iteration 0048/0187: training loss 0.706; learning rate 0.000358
Epoch 9 iteration 0049/0187: training loss 0.703; learning rate 0.000358
Epoch 9 iteration 0050/0187: training loss 0.702; learning rate 0.000358
Epoch 9 iteration 0051/0187: training loss 0.699; learning rate 0.000358
Epoch 9 iteration 0052/0187: training loss 0.699; learning rate 0.000358
Epoch 9 iteration 0053/0187: training loss 0.695; learning rate 0.000358
Epoch 9 iteration 0054/0187: training loss 0.697; learning rate 0.000358
Epoch 9 iteration 0055/0187: training loss 0.697; learning rate 0.000358
Epoch 9 iteration 0056/0187: training loss 0.698; learning rate 0.000358
Epoch 9 iteration 0057/0187: training loss 0.702; learning rate 0.000358
Epoch 9 iteration 0058/0187: training loss 0.705; learning rate 0.000357
Epoch 9 iteration 0059/0187: training loss 0.704; learning rate 0.000357
Epoch 9 iteration 0060/0187: training loss 0.706; learning rate 0.000357
Epoch 9 iteration 0061/0187: training loss 0.707; learning rate 0.000357
Epoch 9 iteration 0062/0187: training loss 0.703; learning rate 0.000357
Epoch 9 iteration 0063/0187: training loss 0.702; learning rate 0.000357
Epoch 9 iteration 0064/0187: training loss 0.709; learning rate 0.000357
Epoch 9 iteration 0065/0187: training loss 0.707; learning rate 0.000357
Epoch 9 iteration 0066/0187: training loss 0.707; learning rate 0.000357
Epoch 9 iteration 0067/0187: training loss 0.707; learning rate 0.000357
Epoch 9 iteration 0068/0187: training loss 0.706; learning rate 0.000357
Epoch 9 iteration 0069/0187: training loss 0.704; learning rate 0.000357
Epoch 9 iteration 0070/0187: training loss 0.703; learning rate 0.000356
Epoch 9 iteration 0071/0187: training loss 0.703; learning rate 0.000356
Epoch 9 iteration 0072/0187: training loss 0.703; learning rate 0.000356
Epoch 9 iteration 0073/0187: training loss 0.702; learning rate 0.000356
Epoch 9 iteration 0074/0187: training loss 0.705; learning rate 0.000356
Epoch 9 iteration 0075/0187: training loss 0.704; learning rate 0.000356
Epoch 9 iteration 0076/0187: training loss 0.702; learning rate 0.000356
Epoch 9 iteration 0077/0187: training loss 0.705; learning rate 0.000356
Epoch 9 iteration 0078/0187: training loss 0.707; learning rate 0.000356
Epoch 9 iteration 0079/0187: training loss 0.705; learning rate 0.000356
Epoch 9 iteration 0080/0187: training loss 0.705; learning rate 0.000356
Epoch 9 iteration 0081/0187: training loss 0.706; learning rate 0.000356
Epoch 9 iteration 0082/0187: training loss 0.704; learning rate 0.000355
Epoch 9 iteration 0083/0187: training loss 0.707; learning rate 0.000355
Epoch 9 iteration 0084/0187: training loss 0.706; learning rate 0.000355
Epoch 9 iteration 0085/0187: training loss 0.705; learning rate 0.000355
Epoch 9 iteration 0086/0187: training loss 0.705; learning rate 0.000355
Epoch 9 iteration 0087/0187: training loss 0.702; learning rate 0.000355
Epoch 9 iteration 0088/0187: training loss 0.700; learning rate 0.000355
Epoch 9 iteration 0089/0187: training loss 0.699; learning rate 0.000355
Epoch 9 iteration 0090/0187: training loss 0.698; learning rate 0.000355
Epoch 9 iteration 0091/0187: training loss 0.698; learning rate 0.000355
Epoch 9 iteration 0092/0187: training loss 0.698; learning rate 0.000355
Epoch 9 iteration 0093/0187: training loss 0.698; learning rate 0.000355
Epoch 9 iteration 0094/0187: training loss 0.699; learning rate 0.000354
Epoch 9 iteration 0095/0187: training loss 0.699; learning rate 0.000354
Epoch 9 iteration 0096/0187: training loss 0.697; learning rate 0.000354
Epoch 9 iteration 0097/0187: training loss 0.698; learning rate 0.000354
Epoch 9 iteration 0098/0187: training loss 0.698; learning rate 0.000354
Epoch 9 iteration 0099/0187: training loss 0.700; learning rate 0.000354
Epoch 9 iteration 0100/0187: training loss 0.699; learning rate 0.000354
Epoch 9 iteration 0101/0187: training loss 0.698; learning rate 0.000354
Epoch 9 iteration 0102/0187: training loss 0.698; learning rate 0.000354
Epoch 9 iteration 0103/0187: training loss 0.701; learning rate 0.000354
Epoch 9 iteration 0104/0187: training loss 0.702; learning rate 0.000354
Epoch 9 iteration 0105/0187: training loss 0.702; learning rate 0.000354
Epoch 9 iteration 0106/0187: training loss 0.700; learning rate 0.000353
Epoch 9 iteration 0107/0187: training loss 0.702; learning rate 0.000353
Epoch 9 iteration 0108/0187: training loss 0.701; learning rate 0.000353
Epoch 9 iteration 0109/0187: training loss 0.699; learning rate 0.000353
Epoch 9 iteration 0110/0187: training loss 0.700; learning rate 0.000353
Epoch 9 iteration 0111/0187: training loss 0.700; learning rate 0.000353
Epoch 9 iteration 0112/0187: training loss 0.700; learning rate 0.000353
Epoch 9 iteration 0113/0187: training loss 0.699; learning rate 0.000353
Epoch 9 iteration 0114/0187: training loss 0.698; learning rate 0.000353
Epoch 9 iteration 0115/0187: training loss 0.699; learning rate 0.000353
Epoch 9 iteration 0116/0187: training loss 0.699; learning rate 0.000353
Epoch 9 iteration 0117/0187: training loss 0.698; learning rate 0.000353
Epoch 9 iteration 0118/0187: training loss 0.696; learning rate 0.000352
Epoch 9 iteration 0119/0187: training loss 0.697; learning rate 0.000352
Epoch 9 iteration 0120/0187: training loss 0.695; learning rate 0.000352
Epoch 9 iteration 0121/0187: training loss 0.693; learning rate 0.000352
Epoch 9 iteration 0122/0187: training loss 0.693; learning rate 0.000352
Epoch 9 iteration 0123/0187: training loss 0.694; learning rate 0.000352
Epoch 9 iteration 0124/0187: training loss 0.693; learning rate 0.000352
Epoch 9 iteration 0125/0187: training loss 0.694; learning rate 0.000352
Epoch 9 iteration 0126/0187: training loss 0.697; learning rate 0.000352
Epoch 9 iteration 0127/0187: training loss 0.698; learning rate 0.000352
Epoch 9 iteration 0128/0187: training loss 0.696; learning rate 0.000352
Epoch 9 iteration 0129/0187: training loss 0.695; learning rate 0.000352
Epoch 9 iteration 0130/0187: training loss 0.696; learning rate 0.000351
Epoch 9 iteration 0131/0187: training loss 0.696; learning rate 0.000351
Epoch 9 iteration 0132/0187: training loss 0.696; learning rate 0.000351
Epoch 9 iteration 0133/0187: training loss 0.697; learning rate 0.000351
Epoch 9 iteration 0134/0187: training loss 0.699; learning rate 0.000351
Epoch 9 iteration 0135/0187: training loss 0.699; learning rate 0.000351
Epoch 9 iteration 0136/0187: training loss 0.698; learning rate 0.000351
Epoch 9 iteration 0137/0187: training loss 0.697; learning rate 0.000351
Epoch 9 iteration 0138/0187: training loss 0.698; learning rate 0.000351
Epoch 9 iteration 0139/0187: training loss 0.698; learning rate 0.000351
Epoch 9 iteration 0140/0187: training loss 0.698; learning rate 0.000351
Epoch 9 iteration 0141/0187: training loss 0.698; learning rate 0.000351
Epoch 9 iteration 0142/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0143/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0144/0187: training loss 0.700; learning rate 0.000350
Epoch 9 iteration 0145/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0146/0187: training loss 0.700; learning rate 0.000350
Epoch 9 iteration 0147/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0148/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0149/0187: training loss 0.700; learning rate 0.000350
Epoch 9 iteration 0150/0187: training loss 0.700; learning rate 0.000350
Epoch 9 iteration 0151/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0152/0187: training loss 0.699; learning rate 0.000350
Epoch 9 iteration 0153/0187: training loss 0.698; learning rate 0.000350
Epoch 9 iteration 0154/0187: training loss 0.698; learning rate 0.000349
Epoch 9 iteration 0155/0187: training loss 0.698; learning rate 0.000349
Epoch 9 iteration 0156/0187: training loss 0.697; learning rate 0.000349
Epoch 9 iteration 0157/0187: training loss 0.696; learning rate 0.000349
Epoch 9 iteration 0158/0187: training loss 0.696; learning rate 0.000349
Epoch 9 iteration 0159/0187: training loss 0.697; learning rate 0.000349
Epoch 9 iteration 0160/0187: training loss 0.697; learning rate 0.000349
Epoch 9 iteration 0161/0187: training loss 0.697; learning rate 0.000349
Epoch 9 iteration 0162/0187: training loss 0.698; learning rate 0.000349
Epoch 9 iteration 0163/0187: training loss 0.698; learning rate 0.000349
Epoch 9 iteration 0164/0187: training loss 0.697; learning rate 0.000349
Epoch 9 iteration 0165/0187: training loss 0.696; learning rate 0.000349
Epoch 9 iteration 0166/0187: training loss 0.696; learning rate 0.000348
Epoch 9 iteration 0167/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0168/0187: training loss 0.699; learning rate 0.000348
Epoch 9 iteration 0169/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0170/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0171/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0172/0187: training loss 0.700; learning rate 0.000348
Epoch 9 iteration 0173/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0174/0187: training loss 0.699; learning rate 0.000348
Epoch 9 iteration 0175/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0176/0187: training loss 0.697; learning rate 0.000348
Epoch 9 iteration 0177/0187: training loss 0.698; learning rate 0.000348
Epoch 9 iteration 0178/0187: training loss 0.698; learning rate 0.000347
Epoch 9 iteration 0179/0187: training loss 0.697; learning rate 0.000347
Epoch 9 iteration 0180/0187: training loss 0.696; learning rate 0.000347
Epoch 9 iteration 0181/0187: training loss 0.697; learning rate 0.000347
Epoch 9 iteration 0182/0187: training loss 0.697; learning rate 0.000347
Epoch 9 iteration 0183/0187: training loss 0.697; learning rate 0.000347
Epoch 9 iteration 0184/0187: training loss 0.697; learning rate 0.000347
Epoch 9 iteration 0185/0187: training loss 0.696; learning rate 0.000347
Epoch 9 iteration 0186/0187: training loss 0.696; learning rate 0.000347
Epoch 9 iteration 0187/0187: training loss 0.695; learning rate 0.000347
Epoch 9 validation pixAcc: 0.340, mIoU: 0.170
Epoch 10 iteration 0001/0187: training loss 0.743; learning rate 0.000347
Epoch 10 iteration 0002/0187: training loss 0.700; learning rate 0.000346
Epoch 10 iteration 0003/0187: training loss 0.656; learning rate 0.000346
Epoch 10 iteration 0004/0187: training loss 0.622; learning rate 0.000346
Epoch 10 iteration 0005/0187: training loss 0.662; learning rate 0.000346
Epoch 10 iteration 0006/0187: training loss 0.679; learning rate 0.000346
Epoch 10 iteration 0007/0187: training loss 0.672; learning rate 0.000346
Epoch 10 iteration 0008/0187: training loss 0.664; learning rate 0.000346
Epoch 10 iteration 0009/0187: training loss 0.659; learning rate 0.000346
Epoch 10 iteration 0010/0187: training loss 0.656; learning rate 0.000346
Epoch 10 iteration 0011/0187: training loss 0.645; learning rate 0.000346
Epoch 10 iteration 0012/0187: training loss 0.639; learning rate 0.000346
Epoch 10 iteration 0013/0187: training loss 0.647; learning rate 0.000346
Epoch 10 iteration 0014/0187: training loss 0.671; learning rate 0.000345
Epoch 10 iteration 0015/0187: training loss 0.662; learning rate 0.000345
Epoch 10 iteration 0016/0187: training loss 0.663; learning rate 0.000345
Epoch 10 iteration 0017/0187: training loss 0.663; learning rate 0.000345
Epoch 10 iteration 0018/0187: training loss 0.666; learning rate 0.000345
Epoch 10 iteration 0019/0187: training loss 0.672; learning rate 0.000345
Epoch 10 iteration 0020/0187: training loss 0.667; learning rate 0.000345
Epoch 10 iteration 0021/0187: training loss 0.685; learning rate 0.000345
Epoch 10 iteration 0022/0187: training loss 0.675; learning rate 0.000345
Epoch 10 iteration 0023/0187: training loss 0.686; learning rate 0.000345
Epoch 10 iteration 0024/0187: training loss 0.694; learning rate 0.000345
Epoch 10 iteration 0025/0187: training loss 0.696; learning rate 0.000345
Epoch 10 iteration 0026/0187: training loss 0.691; learning rate 0.000344
Epoch 10 iteration 0027/0187: training loss 0.686; learning rate 0.000344
Epoch 10 iteration 0028/0187: training loss 0.694; learning rate 0.000344
Epoch 10 iteration 0029/0187: training loss 0.691; learning rate 0.000344
Epoch 10 iteration 0030/0187: training loss 0.692; learning rate 0.000344
Epoch 10 iteration 0031/0187: training loss 0.689; learning rate 0.000344
Epoch 10 iteration 0032/0187: training loss 0.698; learning rate 0.000344
Epoch 10 iteration 0033/0187: training loss 0.692; learning rate 0.000344
Epoch 10 iteration 0034/0187: training loss 0.702; learning rate 0.000344
Epoch 10 iteration 0035/0187: training loss 0.701; learning rate 0.000344
Epoch 10 iteration 0036/0187: training loss 0.700; learning rate 0.000344
Epoch 10 iteration 0037/0187: training loss 0.700; learning rate 0.000344
Epoch 10 iteration 0038/0187: training loss 0.702; learning rate 0.000343
Epoch 10 iteration 0039/0187: training loss 0.700; learning rate 0.000343
Epoch 10 iteration 0040/0187: training loss 0.694; learning rate 0.000343
Epoch 10 iteration 0041/0187: training loss 0.692; learning rate 0.000343
Epoch 10 iteration 0042/0187: training loss 0.693; learning rate 0.000343
Epoch 10 iteration 0043/0187: training loss 0.698; learning rate 0.000343
Epoch 10 iteration 0044/0187: training loss 0.701; learning rate 0.000343
Epoch 10 iteration 0045/0187: training loss 0.699; learning rate 0.000343
Epoch 10 iteration 0046/0187: training loss 0.698; learning rate 0.000343
Epoch 10 iteration 0047/0187: training loss 0.695; learning rate 0.000343
Epoch 10 iteration 0048/0187: training loss 0.693; learning rate 0.000343
Epoch 10 iteration 0049/0187: training loss 0.691; learning rate 0.000343
Epoch 10 iteration 0050/0187: training loss 0.691; learning rate 0.000342
Epoch 10 iteration 0051/0187: training loss 0.687; learning rate 0.000342
Epoch 10 iteration 0052/0187: training loss 0.685; learning rate 0.000342
Epoch 10 iteration 0053/0187: training loss 0.683; learning rate 0.000342
Epoch 10 iteration 0054/0187: training loss 0.685; learning rate 0.000342
Epoch 10 iteration 0055/0187: training loss 0.686; learning rate 0.000342
Epoch 10 iteration 0056/0187: training loss 0.682; learning rate 0.000342
Epoch 10 iteration 0057/0187: training loss 0.681; learning rate 0.000342
Epoch 10 iteration 0058/0187: training loss 0.679; learning rate 0.000342
Epoch 10 iteration 0059/0187: training loss 0.679; learning rate 0.000342
Epoch 10 iteration 0060/0187: training loss 0.674; learning rate 0.000342
Epoch 10 iteration 0061/0187: training loss 0.676; learning rate 0.000341
Epoch 10 iteration 0062/0187: training loss 0.678; learning rate 0.000341
Epoch 10 iteration 0063/0187: training loss 0.677; learning rate 0.000341
Epoch 10 iteration 0064/0187: training loss 0.679; learning rate 0.000341
Epoch 10 iteration 0065/0187: training loss 0.675; learning rate 0.000341
Epoch 10 iteration 0066/0187: training loss 0.675; learning rate 0.000341
Epoch 10 iteration 0067/0187: training loss 0.674; learning rate 0.000341
Epoch 10 iteration 0068/0187: training loss 0.675; learning rate 0.000341
Epoch 10 iteration 0069/0187: training loss 0.678; learning rate 0.000341
Epoch 10 iteration 0070/0187: training loss 0.677; learning rate 0.000341
Epoch 10 iteration 0071/0187: training loss 0.680; learning rate 0.000341
Epoch 10 iteration 0072/0187: training loss 0.680; learning rate 0.000341
Epoch 10 iteration 0073/0187: training loss 0.681; learning rate 0.000340
Epoch 10 iteration 0074/0187: training loss 0.680; learning rate 0.000340
Epoch 10 iteration 0075/0187: training loss 0.681; learning rate 0.000340
Epoch 10 iteration 0076/0187: training loss 0.684; learning rate 0.000340
Epoch 10 iteration 0077/0187: training loss 0.684; learning rate 0.000340
Epoch 10 iteration 0078/0187: training loss 0.684; learning rate 0.000340
Epoch 10 iteration 0079/0187: training loss 0.683; learning rate 0.000340
Epoch 10 iteration 0080/0187: training loss 0.684; learning rate 0.000340
Epoch 10 iteration 0081/0187: training loss 0.685; learning rate 0.000340
Epoch 10 iteration 0082/0187: training loss 0.683; learning rate 0.000340
Epoch 10 iteration 0083/0187: training loss 0.682; learning rate 0.000340
Epoch 10 iteration 0084/0187: training loss 0.682; learning rate 0.000340
Epoch 10 iteration 0085/0187: training loss 0.685; learning rate 0.000339
Epoch 10 iteration 0086/0187: training loss 0.687; learning rate 0.000339
Epoch 10 iteration 0087/0187: training loss 0.685; learning rate 0.000339
Epoch 10 iteration 0088/0187: training loss 0.689; learning rate 0.000339
Epoch 10 iteration 0089/0187: training loss 0.689; learning rate 0.000339
Epoch 10 iteration 0090/0187: training loss 0.690; learning rate 0.000339
Epoch 10 iteration 0091/0188: training loss 0.691; learning rate 0.000339
Epoch 10 iteration 0092/0188: training loss 0.692; learning rate 0.000339
Epoch 10 iteration 0093/0188: training loss 0.691; learning rate 0.000339
Epoch 10 iteration 0094/0188: training loss 0.691; learning rate 0.000339
Epoch 10 iteration 0095/0188: training loss 0.689; learning rate 0.000339
Epoch 10 iteration 0096/0188: training loss 0.690; learning rate 0.000339
Epoch 10 iteration 0097/0188: training loss 0.692; learning rate 0.000338
Epoch 10 iteration 0098/0188: training loss 0.690; learning rate 0.000338
Epoch 10 iteration 0099/0188: training loss 0.692; learning rate 0.000338
Epoch 10 iteration 0100/0188: training loss 0.694; learning rate 0.000338
Epoch 10 iteration 0101/0188: training loss 0.693; learning rate 0.000338
Epoch 10 iteration 0102/0188: training loss 0.691; learning rate 0.000338
Epoch 10 iteration 0103/0188: training loss 0.692; learning rate 0.000338
Epoch 10 iteration 0104/0188: training loss 0.691; learning rate 0.000338
Epoch 10 iteration 0105/0188: training loss 0.691; learning rate 0.000338
Epoch 10 iteration 0106/0188: training loss 0.690; learning rate 0.000338
Epoch 10 iteration 0107/0188: training loss 0.688; learning rate 0.000338
Epoch 10 iteration 0108/0188: training loss 0.689; learning rate 0.000338
Epoch 10 iteration 0109/0188: training loss 0.687; learning rate 0.000337
Epoch 10 iteration 0110/0188: training loss 0.688; learning rate 0.000337
Epoch 10 iteration 0111/0188: training loss 0.688; learning rate 0.000337
Epoch 10 iteration 0112/0188: training loss 0.689; learning rate 0.000337
Epoch 10 iteration 0113/0188: training loss 0.690; learning rate 0.000337
Epoch 10 iteration 0114/0188: training loss 0.688; learning rate 0.000337
Epoch 10 iteration 0115/0188: training loss 0.688; learning rate 0.000337
Epoch 10 iteration 0116/0188: training loss 0.689; learning rate 0.000337
Epoch 10 iteration 0117/0188: training loss 0.689; learning rate 0.000337
Epoch 10 iteration 0118/0188: training loss 0.687; learning rate 0.000337
Epoch 10 iteration 0119/0188: training loss 0.687; learning rate 0.000337
Epoch 10 iteration 0120/0188: training loss 0.685; learning rate 0.000337
Epoch 10 iteration 0121/0188: training loss 0.684; learning rate 0.000336
Epoch 10 iteration 0122/0188: training loss 0.683; learning rate 0.000336
Epoch 10 iteration 0123/0188: training loss 0.684; learning rate 0.000336
Epoch 10 iteration 0124/0188: training loss 0.682; learning rate 0.000336
Epoch 10 iteration 0125/0188: training loss 0.684; learning rate 0.000336
Epoch 10 iteration 0126/0188: training loss 0.683; learning rate 0.000336
Epoch 10 iteration 0127/0188: training loss 0.683; learning rate 0.000336
Epoch 10 iteration 0128/0188: training loss 0.681; learning rate 0.000336
Epoch 10 iteration 0129/0188: training loss 0.682; learning rate 0.000336
Epoch 10 iteration 0130/0188: training loss 0.683; learning rate 0.000336
Epoch 10 iteration 0131/0188: training loss 0.685; learning rate 0.000336
Epoch 10 iteration 0132/0188: training loss 0.685; learning rate 0.000336
Epoch 10 iteration 0133/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0134/0188: training loss 0.686; learning rate 0.000335
Epoch 10 iteration 0135/0188: training loss 0.686; learning rate 0.000335
Epoch 10 iteration 0136/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0137/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0138/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0139/0188: training loss 0.686; learning rate 0.000335
Epoch 10 iteration 0140/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0141/0188: training loss 0.686; learning rate 0.000335
Epoch 10 iteration 0142/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0143/0188: training loss 0.685; learning rate 0.000335
Epoch 10 iteration 0144/0188: training loss 0.686; learning rate 0.000335
Epoch 10 iteration 0145/0188: training loss 0.685; learning rate 0.000334
Epoch 10 iteration 0146/0188: training loss 0.684; learning rate 0.000334
Epoch 10 iteration 0147/0188: training loss 0.683; learning rate 0.000334
Epoch 10 iteration 0148/0188: training loss 0.683; learning rate 0.000334
Epoch 10 iteration 0149/0188: training loss 0.682; learning rate 0.000334
Epoch 10 iteration 0150/0188: training loss 0.682; learning rate 0.000334
Epoch 10 iteration 0151/0188: training loss 0.680; learning rate 0.000334
Epoch 10 iteration 0152/0188: training loss 0.680; learning rate 0.000334
Epoch 10 iteration 0153/0188: training loss 0.679; learning rate 0.000334
Epoch 10 iteration 0154/0188: training loss 0.678; learning rate 0.000334
Epoch 10 iteration 0155/0188: training loss 0.678; learning rate 0.000334
Epoch 10 iteration 0156/0188: training loss 0.676; learning rate 0.000334
Epoch 10 iteration 0157/0188: training loss 0.677; learning rate 0.000333
Epoch 10 iteration 0158/0188: training loss 0.678; learning rate 0.000333
Epoch 10 iteration 0159/0188: training loss 0.677; learning rate 0.000333
Epoch 10 iteration 0160/0188: training loss 0.678; learning rate 0.000333
Epoch 10 iteration 0161/0188: training loss 0.680; learning rate 0.000333
Epoch 10 iteration 0162/0188: training loss 0.680; learning rate 0.000333
Epoch 10 iteration 0163/0188: training loss 0.681; learning rate 0.000333
Epoch 10 iteration 0164/0188: training loss 0.682; learning rate 0.000333
Epoch 10 iteration 0165/0188: training loss 0.681; learning rate 0.000333
Epoch 10 iteration 0166/0188: training loss 0.681; learning rate 0.000333
Epoch 10 iteration 0167/0188: training loss 0.681; learning rate 0.000333
Epoch 10 iteration 0168/0188: training loss 0.681; learning rate 0.000333
Epoch 10 iteration 0169/0188: training loss 0.683; learning rate 0.000332
Epoch 10 iteration 0170/0188: training loss 0.683; learning rate 0.000332
Epoch 10 iteration 0171/0188: training loss 0.683; learning rate 0.000332
Epoch 10 iteration 0172/0188: training loss 0.684; learning rate 0.000332
Epoch 10 iteration 0173/0188: training loss 0.685; learning rate 0.000332
Epoch 10 iteration 0174/0188: training loss 0.685; learning rate 0.000332
Epoch 10 iteration 0175/0188: training loss 0.684; learning rate 0.000332
Epoch 10 iteration 0176/0188: training loss 0.685; learning rate 0.000332
Epoch 10 iteration 0177/0188: training loss 0.685; learning rate 0.000332
Epoch 10 iteration 0178/0188: training loss 0.685; learning rate 0.000332
Epoch 10 iteration 0179/0188: training loss 0.686; learning rate 0.000332
Epoch 10 iteration 0180/0188: training loss 0.686; learning rate 0.000332
Epoch 10 iteration 0181/0188: training loss 0.686; learning rate 0.000331
Epoch 10 iteration 0182/0188: training loss 0.687; learning rate 0.000331
Epoch 10 iteration 0183/0188: training loss 0.688; learning rate 0.000331
Epoch 10 iteration 0184/0188: training loss 0.687; learning rate 0.000331
Epoch 10 iteration 0185/0188: training loss 0.688; learning rate 0.000331
Epoch 10 iteration 0186/0188: training loss 0.688; learning rate 0.000331
Epoch 10 validation pixAcc: 0.339, mIoU: 0.160
Epoch 11 iteration 0001/0187: training loss 0.560; learning rate 0.000331
Epoch 11 iteration 0002/0187: training loss 0.558; learning rate 0.000331
Epoch 11 iteration 0003/0187: training loss 0.616; learning rate 0.000331
Epoch 11 iteration 0004/0187: training loss 0.612; learning rate 0.000331
Epoch 11 iteration 0005/0187: training loss 0.605; learning rate 0.000331
Epoch 11 iteration 0006/0187: training loss 0.624; learning rate 0.000330
Epoch 11 iteration 0007/0187: training loss 0.646; learning rate 0.000330
Epoch 11 iteration 0008/0187: training loss 0.662; learning rate 0.000330
Epoch 11 iteration 0009/0187: training loss 0.650; learning rate 0.000330
Epoch 11 iteration 0010/0187: training loss 0.644; learning rate 0.000330
Epoch 11 iteration 0011/0187: training loss 0.656; learning rate 0.000330
Epoch 11 iteration 0012/0187: training loss 0.664; learning rate 0.000330
Epoch 11 iteration 0013/0187: training loss 0.680; learning rate 0.000330
Epoch 11 iteration 0014/0187: training loss 0.676; learning rate 0.000330
Epoch 11 iteration 0015/0187: training loss 0.683; learning rate 0.000330
Epoch 11 iteration 0016/0187: training loss 0.681; learning rate 0.000330
Epoch 11 iteration 0017/0187: training loss 0.688; learning rate 0.000330
Epoch 11 iteration 0018/0187: training loss 0.681; learning rate 0.000329
Epoch 11 iteration 0019/0187: training loss 0.678; learning rate 0.000329
Epoch 11 iteration 0020/0187: training loss 0.683; learning rate 0.000329
Epoch 11 iteration 0021/0187: training loss 0.686; learning rate 0.000329
Epoch 11 iteration 0022/0187: training loss 0.678; learning rate 0.000329
Epoch 11 iteration 0023/0187: training loss 0.673; learning rate 0.000329
Epoch 11 iteration 0024/0187: training loss 0.684; learning rate 0.000329
Epoch 11 iteration 0025/0187: training loss 0.682; learning rate 0.000329
Epoch 11 iteration 0026/0187: training loss 0.676; learning rate 0.000329
Epoch 11 iteration 0027/0187: training loss 0.671; learning rate 0.000329
Epoch 11 iteration 0028/0187: training loss 0.670; learning rate 0.000329
Epoch 11 iteration 0029/0187: training loss 0.675; learning rate 0.000328
Epoch 11 iteration 0030/0187: training loss 0.673; learning rate 0.000328
Epoch 11 iteration 0031/0187: training loss 0.677; learning rate 0.000328
Epoch 11 iteration 0032/0187: training loss 0.677; learning rate 0.000328
Epoch 11 iteration 0033/0187: training loss 0.678; learning rate 0.000328
Epoch 11 iteration 0034/0187: training loss 0.677; learning rate 0.000328
Epoch 11 iteration 0035/0187: training loss 0.679; learning rate 0.000328
Epoch 11 iteration 0036/0187: training loss 0.677; learning rate 0.000328
Epoch 11 iteration 0037/0187: training loss 0.680; learning rate 0.000328
Epoch 11 iteration 0038/0187: training loss 0.683; learning rate 0.000328
Epoch 11 iteration 0039/0187: training loss 0.683; learning rate 0.000328
Epoch 11 iteration 0040/0187: training loss 0.685; learning rate 0.000328
Epoch 11 iteration 0041/0187: training loss 0.686; learning rate 0.000327
Epoch 11 iteration 0042/0187: training loss 0.691; learning rate 0.000327
Epoch 11 iteration 0043/0187: training loss 0.693; learning rate 0.000327
Epoch 11 iteration 0044/0187: training loss 0.689; learning rate 0.000327
Epoch 11 iteration 0045/0187: training loss 0.684; learning rate 0.000327
Epoch 11 iteration 0046/0187: training loss 0.680; learning rate 0.000327
Epoch 11 iteration 0047/0187: training loss 0.680; learning rate 0.000327
Epoch 11 iteration 0048/0187: training loss 0.684; learning rate 0.000327
Epoch 11 iteration 0049/0187: training loss 0.686; learning rate 0.000327
Epoch 11 iteration 0050/0187: training loss 0.689; learning rate 0.000327
Epoch 11 iteration 0051/0187: training loss 0.692; learning rate 0.000327
Epoch 11 iteration 0052/0187: training loss 0.689; learning rate 0.000327
Epoch 11 iteration 0053/0187: training loss 0.691; learning rate 0.000326
Epoch 11 iteration 0054/0187: training loss 0.697; learning rate 0.000326
Epoch 11 iteration 0055/0187: training loss 0.697; learning rate 0.000326
Epoch 11 iteration 0056/0187: training loss 0.697; learning rate 0.000326
Epoch 11 iteration 0057/0187: training loss 0.694; learning rate 0.000326
Epoch 11 iteration 0058/0187: training loss 0.698; learning rate 0.000326
Epoch 11 iteration 0059/0187: training loss 0.695; learning rate 0.000326
Epoch 11 iteration 0060/0187: training loss 0.693; learning rate 0.000326
Epoch 11 iteration 0061/0187: training loss 0.694; learning rate 0.000326
Epoch 11 iteration 0062/0187: training loss 0.691; learning rate 0.000326
Epoch 11 iteration 0063/0187: training loss 0.688; learning rate 0.000326
Epoch 11 iteration 0064/0187: training loss 0.686; learning rate 0.000326
Epoch 11 iteration 0065/0187: training loss 0.685; learning rate 0.000325
Epoch 11 iteration 0066/0187: training loss 0.684; learning rate 0.000325
Epoch 11 iteration 0067/0187: training loss 0.684; learning rate 0.000325
Epoch 11 iteration 0068/0187: training loss 0.686; learning rate 0.000325
Epoch 11 iteration 0069/0187: training loss 0.684; learning rate 0.000325
Epoch 11 iteration 0070/0187: training loss 0.683; learning rate 0.000325
Epoch 11 iteration 0071/0187: training loss 0.683; learning rate 0.000325
Epoch 11 iteration 0072/0187: training loss 0.682; learning rate 0.000325
Epoch 11 iteration 0073/0187: training loss 0.682; learning rate 0.000325
Epoch 11 iteration 0074/0187: training loss 0.685; learning rate 0.000325
Epoch 11 iteration 0075/0187: training loss 0.687; learning rate 0.000325
Epoch 11 iteration 0076/0187: training loss 0.687; learning rate 0.000325
Epoch 11 iteration 0077/0187: training loss 0.690; learning rate 0.000324
Epoch 11 iteration 0078/0187: training loss 0.690; learning rate 0.000324
Epoch 11 iteration 0079/0187: training loss 0.689; learning rate 0.000324
Epoch 11 iteration 0080/0187: training loss 0.690; learning rate 0.000324
Epoch 11 iteration 0081/0187: training loss 0.692; learning rate 0.000324
Epoch 11 iteration 0082/0187: training loss 0.693; learning rate 0.000324
Epoch 11 iteration 0083/0187: training loss 0.693; learning rate 0.000324
Epoch 11 iteration 0084/0187: training loss 0.694; learning rate 0.000324
Epoch 11 iteration 0085/0187: training loss 0.695; learning rate 0.000324
Epoch 11 iteration 0086/0187: training loss 0.694; learning rate 0.000324
Epoch 11 iteration 0087/0187: training loss 0.692; learning rate 0.000324
Epoch 11 iteration 0088/0187: training loss 0.691; learning rate 0.000324
Epoch 11 iteration 0089/0187: training loss 0.688; learning rate 0.000323
Epoch 11 iteration 0090/0187: training loss 0.689; learning rate 0.000323
Epoch 11 iteration 0091/0187: training loss 0.691; learning rate 0.000323
Epoch 11 iteration 0092/0187: training loss 0.691; learning rate 0.000323
Epoch 11 iteration 0093/0187: training loss 0.692; learning rate 0.000323
Epoch 11 iteration 0094/0187: training loss 0.693; learning rate 0.000323
Epoch 11 iteration 0095/0187: training loss 0.691; learning rate 0.000323
Epoch 11 iteration 0096/0187: training loss 0.694; learning rate 0.000323
Epoch 11 iteration 0097/0187: training loss 0.693; learning rate 0.000323
Epoch 11 iteration 0098/0187: training loss 0.693; learning rate 0.000323
Epoch 11 iteration 0099/0187: training loss 0.693; learning rate 0.000323
Epoch 11 iteration 0100/0187: training loss 0.691; learning rate 0.000323
Epoch 11 iteration 0101/0187: training loss 0.691; learning rate 0.000322
Epoch 11 iteration 0102/0187: training loss 0.691; learning rate 0.000322
Epoch 11 iteration 0103/0187: training loss 0.689; learning rate 0.000322
Epoch 11 iteration 0104/0187: training loss 0.688; learning rate 0.000322
Epoch 11 iteration 0105/0187: training loss 0.688; learning rate 0.000322
Epoch 11 iteration 0106/0187: training loss 0.687; learning rate 0.000322
Epoch 11 iteration 0107/0187: training loss 0.690; learning rate 0.000322
Epoch 11 iteration 0108/0187: training loss 0.689; learning rate 0.000322
Epoch 11 iteration 0109/0187: training loss 0.687; learning rate 0.000322
Epoch 11 iteration 0110/0187: training loss 0.687; learning rate 0.000322
Epoch 11 iteration 0111/0187: training loss 0.691; learning rate 0.000322
Epoch 11 iteration 0112/0187: training loss 0.693; learning rate 0.000322
Epoch 11 iteration 0113/0187: training loss 0.692; learning rate 0.000321
Epoch 11 iteration 0114/0187: training loss 0.694; learning rate 0.000321
Epoch 11 iteration 0115/0187: training loss 0.693; learning rate 0.000321
Epoch 11 iteration 0116/0187: training loss 0.693; learning rate 0.000321
Epoch 11 iteration 0117/0187: training loss 0.695; learning rate 0.000321
Epoch 11 iteration 0118/0187: training loss 0.693; learning rate 0.000321
Epoch 11 iteration 0119/0187: training loss 0.692; learning rate 0.000321
Epoch 11 iteration 0120/0187: training loss 0.693; learning rate 0.000321
Epoch 11 iteration 0121/0187: training loss 0.692; learning rate 0.000321
Epoch 11 iteration 0122/0187: training loss 0.692; learning rate 0.000321
Epoch 11 iteration 0123/0187: training loss 0.691; learning rate 0.000321
Epoch 11 iteration 0124/0187: training loss 0.690; learning rate 0.000320
Epoch 11 iteration 0125/0187: training loss 0.692; learning rate 0.000320
Epoch 11 iteration 0126/0187: training loss 0.693; learning rate 0.000320
Epoch 11 iteration 0127/0187: training loss 0.693; learning rate 0.000320
Epoch 11 iteration 0128/0187: training loss 0.692; learning rate 0.000320
Epoch 11 iteration 0129/0187: training loss 0.694; learning rate 0.000320
Epoch 11 iteration 0130/0187: training loss 0.694; learning rate 0.000320
Epoch 11 iteration 0131/0187: training loss 0.693; learning rate 0.000320
Epoch 11 iteration 0132/0187: training loss 0.694; learning rate 0.000320
Epoch 11 iteration 0133/0187: training loss 0.695; learning rate 0.000320
Epoch 11 iteration 0134/0187: training loss 0.696; learning rate 0.000320
Epoch 11 iteration 0135/0187: training loss 0.696; learning rate 0.000320
Epoch 11 iteration 0136/0187: training loss 0.696; learning rate 0.000319
Epoch 11 iteration 0137/0187: training loss 0.694; learning rate 0.000319
Epoch 11 iteration 0138/0187: training loss 0.694; learning rate 0.000319
Epoch 11 iteration 0139/0187: training loss 0.693; learning rate 0.000319
Epoch 11 iteration 0140/0187: training loss 0.694; learning rate 0.000319
Epoch 11 iteration 0141/0187: training loss 0.695; learning rate 0.000319
Epoch 11 iteration 0142/0187: training loss 0.694; learning rate 0.000319
Epoch 11 iteration 0143/0187: training loss 0.696; learning rate 0.000319
Epoch 11 iteration 0144/0187: training loss 0.695; learning rate 0.000319
Epoch 11 iteration 0145/0187: training loss 0.695; learning rate 0.000319
Epoch 11 iteration 0146/0187: training loss 0.695; learning rate 0.000319
Epoch 11 iteration 0147/0187: training loss 0.696; learning rate 0.000319
Epoch 11 iteration 0148/0187: training loss 0.696; learning rate 0.000318
Epoch 11 iteration 0149/0187: training loss 0.696; learning rate 0.000318
Epoch 11 iteration 0150/0187: training loss 0.696; learning rate 0.000318
Epoch 11 iteration 0151/0187: training loss 0.698; learning rate 0.000318
Epoch 11 iteration 0152/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0153/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0154/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0155/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0156/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0157/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0158/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0159/0187: training loss 0.699; learning rate 0.000318
Epoch 11 iteration 0160/0187: training loss 0.698; learning rate 0.000317
Epoch 11 iteration 0161/0187: training loss 0.698; learning rate 0.000317
Epoch 11 iteration 0162/0187: training loss 0.699; learning rate 0.000317
Epoch 11 iteration 0163/0187: training loss 0.699; learning rate 0.000317
Epoch 11 iteration 0164/0187: training loss 0.699; learning rate 0.000317
Epoch 11 iteration 0165/0187: training loss 0.697; learning rate 0.000317
Epoch 11 iteration 0166/0187: training loss 0.697; learning rate 0.000317
Epoch 11 iteration 0167/0187: training loss 0.696; learning rate 0.000317
Epoch 11 iteration 0168/0187: training loss 0.696; learning rate 0.000317
Epoch 11 iteration 0169/0187: training loss 0.696; learning rate 0.000317
Epoch 11 iteration 0170/0187: training loss 0.696; learning rate 0.000317
Epoch 11 iteration 0171/0187: training loss 0.696; learning rate 0.000317
Epoch 11 iteration 0172/0187: training loss 0.696; learning rate 0.000316
Epoch 11 iteration 0173/0187: training loss 0.697; learning rate 0.000316
Epoch 11 iteration 0174/0187: training loss 0.695; learning rate 0.000316
Epoch 11 iteration 0175/0187: training loss 0.695; learning rate 0.000316
Epoch 11 iteration 0176/0187: training loss 0.695; learning rate 0.000316
Epoch 11 iteration 0177/0187: training loss 0.696; learning rate 0.000316
Epoch 11 iteration 0178/0187: training loss 0.696; learning rate 0.000316
Epoch 11 iteration 0179/0187: training loss 0.696; learning rate 0.000316
Epoch 11 iteration 0180/0187: training loss 0.695; learning rate 0.000316
Epoch 11 iteration 0181/0187: training loss 0.694; learning rate 0.000316
Epoch 11 iteration 0182/0187: training loss 0.694; learning rate 0.000316
Epoch 11 iteration 0183/0187: training loss 0.695; learning rate 0.000316
Epoch 11 iteration 0184/0187: training loss 0.695; learning rate 0.000315
Epoch 11 iteration 0185/0187: training loss 0.694; learning rate 0.000315
Epoch 11 iteration 0186/0187: training loss 0.694; learning rate 0.000315
Epoch 11 iteration 0187/0187: training loss 0.695; learning rate 0.000315
Epoch 11 validation pixAcc: 0.341, mIoU: 0.164
Epoch 12 iteration 0001/0187: training loss 0.757; learning rate 0.000315
Epoch 12 iteration 0002/0187: training loss 0.790; learning rate 0.000315
Epoch 12 iteration 0003/0187: training loss 0.845; learning rate 0.000315
Epoch 12 iteration 0004/0187: training loss 0.818; learning rate 0.000315
Epoch 12 iteration 0005/0187: training loss 0.832; learning rate 0.000315
Epoch 12 iteration 0006/0187: training loss 0.830; learning rate 0.000315
Epoch 12 iteration 0007/0187: training loss 0.803; learning rate 0.000315
Epoch 12 iteration 0008/0187: training loss 0.785; learning rate 0.000314
Epoch 12 iteration 0009/0187: training loss 0.766; learning rate 0.000314
Epoch 12 iteration 0010/0187: training loss 0.742; learning rate 0.000314
Epoch 12 iteration 0011/0187: training loss 0.742; learning rate 0.000314
Epoch 12 iteration 0012/0187: training loss 0.727; learning rate 0.000314
Epoch 12 iteration 0013/0187: training loss 0.712; learning rate 0.000314
Epoch 12 iteration 0014/0187: training loss 0.716; learning rate 0.000314
Epoch 12 iteration 0015/0187: training loss 0.709; learning rate 0.000314
Epoch 12 iteration 0016/0187: training loss 0.703; learning rate 0.000314
Epoch 12 iteration 0017/0187: training loss 0.698; learning rate 0.000314
Epoch 12 iteration 0018/0187: training loss 0.690; learning rate 0.000314
Epoch 12 iteration 0019/0187: training loss 0.690; learning rate 0.000313
Epoch 12 iteration 0020/0187: training loss 0.693; learning rate 0.000313
Epoch 12 iteration 0021/0187: training loss 0.688; learning rate 0.000313
Epoch 12 iteration 0022/0187: training loss 0.681; learning rate 0.000313
Epoch 12 iteration 0023/0187: training loss 0.690; learning rate 0.000313
Epoch 12 iteration 0024/0187: training loss 0.688; learning rate 0.000313
Epoch 12 iteration 0025/0187: training loss 0.680; learning rate 0.000313
Epoch 12 iteration 0026/0187: training loss 0.679; learning rate 0.000313
Epoch 12 iteration 0027/0187: training loss 0.669; learning rate 0.000313
Epoch 12 iteration 0028/0187: training loss 0.664; learning rate 0.000313
Epoch 12 iteration 0029/0187: training loss 0.663; learning rate 0.000313
Epoch 12 iteration 0030/0187: training loss 0.661; learning rate 0.000313
Epoch 12 iteration 0031/0187: training loss 0.667; learning rate 0.000312
Epoch 12 iteration 0032/0187: training loss 0.676; learning rate 0.000312
Epoch 12 iteration 0033/0187: training loss 0.669; learning rate 0.000312
Epoch 12 iteration 0034/0187: training loss 0.669; learning rate 0.000312
Epoch 12 iteration 0035/0187: training loss 0.672; learning rate 0.000312
Epoch 12 iteration 0036/0187: training loss 0.689; learning rate 0.000312
Epoch 12 iteration 0037/0187: training loss 0.690; learning rate 0.000312
Epoch 12 iteration 0038/0187: training loss 0.689; learning rate 0.000312
Epoch 12 iteration 0039/0187: training loss 0.693; learning rate 0.000312
Epoch 12 iteration 0040/0187: training loss 0.697; learning rate 0.000312
Epoch 12 iteration 0041/0187: training loss 0.696; learning rate 0.000312
Epoch 12 iteration 0042/0187: training loss 0.697; learning rate 0.000312
Epoch 12 iteration 0043/0187: training loss 0.694; learning rate 0.000311
Epoch 12 iteration 0044/0187: training loss 0.690; learning rate 0.000311
Epoch 12 iteration 0045/0187: training loss 0.685; learning rate 0.000311
Epoch 12 iteration 0046/0187: training loss 0.683; learning rate 0.000311
Epoch 12 iteration 0047/0187: training loss 0.683; learning rate 0.000311
Epoch 12 iteration 0048/0187: training loss 0.688; learning rate 0.000311
Epoch 12 iteration 0049/0187: training loss 0.692; learning rate 0.000311
Epoch 12 iteration 0050/0187: training loss 0.691; learning rate 0.000311
Epoch 12 iteration 0051/0187: training loss 0.689; learning rate 0.000311
Epoch 12 iteration 0052/0187: training loss 0.684; learning rate 0.000311
Epoch 12 iteration 0053/0187: training loss 0.687; learning rate 0.000311
Epoch 12 iteration 0054/0187: training loss 0.686; learning rate 0.000311
Epoch 12 iteration 0055/0187: training loss 0.684; learning rate 0.000310
Epoch 12 iteration 0056/0187: training loss 0.685; learning rate 0.000310
Epoch 12 iteration 0057/0187: training loss 0.692; learning rate 0.000310
Epoch 12 iteration 0058/0187: training loss 0.693; learning rate 0.000310
Epoch 12 iteration 0059/0187: training loss 0.693; learning rate 0.000310
Epoch 12 iteration 0060/0187: training loss 0.692; learning rate 0.000310
Epoch 12 iteration 0061/0187: training loss 0.696; learning rate 0.000310
Epoch 12 iteration 0062/0187: training loss 0.694; learning rate 0.000310
Epoch 12 iteration 0063/0187: training loss 0.691; learning rate 0.000310
Epoch 12 iteration 0064/0187: training loss 0.692; learning rate 0.000310
Epoch 12 iteration 0065/0187: training loss 0.691; learning rate 0.000310
Epoch 12 iteration 0066/0187: training loss 0.691; learning rate 0.000310
Epoch 12 iteration 0067/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0068/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0069/0187: training loss 0.692; learning rate 0.000309
Epoch 12 iteration 0070/0187: training loss 0.695; learning rate 0.000309
Epoch 12 iteration 0071/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0072/0187: training loss 0.695; learning rate 0.000309
Epoch 12 iteration 0073/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0074/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0075/0187: training loss 0.695; learning rate 0.000309
Epoch 12 iteration 0076/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0077/0187: training loss 0.693; learning rate 0.000309
Epoch 12 iteration 0078/0187: training loss 0.694; learning rate 0.000309
Epoch 12 iteration 0079/0187: training loss 0.693; learning rate 0.000308
Epoch 12 iteration 0080/0187: training loss 0.694; learning rate 0.000308
Epoch 12 iteration 0081/0187: training loss 0.692; learning rate 0.000308
Epoch 12 iteration 0082/0187: training loss 0.692; learning rate 0.000308
Epoch 12 iteration 0083/0187: training loss 0.692; learning rate 0.000308
Epoch 12 iteration 0084/0187: training loss 0.694; learning rate 0.000308
Epoch 12 iteration 0085/0187: training loss 0.694; learning rate 0.000308
Epoch 12 iteration 0086/0187: training loss 0.691; learning rate 0.000308
Epoch 12 iteration 0087/0187: training loss 0.692; learning rate 0.000308
Epoch 12 iteration 0088/0187: training loss 0.694; learning rate 0.000308
Epoch 12 iteration 0089/0187: training loss 0.693; learning rate 0.000308
Epoch 12 iteration 0090/0187: training loss 0.694; learning rate 0.000307
Epoch 12 iteration 0091/0188: training loss 0.694; learning rate 0.000307
Epoch 12 iteration 0092/0188: training loss 0.693; learning rate 0.000307
Epoch 12 iteration 0093/0188: training loss 0.693; learning rate 0.000307
Epoch 12 iteration 0094/0188: training loss 0.692; learning rate 0.000307
Epoch 12 iteration 0095/0188: training loss 0.690; learning rate 0.000307
Epoch 12 iteration 0096/0188: training loss 0.690; learning rate 0.000307
Epoch 12 iteration 0097/0188: training loss 0.692; learning rate 0.000307
Epoch 12 iteration 0098/0188: training loss 0.690; learning rate 0.000307
Epoch 12 iteration 0099/0188: training loss 0.689; learning rate 0.000307
Epoch 12 iteration 0100/0188: training loss 0.688; learning rate 0.000307
Epoch 12 iteration 0101/0188: training loss 0.686; learning rate 0.000307
Epoch 12 iteration 0102/0188: training loss 0.685; learning rate 0.000306
Epoch 12 iteration 0103/0188: training loss 0.685; learning rate 0.000306
Epoch 12 iteration 0104/0188: training loss 0.684; learning rate 0.000306
Epoch 12 iteration 0105/0188: training loss 0.684; learning rate 0.000306
Epoch 12 iteration 0106/0188: training loss 0.684; learning rate 0.000306
Epoch 12 iteration 0107/0188: training loss 0.683; learning rate 0.000306
Epoch 12 iteration 0108/0188: training loss 0.682; learning rate 0.000306
Epoch 12 iteration 0109/0188: training loss 0.682; learning rate 0.000306
Epoch 12 iteration 0110/0188: training loss 0.681; learning rate 0.000306
Epoch 12 iteration 0111/0188: training loss 0.682; learning rate 0.000306
Epoch 12 iteration 0112/0188: training loss 0.683; learning rate 0.000306
Epoch 12 iteration 0113/0188: training loss 0.681; learning rate 0.000306
Epoch 12 iteration 0114/0188: training loss 0.682; learning rate 0.000305
Epoch 12 iteration 0115/0188: training loss 0.681; learning rate 0.000305
Epoch 12 iteration 0116/0188: training loss 0.681; learning rate 0.000305
Epoch 12 iteration 0117/0188: training loss 0.681; learning rate 0.000305
Epoch 12 iteration 0118/0188: training loss 0.679; learning rate 0.000305
Epoch 12 iteration 0119/0188: training loss 0.679; learning rate 0.000305
Epoch 12 iteration 0120/0188: training loss 0.677; learning rate 0.000305
Epoch 12 iteration 0121/0188: training loss 0.676; learning rate 0.000305
Epoch 12 iteration 0122/0188: training loss 0.676; learning rate 0.000305
Epoch 12 iteration 0123/0188: training loss 0.677; learning rate 0.000305
Epoch 12 iteration 0124/0188: training loss 0.678; learning rate 0.000305
Epoch 12 iteration 0125/0188: training loss 0.677; learning rate 0.000305
Epoch 12 iteration 0126/0188: training loss 0.676; learning rate 0.000304
Epoch 12 iteration 0127/0188: training loss 0.675; learning rate 0.000304
Epoch 12 iteration 0128/0188: training loss 0.676; learning rate 0.000304
Epoch 12 iteration 0129/0188: training loss 0.677; learning rate 0.000304
Epoch 12 iteration 0130/0188: training loss 0.675; learning rate 0.000304
Epoch 12 iteration 0131/0188: training loss 0.674; learning rate 0.000304
Epoch 12 iteration 0132/0188: training loss 0.676; learning rate 0.000304
Epoch 12 iteration 0133/0188: training loss 0.676; learning rate 0.000304
Epoch 12 iteration 0134/0188: training loss 0.676; learning rate 0.000304
Epoch 12 iteration 0135/0188: training loss 0.677; learning rate 0.000304
Epoch 12 iteration 0136/0188: training loss 0.677; learning rate 0.000304
Epoch 12 iteration 0137/0188: training loss 0.677; learning rate 0.000304
Epoch 12 iteration 0138/0188: training loss 0.676; learning rate 0.000303
Epoch 12 iteration 0139/0188: training loss 0.676; learning rate 0.000303
Epoch 12 iteration 0140/0188: training loss 0.677; learning rate 0.000303
Epoch 12 iteration 0141/0188: training loss 0.678; learning rate 0.000303
Epoch 12 iteration 0142/0188: training loss 0.679; learning rate 0.000303
Epoch 12 iteration 0143/0188: training loss 0.679; learning rate 0.000303
Epoch 12 iteration 0144/0188: training loss 0.681; learning rate 0.000303
Epoch 12 iteration 0145/0188: training loss 0.681; learning rate 0.000303
Epoch 12 iteration 0146/0188: training loss 0.680; learning rate 0.000303
Epoch 12 iteration 0147/0188: training loss 0.681; learning rate 0.000303
Epoch 12 iteration 0148/0188: training loss 0.682; learning rate 0.000303
Epoch 12 iteration 0149/0188: training loss 0.683; learning rate 0.000302
Epoch 12 iteration 0150/0188: training loss 0.682; learning rate 0.000302
Epoch 12 iteration 0151/0188: training loss 0.684; learning rate 0.000302
Epoch 12 iteration 0152/0188: training loss 0.685; learning rate 0.000302
Epoch 12 iteration 0153/0188: training loss 0.685; learning rate 0.000302
Epoch 12 iteration 0154/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0155/0188: training loss 0.687; learning rate 0.000302
Epoch 12 iteration 0156/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0157/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0158/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0159/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0160/0188: training loss 0.686; learning rate 0.000302
Epoch 12 iteration 0161/0188: training loss 0.686; learning rate 0.000301
Epoch 12 iteration 0162/0188: training loss 0.685; learning rate 0.000301
Epoch 12 iteration 0163/0188: training loss 0.684; learning rate 0.000301
Epoch 12 iteration 0164/0188: training loss 0.683; learning rate 0.000301
Epoch 12 iteration 0165/0188: training loss 0.683; learning rate 0.000301
Epoch 12 iteration 0166/0188: training loss 0.683; learning rate 0.000301
Epoch 12 iteration 0167/0188: training loss 0.683; learning rate 0.000301
Epoch 12 iteration 0168/0188: training loss 0.684; learning rate 0.000301
Epoch 12 iteration 0169/0188: training loss 0.684; learning rate 0.000301
Epoch 12 iteration 0170/0188: training loss 0.684; learning rate 0.000301
Epoch 12 iteration 0171/0188: training loss 0.684; learning rate 0.000301
Epoch 12 iteration 0172/0188: training loss 0.683; learning rate 0.000301
Epoch 12 iteration 0173/0188: training loss 0.684; learning rate 0.000300
Epoch 12 iteration 0174/0188: training loss 0.684; learning rate 0.000300
Epoch 12 iteration 0175/0188: training loss 0.683; learning rate 0.000300
Epoch 12 iteration 0176/0188: training loss 0.683; learning rate 0.000300
Epoch 12 iteration 0177/0188: training loss 0.682; learning rate 0.000300
Epoch 12 iteration 0178/0188: training loss 0.681; learning rate 0.000300
Epoch 12 iteration 0179/0188: training loss 0.681; learning rate 0.000300
Epoch 12 iteration 0180/0188: training loss 0.681; learning rate 0.000300
Epoch 12 iteration 0181/0188: training loss 0.679; learning rate 0.000300
Epoch 12 iteration 0182/0188: training loss 0.679; learning rate 0.000300
Epoch 12 iteration 0183/0188: training loss 0.678; learning rate 0.000300
Epoch 12 iteration 0184/0188: training loss 0.679; learning rate 0.000300
Epoch 12 iteration 0185/0188: training loss 0.678; learning rate 0.000299
Epoch 12 iteration 0186/0188: training loss 0.678; learning rate 0.000299
Epoch 12 validation pixAcc: 0.340, mIoU: 0.171
Epoch 13 iteration 0001/0187: training loss 0.617; learning rate 0.000299
Epoch 13 iteration 0002/0187: training loss 0.625; learning rate 0.000299
Epoch 13 iteration 0003/0187: training loss 0.661; learning rate 0.000299
Epoch 13 iteration 0004/0187: training loss 0.668; learning rate 0.000299
Epoch 13 iteration 0005/0187: training loss 0.644; learning rate 0.000299
Epoch 13 iteration 0006/0187: training loss 0.679; learning rate 0.000299
Epoch 13 iteration 0007/0187: training loss 0.681; learning rate 0.000299
Epoch 13 iteration 0008/0187: training loss 0.674; learning rate 0.000299
Epoch 13 iteration 0009/0187: training loss 0.685; learning rate 0.000298
Epoch 13 iteration 0010/0187: training loss 0.694; learning rate 0.000298
Epoch 13 iteration 0011/0187: training loss 0.703; learning rate 0.000298
Epoch 13 iteration 0012/0187: training loss 0.696; learning rate 0.000298
Epoch 13 iteration 0013/0187: training loss 0.700; learning rate 0.000298
Epoch 13 iteration 0014/0187: training loss 0.697; learning rate 0.000298
Epoch 13 iteration 0015/0187: training loss 0.698; learning rate 0.000298
Epoch 13 iteration 0016/0187: training loss 0.697; learning rate 0.000298
Epoch 13 iteration 0017/0187: training loss 0.691; learning rate 0.000298
Epoch 13 iteration 0018/0187: training loss 0.684; learning rate 0.000298
Epoch 13 iteration 0019/0187: training loss 0.680; learning rate 0.000298
Epoch 13 iteration 0020/0187: training loss 0.684; learning rate 0.000298
Epoch 13 iteration 0021/0187: training loss 0.686; learning rate 0.000297
Epoch 13 iteration 0022/0187: training loss 0.687; learning rate 0.000297
Epoch 13 iteration 0023/0187: training loss 0.691; learning rate 0.000297
Epoch 13 iteration 0024/0187: training loss 0.704; learning rate 0.000297
Epoch 13 iteration 0025/0187: training loss 0.699; learning rate 0.000297
Epoch 13 iteration 0026/0187: training loss 0.698; learning rate 0.000297
Epoch 13 iteration 0027/0187: training loss 0.699; learning rate 0.000297
Epoch 13 iteration 0028/0187: training loss 0.696; learning rate 0.000297
Epoch 13 iteration 0029/0187: training loss 0.692; learning rate 0.000297
Epoch 13 iteration 0030/0187: training loss 0.689; learning rate 0.000297
Epoch 13 iteration 0031/0187: training loss 0.688; learning rate 0.000297
Epoch 13 iteration 0032/0187: training loss 0.696; learning rate 0.000297
Epoch 13 iteration 0033/0187: training loss 0.700; learning rate 0.000296
Epoch 13 iteration 0034/0187: training loss 0.697; learning rate 0.000296
Epoch 13 iteration 0035/0187: training loss 0.693; learning rate 0.000296
Epoch 13 iteration 0036/0187: training loss 0.696; learning rate 0.000296
Epoch 13 iteration 0037/0187: training loss 0.697; learning rate 0.000296
Epoch 13 iteration 0038/0187: training loss 0.696; learning rate 0.000296
Epoch 13 iteration 0039/0187: training loss 0.698; learning rate 0.000296
Epoch 13 iteration 0040/0187: training loss 0.698; learning rate 0.000296
Epoch 13 iteration 0041/0187: training loss 0.703; learning rate 0.000296
Epoch 13 iteration 0042/0187: training loss 0.703; learning rate 0.000296
Epoch 13 iteration 0043/0187: training loss 0.700; learning rate 0.000296
Epoch 13 iteration 0044/0187: training loss 0.698; learning rate 0.000296
Epoch 13 iteration 0045/0187: training loss 0.696; learning rate 0.000295
Epoch 13 iteration 0046/0187: training loss 0.697; learning rate 0.000295
Epoch 13 iteration 0047/0187: training loss 0.702; learning rate 0.000295
Epoch 13 iteration 0048/0187: training loss 0.705; learning rate 0.000295
Epoch 13 iteration 0049/0187: training loss 0.705; learning rate 0.000295
Epoch 13 iteration 0050/0187: training loss 0.706; learning rate 0.000295
Epoch 13 iteration 0051/0187: training loss 0.706; learning rate 0.000295
Epoch 13 iteration 0052/0187: training loss 0.702; learning rate 0.000295
Epoch 13 iteration 0053/0187: training loss 0.706; learning rate 0.000295
Epoch 13 iteration 0054/0187: training loss 0.702; learning rate 0.000295
Epoch 13 iteration 0055/0187: training loss 0.699; learning rate 0.000295
Epoch 13 iteration 0056/0187: training loss 0.696; learning rate 0.000295
Epoch 13 iteration 0057/0187: training loss 0.696; learning rate 0.000294
Epoch 13 iteration 0058/0187: training loss 0.693; learning rate 0.000294
Epoch 13 iteration 0059/0187: training loss 0.695; learning rate 0.000294
Epoch 13 iteration 0060/0187: training loss 0.694; learning rate 0.000294
Epoch 13 iteration 0061/0187: training loss 0.696; learning rate 0.000294
Epoch 13 iteration 0062/0187: training loss 0.696; learning rate 0.000294
Epoch 13 iteration 0063/0187: training loss 0.694; learning rate 0.000294
Epoch 13 iteration 0064/0187: training loss 0.692; learning rate 0.000294
Epoch 13 iteration 0065/0187: training loss 0.697; learning rate 0.000294
Epoch 13 iteration 0066/0187: training loss 0.695; learning rate 0.000294
Epoch 13 iteration 0067/0187: training loss 0.692; learning rate 0.000294
Epoch 13 iteration 0068/0187: training loss 0.691; learning rate 0.000293
Epoch 13 iteration 0069/0187: training loss 0.690; learning rate 0.000293
Epoch 13 iteration 0070/0187: training loss 0.692; learning rate 0.000293
Epoch 13 iteration 0071/0187: training loss 0.689; learning rate 0.000293
Epoch 13 iteration 0072/0187: training loss 0.688; learning rate 0.000293
Epoch 13 iteration 0073/0187: training loss 0.688; learning rate 0.000293
Epoch 13 iteration 0074/0187: training loss 0.684; learning rate 0.000293
Epoch 13 iteration 0075/0187: training loss 0.687; learning rate 0.000293
Epoch 13 iteration 0076/0187: training loss 0.688; learning rate 0.000293
Epoch 13 iteration 0077/0187: training loss 0.687; learning rate 0.000293
Epoch 13 iteration 0078/0187: training loss 0.684; learning rate 0.000293
Epoch 13 iteration 0079/0187: training loss 0.684; learning rate 0.000293
Epoch 13 iteration 0080/0187: training loss 0.683; learning rate 0.000292
Epoch 13 iteration 0081/0187: training loss 0.683; learning rate 0.000292
Epoch 13 iteration 0082/0187: training loss 0.683; learning rate 0.000292
Epoch 13 iteration 0083/0187: training loss 0.686; learning rate 0.000292
Epoch 13 iteration 0084/0187: training loss 0.687; learning rate 0.000292
Epoch 13 iteration 0085/0187: training loss 0.688; learning rate 0.000292
Epoch 13 iteration 0086/0187: training loss 0.686; learning rate 0.000292
Epoch 13 iteration 0087/0187: training loss 0.684; learning rate 0.000292
Epoch 13 iteration 0088/0187: training loss 0.683; learning rate 0.000292
Epoch 13 iteration 0089/0187: training loss 0.682; learning rate 0.000292
Epoch 13 iteration 0090/0187: training loss 0.684; learning rate 0.000292
Epoch 13 iteration 0091/0187: training loss 0.683; learning rate 0.000292
Epoch 13 iteration 0092/0187: training loss 0.683; learning rate 0.000291
Epoch 13 iteration 0093/0187: training loss 0.684; learning rate 0.000291
Epoch 13 iteration 0094/0187: training loss 0.688; learning rate 0.000291
Epoch 13 iteration 0095/0187: training loss 0.687; learning rate 0.000291
Epoch 13 iteration 0096/0187: training loss 0.685; learning rate 0.000291
Epoch 13 iteration 0097/0187: training loss 0.686; learning rate 0.000291
Epoch 13 iteration 0098/0187: training loss 0.686; learning rate 0.000291
Epoch 13 iteration 0099/0187: training loss 0.685; learning rate 0.000291
Epoch 13 iteration 0100/0187: training loss 0.684; learning rate 0.000291
Epoch 13 iteration 0101/0187: training loss 0.685; learning rate 0.000291
Epoch 13 iteration 0102/0187: training loss 0.684; learning rate 0.000291
Epoch 13 iteration 0103/0187: training loss 0.685; learning rate 0.000290
Epoch 13 iteration 0104/0187: training loss 0.686; learning rate 0.000290
Epoch 13 iteration 0105/0187: training loss 0.685; learning rate 0.000290
Epoch 13 iteration 0106/0187: training loss 0.684; learning rate 0.000290
Epoch 13 iteration 0107/0187: training loss 0.682; learning rate 0.000290
Epoch 13 iteration 0108/0187: training loss 0.684; learning rate 0.000290
Epoch 13 iteration 0109/0187: training loss 0.683; learning rate 0.000290
Epoch 13 iteration 0110/0187: training loss 0.686; learning rate 0.000290
Epoch 13 iteration 0111/0187: training loss 0.685; learning rate 0.000290
Epoch 13 iteration 0112/0187: training loss 0.686; learning rate 0.000290
Epoch 13 iteration 0113/0187: training loss 0.690; learning rate 0.000290
Epoch 13 iteration 0114/0187: training loss 0.691; learning rate 0.000290
Epoch 13 iteration 0115/0187: training loss 0.690; learning rate 0.000289
Epoch 13 iteration 0116/0187: training loss 0.690; learning rate 0.000289
Epoch 13 iteration 0117/0187: training loss 0.690; learning rate 0.000289
Epoch 13 iteration 0118/0187: training loss 0.691; learning rate 0.000289
Epoch 13 iteration 0119/0187: training loss 0.691; learning rate 0.000289
Epoch 13 iteration 0120/0187: training loss 0.689; learning rate 0.000289
Epoch 13 iteration 0121/0187: training loss 0.688; learning rate 0.000289
Epoch 13 iteration 0122/0187: training loss 0.690; learning rate 0.000289
Epoch 13 iteration 0123/0187: training loss 0.691; learning rate 0.000289
Epoch 13 iteration 0124/0187: training loss 0.692; learning rate 0.000289
Epoch 13 iteration 0125/0187: training loss 0.693; learning rate 0.000289
Epoch 13 iteration 0126/0187: training loss 0.692; learning rate 0.000289
Epoch 13 iteration 0127/0187: training loss 0.692; learning rate 0.000288
Epoch 13 iteration 0128/0187: training loss 0.691; learning rate 0.000288
Epoch 13 iteration 0129/0187: training loss 0.694; learning rate 0.000288
Epoch 13 iteration 0130/0187: training loss 0.692; learning rate 0.000288
Epoch 13 iteration 0131/0187: training loss 0.692; learning rate 0.000288
Epoch 13 iteration 0132/0187: training loss 0.693; learning rate 0.000288
Epoch 13 iteration 0133/0187: training loss 0.695; learning rate 0.000288
Epoch 13 iteration 0134/0187: training loss 0.695; learning rate 0.000288
Epoch 13 iteration 0135/0187: training loss 0.696; learning rate 0.000288
Epoch 13 iteration 0136/0187: training loss 0.696; learning rate 0.000288
Epoch 13 iteration 0137/0187: training loss 0.695; learning rate 0.000288
Epoch 13 iteration 0138/0187: training loss 0.696; learning rate 0.000288
Epoch 13 iteration 0139/0187: training loss 0.697; learning rate 0.000287
Epoch 13 iteration 0140/0187: training loss 0.698; learning rate 0.000287
Epoch 13 iteration 0141/0187: training loss 0.697; learning rate 0.000287
Epoch 13 iteration 0142/0187: training loss 0.696; learning rate 0.000287
Epoch 13 iteration 0143/0187: training loss 0.697; learning rate 0.000287
Epoch 13 iteration 0144/0187: training loss 0.695; learning rate 0.000287
Epoch 13 iteration 0145/0187: training loss 0.695; learning rate 0.000287
Epoch 13 iteration 0146/0187: training loss 0.694; learning rate 0.000287
Epoch 13 iteration 0147/0187: training loss 0.697; learning rate 0.000287
Epoch 13 iteration 0148/0187: training loss 0.697; learning rate 0.000287
Epoch 13 iteration 0149/0187: training loss 0.698; learning rate 0.000287
Epoch 13 iteration 0150/0187: training loss 0.698; learning rate 0.000286
Epoch 13 iteration 0151/0187: training loss 0.698; learning rate 0.000286
Epoch 13 iteration 0152/0187: training loss 0.699; learning rate 0.000286
Epoch 13 iteration 0153/0187: training loss 0.698; learning rate 0.000286
Epoch 13 iteration 0154/0187: training loss 0.698; learning rate 0.000286
Epoch 13 iteration 0155/0187: training loss 0.698; learning rate 0.000286
Epoch 13 iteration 0156/0187: training loss 0.699; learning rate 0.000286
Epoch 13 iteration 0157/0187: training loss 0.697; learning rate 0.000286
Epoch 13 iteration 0158/0187: training loss 0.697; learning rate 0.000286
Epoch 13 iteration 0159/0187: training loss 0.697; learning rate 0.000286
Epoch 13 iteration 0160/0187: training loss 0.695; learning rate 0.000286
Epoch 13 iteration 0161/0187: training loss 0.694; learning rate 0.000286
Epoch 13 iteration 0162/0187: training loss 0.694; learning rate 0.000285
Epoch 13 iteration 0163/0187: training loss 0.694; learning rate 0.000285
Epoch 13 iteration 0164/0187: training loss 0.693; learning rate 0.000285
Epoch 13 iteration 0165/0187: training loss 0.693; learning rate 0.000285
Epoch 13 iteration 0166/0187: training loss 0.693; learning rate 0.000285
Epoch 13 iteration 0167/0187: training loss 0.693; learning rate 0.000285
Epoch 13 iteration 0168/0187: training loss 0.693; learning rate 0.000285
Epoch 13 iteration 0169/0187: training loss 0.692; learning rate 0.000285
Epoch 13 iteration 0170/0187: training loss 0.692; learning rate 0.000285
Epoch 13 iteration 0171/0187: training loss 0.692; learning rate 0.000285
Epoch 13 iteration 0172/0187: training loss 0.692; learning rate 0.000285
Epoch 13 iteration 0173/0187: training loss 0.692; learning rate 0.000285
Epoch 13 iteration 0174/0187: training loss 0.691; learning rate 0.000284
Epoch 13 iteration 0175/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0176/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0177/0187: training loss 0.693; learning rate 0.000284
Epoch 13 iteration 0178/0187: training loss 0.693; learning rate 0.000284
Epoch 13 iteration 0179/0187: training loss 0.693; learning rate 0.000284
Epoch 13 iteration 0180/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0181/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0182/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0183/0187: training loss 0.692; learning rate 0.000284
Epoch 13 iteration 0184/0187: training loss 0.691; learning rate 0.000284
Epoch 13 iteration 0185/0187: training loss 0.691; learning rate 0.000284
Epoch 13 iteration 0186/0187: training loss 0.691; learning rate 0.000283
Epoch 13 iteration 0187/0187: training loss 0.691; learning rate 0.000283
Epoch 13 validation pixAcc: 0.339, mIoU: 0.166
Epoch 14 iteration 0001/0187: training loss 0.614; learning rate 0.000283
Epoch 14 iteration 0002/0187: training loss 0.699; learning rate 0.000283
Epoch 14 iteration 0003/0187: training loss 0.710; learning rate 0.000283
Epoch 14 iteration 0004/0187: training loss 0.724; learning rate 0.000283
Epoch 14 iteration 0005/0187: training loss 0.720; learning rate 0.000283
Epoch 14 iteration 0006/0187: training loss 0.737; learning rate 0.000283
Epoch 14 iteration 0007/0187: training loss 0.717; learning rate 0.000283
Epoch 14 iteration 0008/0187: training loss 0.721; learning rate 0.000283
Epoch 14 iteration 0009/0187: training loss 0.708; learning rate 0.000282
Epoch 14 iteration 0010/0187: training loss 0.714; learning rate 0.000282
Epoch 14 iteration 0011/0187: training loss 0.726; learning rate 0.000282
Epoch 14 iteration 0012/0187: training loss 0.726; learning rate 0.000282
Epoch 14 iteration 0013/0187: training loss 0.720; learning rate 0.000282
Epoch 14 iteration 0014/0187: training loss 0.714; learning rate 0.000282
Epoch 14 iteration 0015/0187: training loss 0.708; learning rate 0.000282
Epoch 14 iteration 0016/0187: training loss 0.706; learning rate 0.000282
Epoch 14 iteration 0017/0187: training loss 0.690; learning rate 0.000282
Epoch 14 iteration 0018/0187: training loss 0.705; learning rate 0.000282
Epoch 14 iteration 0019/0187: training loss 0.696; learning rate 0.000282
Epoch 14 iteration 0020/0187: training loss 0.689; learning rate 0.000282
Epoch 14 iteration 0021/0187: training loss 0.681; learning rate 0.000281
Epoch 14 iteration 0022/0187: training loss 0.679; learning rate 0.000281
Epoch 14 iteration 0023/0187: training loss 0.680; learning rate 0.000281
Epoch 14 iteration 0024/0187: training loss 0.678; learning rate 0.000281
Epoch 14 iteration 0025/0187: training loss 0.677; learning rate 0.000281
Epoch 14 iteration 0026/0187: training loss 0.675; learning rate 0.000281
Epoch 14 iteration 0027/0187: training loss 0.668; learning rate 0.000281
Epoch 14 iteration 0028/0187: training loss 0.662; learning rate 0.000281
Epoch 14 iteration 0029/0187: training loss 0.660; learning rate 0.000281
Epoch 14 iteration 0030/0187: training loss 0.659; learning rate 0.000281
Epoch 14 iteration 0031/0187: training loss 0.656; learning rate 0.000281
Epoch 14 iteration 0032/0187: training loss 0.651; learning rate 0.000281
Epoch 14 iteration 0033/0187: training loss 0.649; learning rate 0.000280
Epoch 14 iteration 0034/0187: training loss 0.642; learning rate 0.000280
Epoch 14 iteration 0035/0187: training loss 0.646; learning rate 0.000280
Epoch 14 iteration 0036/0187: training loss 0.646; learning rate 0.000280
Epoch 14 iteration 0037/0187: training loss 0.653; learning rate 0.000280
Epoch 14 iteration 0038/0187: training loss 0.651; learning rate 0.000280
Epoch 14 iteration 0039/0187: training loss 0.647; learning rate 0.000280
Epoch 14 iteration 0040/0187: training loss 0.648; learning rate 0.000280
Epoch 14 iteration 0041/0187: training loss 0.650; learning rate 0.000280
Epoch 14 iteration 0042/0187: training loss 0.649; learning rate 0.000280
Epoch 14 iteration 0043/0187: training loss 0.654; learning rate 0.000280
Epoch 14 iteration 0044/0187: training loss 0.654; learning rate 0.000279
Epoch 14 iteration 0045/0187: training loss 0.661; learning rate 0.000279
Epoch 14 iteration 0046/0187: training loss 0.667; learning rate 0.000279
Epoch 14 iteration 0047/0187: training loss 0.665; learning rate 0.000279
Epoch 14 iteration 0048/0187: training loss 0.667; learning rate 0.000279
Epoch 14 iteration 0049/0187: training loss 0.670; learning rate 0.000279
Epoch 14 iteration 0050/0187: training loss 0.669; learning rate 0.000279
Epoch 14 iteration 0051/0187: training loss 0.674; learning rate 0.000279
Epoch 14 iteration 0052/0187: training loss 0.674; learning rate 0.000279
Epoch 14 iteration 0053/0187: training loss 0.674; learning rate 0.000279
Epoch 14 iteration 0054/0187: training loss 0.672; learning rate 0.000279
Epoch 14 iteration 0055/0187: training loss 0.671; learning rate 0.000279
Epoch 14 iteration 0056/0187: training loss 0.669; learning rate 0.000278
Epoch 14 iteration 0057/0187: training loss 0.667; learning rate 0.000278
Epoch 14 iteration 0058/0187: training loss 0.664; learning rate 0.000278
Epoch 14 iteration 0059/0187: training loss 0.665; learning rate 0.000278
Epoch 14 iteration 0060/0187: training loss 0.664; learning rate 0.000278
Epoch 14 iteration 0061/0187: training loss 0.664; learning rate 0.000278
Epoch 14 iteration 0062/0187: training loss 0.662; learning rate 0.000278
Epoch 14 iteration 0063/0187: training loss 0.662; learning rate 0.000278
Epoch 14 iteration 0064/0187: training loss 0.662; learning rate 0.000278
Epoch 14 iteration 0065/0187: training loss 0.662; learning rate 0.000278
Epoch 14 iteration 0066/0187: training loss 0.662; learning rate 0.000278
Epoch 14 iteration 0067/0187: training loss 0.663; learning rate 0.000278
Epoch 14 iteration 0068/0187: training loss 0.664; learning rate 0.000277
Epoch 14 iteration 0069/0187: training loss 0.663; learning rate 0.000277
Epoch 14 iteration 0070/0187: training loss 0.664; learning rate 0.000277
Epoch 14 iteration 0071/0187: training loss 0.664; learning rate 0.000277
Epoch 14 iteration 0072/0187: training loss 0.663; learning rate 0.000277
Epoch 14 iteration 0073/0187: training loss 0.662; learning rate 0.000277
Epoch 14 iteration 0074/0187: training loss 0.663; learning rate 0.000277
Epoch 14 iteration 0075/0187: training loss 0.663; learning rate 0.000277
Epoch 14 iteration 0076/0187: training loss 0.662; learning rate 0.000277
Epoch 14 iteration 0077/0187: training loss 0.662; learning rate 0.000277
Epoch 14 iteration 0078/0187: training loss 0.664; learning rate 0.000277
Epoch 14 iteration 0079/0187: training loss 0.664; learning rate 0.000276
Epoch 14 iteration 0080/0187: training loss 0.666; learning rate 0.000276
Epoch 14 iteration 0081/0187: training loss 0.666; learning rate 0.000276
Epoch 14 iteration 0082/0187: training loss 0.665; learning rate 0.000276
Epoch 14 iteration 0083/0187: training loss 0.665; learning rate 0.000276
Epoch 14 iteration 0084/0187: training loss 0.666; learning rate 0.000276
Epoch 14 iteration 0085/0187: training loss 0.666; learning rate 0.000276
Epoch 14 iteration 0086/0187: training loss 0.667; learning rate 0.000276
Epoch 14 iteration 0087/0187: training loss 0.669; learning rate 0.000276
Epoch 14 iteration 0088/0187: training loss 0.668; learning rate 0.000276
Epoch 14 iteration 0089/0187: training loss 0.668; learning rate 0.000276
Epoch 14 iteration 0090/0187: training loss 0.670; learning rate 0.000276
Epoch 14 iteration 0091/0188: training loss 0.672; learning rate 0.000275
Epoch 14 iteration 0092/0188: training loss 0.670; learning rate 0.000275
Epoch 14 iteration 0093/0188: training loss 0.670; learning rate 0.000275
Epoch 14 iteration 0094/0188: training loss 0.671; learning rate 0.000275
Epoch 14 iteration 0095/0188: training loss 0.671; learning rate 0.000275
Epoch 14 iteration 0096/0188: training loss 0.672; learning rate 0.000275
Epoch 14 iteration 0097/0188: training loss 0.672; learning rate 0.000275
Epoch 14 iteration 0098/0188: training loss 0.671; learning rate 0.000275
Epoch 14 iteration 0099/0188: training loss 0.671; learning rate 0.000275
Epoch 14 iteration 0100/0188: training loss 0.674; learning rate 0.000275
Epoch 14 iteration 0101/0188: training loss 0.673; learning rate 0.000275
Epoch 14 iteration 0102/0188: training loss 0.674; learning rate 0.000275
Epoch 14 iteration 0103/0188: training loss 0.675; learning rate 0.000274
Epoch 14 iteration 0104/0188: training loss 0.674; learning rate 0.000274
Epoch 14 iteration 0105/0188: training loss 0.676; learning rate 0.000274
Epoch 14 iteration 0106/0188: training loss 0.677; learning rate 0.000274
Epoch 14 iteration 0107/0188: training loss 0.677; learning rate 0.000274
Epoch 14 iteration 0108/0188: training loss 0.676; learning rate 0.000274
Epoch 14 iteration 0109/0188: training loss 0.679; learning rate 0.000274
Epoch 14 iteration 0110/0188: training loss 0.679; learning rate 0.000274
Epoch 14 iteration 0111/0188: training loss 0.678; learning rate 0.000274
Epoch 14 iteration 0112/0188: training loss 0.678; learning rate 0.000274
Epoch 14 iteration 0113/0188: training loss 0.677; learning rate 0.000274
Epoch 14 iteration 0114/0188: training loss 0.676; learning rate 0.000273
Epoch 14 iteration 0115/0188: training loss 0.681; learning rate 0.000273
Epoch 14 iteration 0116/0188: training loss 0.680; learning rate 0.000273
Epoch 14 iteration 0117/0188: training loss 0.678; learning rate 0.000273
Epoch 14 iteration 0118/0188: training loss 0.680; learning rate 0.000273
Epoch 14 iteration 0119/0188: training loss 0.680; learning rate 0.000273
Epoch 14 iteration 0120/0188: training loss 0.678; learning rate 0.000273
Epoch 14 iteration 0121/0188: training loss 0.678; learning rate 0.000273
Epoch 14 iteration 0122/0188: training loss 0.680; learning rate 0.000273
Epoch 14 iteration 0123/0188: training loss 0.681; learning rate 0.000273
Epoch 14 iteration 0124/0188: training loss 0.678; learning rate 0.000273
Epoch 14 iteration 0125/0188: training loss 0.677; learning rate 0.000273
Epoch 14 iteration 0126/0188: training loss 0.676; learning rate 0.000272
Epoch 14 iteration 0127/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0128/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0129/0188: training loss 0.676; learning rate 0.000272
Epoch 14 iteration 0130/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0131/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0132/0188: training loss 0.678; learning rate 0.000272
Epoch 14 iteration 0133/0188: training loss 0.678; learning rate 0.000272
Epoch 14 iteration 0134/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0135/0188: training loss 0.677; learning rate 0.000272
Epoch 14 iteration 0136/0188: training loss 0.676; learning rate 0.000272
Epoch 14 iteration 0137/0188: training loss 0.675; learning rate 0.000272
Epoch 14 iteration 0138/0188: training loss 0.675; learning rate 0.000271
Epoch 14 iteration 0139/0188: training loss 0.676; learning rate 0.000271
Epoch 14 iteration 0140/0188: training loss 0.676; learning rate 0.000271
Epoch 14 iteration 0141/0188: training loss 0.677; learning rate 0.000271
Epoch 14 iteration 0142/0188: training loss 0.677; learning rate 0.000271
Epoch 14 iteration 0143/0188: training loss 0.677; learning rate 0.000271
Epoch 14 iteration 0144/0188: training loss 0.680; learning rate 0.000271
Epoch 14 iteration 0145/0188: training loss 0.681; learning rate 0.000271
Epoch 14 iteration 0146/0188: training loss 0.681; learning rate 0.000271
Epoch 14 iteration 0147/0188: training loss 0.679; learning rate 0.000271
Epoch 14 iteration 0148/0188: training loss 0.679; learning rate 0.000271
Epoch 14 iteration 0149/0188: training loss 0.678; learning rate 0.000270
Epoch 14 iteration 0150/0188: training loss 0.677; learning rate 0.000270
Epoch 14 iteration 0151/0188: training loss 0.678; learning rate 0.000270
Epoch 14 iteration 0152/0188: training loss 0.680; learning rate 0.000270
Epoch 14 iteration 0153/0188: training loss 0.681; learning rate 0.000270
Epoch 14 iteration 0154/0188: training loss 0.681; learning rate 0.000270
Epoch 14 iteration 0155/0188: training loss 0.681; learning rate 0.000270
Epoch 14 iteration 0156/0188: training loss 0.680; learning rate 0.000270
Epoch 14 iteration 0157/0188: training loss 0.679; learning rate 0.000270
Epoch 14 iteration 0158/0188: training loss 0.679; learning rate 0.000270
Epoch 14 iteration 0159/0188: training loss 0.679; learning rate 0.000270
Epoch 14 iteration 0160/0188: training loss 0.680; learning rate 0.000270
Epoch 14 iteration 0161/0188: training loss 0.679; learning rate 0.000269
Epoch 14 iteration 0162/0188: training loss 0.679; learning rate 0.000269
Epoch 14 iteration 0163/0188: training loss 0.678; learning rate 0.000269
Epoch 14 iteration 0164/0188: training loss 0.679; learning rate 0.000269
Epoch 14 iteration 0165/0188: training loss 0.679; learning rate 0.000269
Epoch 14 iteration 0166/0188: training loss 0.680; learning rate 0.000269
Epoch 14 iteration 0167/0188: training loss 0.681; learning rate 0.000269
Epoch 14 iteration 0168/0188: training loss 0.681; learning rate 0.000269
Epoch 14 iteration 0169/0188: training loss 0.681; learning rate 0.000269
Epoch 14 iteration 0170/0188: training loss 0.681; learning rate 0.000269
Epoch 14 iteration 0171/0188: training loss 0.682; learning rate 0.000269
Epoch 14 iteration 0172/0188: training loss 0.683; learning rate 0.000269
Epoch 14 iteration 0173/0188: training loss 0.683; learning rate 0.000268
Epoch 14 iteration 0174/0188: training loss 0.684; learning rate 0.000268
Epoch 14 iteration 0175/0188: training loss 0.683; learning rate 0.000268
Epoch 14 iteration 0176/0188: training loss 0.682; learning rate 0.000268
Epoch 14 iteration 0177/0188: training loss 0.681; learning rate 0.000268
Epoch 14 iteration 0178/0188: training loss 0.681; learning rate 0.000268
Epoch 14 iteration 0179/0188: training loss 0.680; learning rate 0.000268
Epoch 14 iteration 0180/0188: training loss 0.682; learning rate 0.000268
Epoch 14 iteration 0181/0188: training loss 0.682; learning rate 0.000268
Epoch 14 iteration 0182/0188: training loss 0.682; learning rate 0.000268
Epoch 14 iteration 0183/0188: training loss 0.682; learning rate 0.000268
Epoch 14 iteration 0184/0188: training loss 0.684; learning rate 0.000267
Epoch 14 iteration 0185/0188: training loss 0.684; learning rate 0.000267
Epoch 14 iteration 0186/0188: training loss 0.685; learning rate 0.000267
Epoch 14 validation pixAcc: 0.337, mIoU: 0.171
Epoch 15 iteration 0001/0187: training loss 0.706; learning rate 0.000267
Epoch 15 iteration 0002/0187: training loss 0.656; learning rate 0.000267
Epoch 15 iteration 0003/0187: training loss 0.704; learning rate 0.000267
Epoch 15 iteration 0004/0187: training loss 0.680; learning rate 0.000267
Epoch 15 iteration 0005/0187: training loss 0.660; learning rate 0.000267
Epoch 15 iteration 0006/0187: training loss 0.639; learning rate 0.000267
Epoch 15 iteration 0007/0187: training loss 0.679; learning rate 0.000267
Epoch 15 iteration 0008/0187: training loss 0.655; learning rate 0.000267
Epoch 15 iteration 0009/0187: training loss 0.639; learning rate 0.000266
Epoch 15 iteration 0010/0187: training loss 0.651; learning rate 0.000266
Epoch 15 iteration 0011/0187: training loss 0.645; learning rate 0.000266
Epoch 15 iteration 0012/0187: training loss 0.643; learning rate 0.000266
Epoch 15 iteration 0013/0187: training loss 0.647; learning rate 0.000266
Epoch 15 iteration 0014/0187: training loss 0.652; learning rate 0.000266
Epoch 15 iteration 0015/0187: training loss 0.650; learning rate 0.000266
Epoch 15 iteration 0016/0187: training loss 0.643; learning rate 0.000266
Epoch 15 iteration 0017/0187: training loss 0.645; learning rate 0.000266
Epoch 15 iteration 0018/0187: training loss 0.650; learning rate 0.000266
Epoch 15 iteration 0019/0187: training loss 0.654; learning rate 0.000266
Epoch 15 iteration 0020/0187: training loss 0.653; learning rate 0.000265
Epoch 15 iteration 0021/0187: training loss 0.649; learning rate 0.000265
Epoch 15 iteration 0022/0187: training loss 0.645; learning rate 0.000265
Epoch 15 iteration 0023/0187: training loss 0.642; learning rate 0.000265
Epoch 15 iteration 0024/0187: training loss 0.643; learning rate 0.000265
Epoch 15 iteration 0025/0187: training loss 0.652; learning rate 0.000265
Epoch 15 iteration 0026/0187: training loss 0.645; learning rate 0.000265
Epoch 15 iteration 0027/0187: training loss 0.644; learning rate 0.000265
Epoch 15 iteration 0028/0187: training loss 0.643; learning rate 0.000265
Epoch 15 iteration 0029/0187: training loss 0.638; learning rate 0.000265
Epoch 15 iteration 0030/0187: training loss 0.646; learning rate 0.000265
Epoch 15 iteration 0031/0187: training loss 0.651; learning rate 0.000265
Epoch 15 iteration 0032/0187: training loss 0.646; learning rate 0.000264
Epoch 15 iteration 0033/0187: training loss 0.644; learning rate 0.000264
Epoch 15 iteration 0034/0187: training loss 0.639; learning rate 0.000264
Epoch 15 iteration 0035/0187: training loss 0.641; learning rate 0.000264
Epoch 15 iteration 0036/0187: training loss 0.648; learning rate 0.000264
Epoch 15 iteration 0037/0187: training loss 0.647; learning rate 0.000264
Epoch 15 iteration 0038/0187: training loss 0.647; learning rate 0.000264
Epoch 15 iteration 0039/0187: training loss 0.646; learning rate 0.000264
Epoch 15 iteration 0040/0187: training loss 0.641; learning rate 0.000264
Epoch 15 iteration 0041/0187: training loss 0.641; learning rate 0.000264
Epoch 15 iteration 0042/0187: training loss 0.646; learning rate 0.000264
Epoch 15 iteration 0043/0187: training loss 0.648; learning rate 0.000264
Epoch 15 iteration 0044/0187: training loss 0.653; learning rate 0.000263
Epoch 15 iteration 0045/0187: training loss 0.654; learning rate 0.000263
Epoch 15 iteration 0046/0187: training loss 0.651; learning rate 0.000263
Epoch 15 iteration 0047/0187: training loss 0.650; learning rate 0.000263
Epoch 15 iteration 0048/0187: training loss 0.651; learning rate 0.000263
Epoch 15 iteration 0049/0187: training loss 0.654; learning rate 0.000263
Epoch 15 iteration 0050/0187: training loss 0.652; learning rate 0.000263
Epoch 15 iteration 0051/0187: training loss 0.654; learning rate 0.000263
Epoch 15 iteration 0052/0187: training loss 0.654; learning rate 0.000263
Epoch 15 iteration 0053/0187: training loss 0.651; learning rate 0.000263
Epoch 15 iteration 0054/0187: training loss 0.648; learning rate 0.000263
Epoch 15 iteration 0055/0187: training loss 0.650; learning rate 0.000262
Epoch 15 iteration 0056/0187: training loss 0.649; learning rate 0.000262
Epoch 15 iteration 0057/0187: training loss 0.648; learning rate 0.000262
Epoch 15 iteration 0058/0187: training loss 0.651; learning rate 0.000262
Epoch 15 iteration 0059/0187: training loss 0.651; learning rate 0.000262
Epoch 15 iteration 0060/0187: training loss 0.650; learning rate 0.000262
Epoch 15 iteration 0061/0187: training loss 0.652; learning rate 0.000262
Epoch 15 iteration 0062/0187: training loss 0.653; learning rate 0.000262
Epoch 15 iteration 0063/0187: training loss 0.661; learning rate 0.000262
Epoch 15 iteration 0064/0187: training loss 0.664; learning rate 0.000262
Epoch 15 iteration 0065/0187: training loss 0.663; learning rate 0.000262
Epoch 15 iteration 0066/0187: training loss 0.665; learning rate 0.000262
Epoch 15 iteration 0067/0187: training loss 0.663; learning rate 0.000261
Epoch 15 iteration 0068/0187: training loss 0.663; learning rate 0.000261
Epoch 15 iteration 0069/0187: training loss 0.665; learning rate 0.000261
Epoch 15 iteration 0070/0187: training loss 0.665; learning rate 0.000261
Epoch 15 iteration 0071/0187: training loss 0.666; learning rate 0.000261
Epoch 15 iteration 0072/0187: training loss 0.668; learning rate 0.000261
Epoch 15 iteration 0073/0187: training loss 0.669; learning rate 0.000261
Epoch 15 iteration 0074/0187: training loss 0.674; learning rate 0.000261
Epoch 15 iteration 0075/0187: training loss 0.675; learning rate 0.000261
Epoch 15 iteration 0076/0187: training loss 0.674; learning rate 0.000261
Epoch 15 iteration 0077/0187: training loss 0.672; learning rate 0.000261
Epoch 15 iteration 0078/0187: training loss 0.673; learning rate 0.000260
Epoch 15 iteration 0079/0187: training loss 0.672; learning rate 0.000260
Epoch 15 iteration 0080/0187: training loss 0.675; learning rate 0.000260
Epoch 15 iteration 0081/0187: training loss 0.676; learning rate 0.000260
Epoch 15 iteration 0082/0187: training loss 0.681; learning rate 0.000260
Epoch 15 iteration 0083/0187: training loss 0.679; learning rate 0.000260
Epoch 15 iteration 0084/0187: training loss 0.684; learning rate 0.000260
Epoch 15 iteration 0085/0187: training loss 0.686; learning rate 0.000260
Epoch 15 iteration 0086/0187: training loss 0.687; learning rate 0.000260
Epoch 15 iteration 0087/0187: training loss 0.685; learning rate 0.000260
Epoch 15 iteration 0088/0187: training loss 0.685; learning rate 0.000260
Epoch 15 iteration 0089/0187: training loss 0.690; learning rate 0.000260
Epoch 15 iteration 0090/0187: training loss 0.691; learning rate 0.000259
Epoch 15 iteration 0091/0187: training loss 0.695; learning rate 0.000259
Epoch 15 iteration 0092/0187: training loss 0.693; learning rate 0.000259
Epoch 15 iteration 0093/0187: training loss 0.694; learning rate 0.000259
Epoch 15 iteration 0094/0187: training loss 0.694; learning rate 0.000259
Epoch 15 iteration 0095/0187: training loss 0.693; learning rate 0.000259
Epoch 15 iteration 0096/0187: training loss 0.692; learning rate 0.000259
Epoch 15 iteration 0097/0187: training loss 0.692; learning rate 0.000259
Epoch 15 iteration 0098/0187: training loss 0.695; learning rate 0.000259
Epoch 15 iteration 0099/0187: training loss 0.697; learning rate 0.000259
Epoch 15 iteration 0100/0187: training loss 0.701; learning rate 0.000259
Epoch 15 iteration 0101/0187: training loss 0.705; learning rate 0.000259
Epoch 15 iteration 0102/0187: training loss 0.705; learning rate 0.000258
Epoch 15 iteration 0103/0187: training loss 0.703; learning rate 0.000258
Epoch 15 iteration 0104/0187: training loss 0.701; learning rate 0.000258
Epoch 15 iteration 0105/0187: training loss 0.699; learning rate 0.000258
Epoch 15 iteration 0106/0187: training loss 0.701; learning rate 0.000258
Epoch 15 iteration 0107/0187: training loss 0.699; learning rate 0.000258
Epoch 15 iteration 0108/0187: training loss 0.699; learning rate 0.000258
Epoch 15 iteration 0109/0187: training loss 0.700; learning rate 0.000258
Epoch 15 iteration 0110/0187: training loss 0.700; learning rate 0.000258
Epoch 15 iteration 0111/0187: training loss 0.698; learning rate 0.000258
Epoch 15 iteration 0112/0187: training loss 0.697; learning rate 0.000258
Epoch 15 iteration 0113/0187: training loss 0.699; learning rate 0.000257
Epoch 15 iteration 0114/0187: training loss 0.699; learning rate 0.000257
Epoch 15 iteration 0115/0187: training loss 0.697; learning rate 0.000257
Epoch 15 iteration 0116/0187: training loss 0.698; learning rate 0.000257
Epoch 15 iteration 0117/0187: training loss 0.703; learning rate 0.000257
Epoch 15 iteration 0118/0187: training loss 0.703; learning rate 0.000257
Epoch 15 iteration 0119/0187: training loss 0.703; learning rate 0.000257
Epoch 15 iteration 0120/0187: training loss 0.702; learning rate 0.000257
Epoch 15 iteration 0121/0187: training loss 0.701; learning rate 0.000257
Epoch 15 iteration 0122/0187: training loss 0.701; learning rate 0.000257
Epoch 15 iteration 0123/0187: training loss 0.700; learning rate 0.000257
Epoch 15 iteration 0124/0187: training loss 0.698; learning rate 0.000257
Epoch 15 iteration 0125/0187: training loss 0.697; learning rate 0.000256
Epoch 15 iteration 0126/0187: training loss 0.697; learning rate 0.000256
Epoch 15 iteration 0127/0187: training loss 0.699; learning rate 0.000256
Epoch 15 iteration 0128/0187: training loss 0.700; learning rate 0.000256
Epoch 15 iteration 0129/0187: training loss 0.700; learning rate 0.000256
Epoch 15 iteration 0130/0187: training loss 0.699; learning rate 0.000256
Epoch 15 iteration 0131/0187: training loss 0.700; learning rate 0.000256
Epoch 15 iteration 0132/0187: training loss 0.700; learning rate 0.000256
Epoch 15 iteration 0133/0187: training loss 0.699; learning rate 0.000256
Epoch 15 iteration 0134/0187: training loss 0.700; learning rate 0.000256
Epoch 15 iteration 0135/0187: training loss 0.699; learning rate 0.000256
Epoch 15 iteration 0136/0187: training loss 0.697; learning rate 0.000255
Epoch 15 iteration 0137/0187: training loss 0.698; learning rate 0.000255
Epoch 15 iteration 0138/0187: training loss 0.698; learning rate 0.000255
Epoch 15 iteration 0139/0187: training loss 0.699; learning rate 0.000255
Epoch 15 iteration 0140/0187: training loss 0.699; learning rate 0.000255
Epoch 15 iteration 0141/0187: training loss 0.699; learning rate 0.000255
Epoch 15 iteration 0142/0187: training loss 0.698; learning rate 0.000255
Epoch 15 iteration 0143/0187: training loss 0.698; learning rate 0.000255
Epoch 15 iteration 0144/0187: training loss 0.698; learning rate 0.000255
Epoch 15 iteration 0145/0187: training loss 0.699; learning rate 0.000255
Epoch 15 iteration 0146/0187: training loss 0.697; learning rate 0.000255
Epoch 15 iteration 0147/0187: training loss 0.697; learning rate 0.000255
Epoch 15 iteration 0148/0187: training loss 0.696; learning rate 0.000254
Epoch 15 iteration 0149/0187: training loss 0.695; learning rate 0.000254
Epoch 15 iteration 0150/0187: training loss 0.696; learning rate 0.000254
Epoch 15 iteration 0151/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0152/0187: training loss 0.698; learning rate 0.000254
Epoch 15 iteration 0153/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0154/0187: training loss 0.698; learning rate 0.000254
Epoch 15 iteration 0155/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0156/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0157/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0158/0187: training loss 0.697; learning rate 0.000254
Epoch 15 iteration 0159/0187: training loss 0.697; learning rate 0.000253
Epoch 15 iteration 0160/0187: training loss 0.698; learning rate 0.000253
Epoch 15 iteration 0161/0187: training loss 0.697; learning rate 0.000253
Epoch 15 iteration 0162/0187: training loss 0.697; learning rate 0.000253
Epoch 15 iteration 0163/0187: training loss 0.697; learning rate 0.000253
Epoch 15 iteration 0164/0187: training loss 0.695; learning rate 0.000253
Epoch 15 iteration 0165/0187: training loss 0.695; learning rate 0.000253
Epoch 15 iteration 0166/0187: training loss 0.694; learning rate 0.000253
Epoch 15 iteration 0167/0187: training loss 0.696; learning rate 0.000253
Epoch 15 iteration 0168/0187: training loss 0.694; learning rate 0.000253
Epoch 15 iteration 0169/0187: training loss 0.696; learning rate 0.000253
Epoch 15 iteration 0170/0187: training loss 0.696; learning rate 0.000253
Epoch 15 iteration 0171/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0172/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0173/0187: training loss 0.697; learning rate 0.000252
Epoch 15 iteration 0174/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0175/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0176/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0177/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0178/0187: training loss 0.695; learning rate 0.000252
Epoch 15 iteration 0179/0187: training loss 0.696; learning rate 0.000252
Epoch 15 iteration 0180/0187: training loss 0.694; learning rate 0.000252
Epoch 15 iteration 0181/0187: training loss 0.694; learning rate 0.000252
Epoch 15 iteration 0182/0187: training loss 0.694; learning rate 0.000252
Epoch 15 iteration 0183/0187: training loss 0.694; learning rate 0.000251
Epoch 15 iteration 0184/0187: training loss 0.693; learning rate 0.000251
Epoch 15 iteration 0185/0187: training loss 0.693; learning rate 0.000251
Epoch 15 iteration 0186/0187: training loss 0.693; learning rate 0.000251
Epoch 15 iteration 0187/0187: training loss 0.693; learning rate 0.000251
Epoch 15 validation pixAcc: 0.338, mIoU: 0.170
Epoch 16 iteration 0001/0187: training loss 0.755; learning rate 0.000251
Epoch 16 iteration 0002/0187: training loss 0.704; learning rate 0.000251
Epoch 16 iteration 0003/0187: training loss 0.686; learning rate 0.000251
Epoch 16 iteration 0004/0187: training loss 0.715; learning rate 0.000251
Epoch 16 iteration 0005/0187: training loss 0.707; learning rate 0.000251
Epoch 16 iteration 0006/0187: training loss 0.697; learning rate 0.000250
Epoch 16 iteration 0007/0187: training loss 0.690; learning rate 0.000250
Epoch 16 iteration 0008/0187: training loss 0.710; learning rate 0.000250
Epoch 16 iteration 0009/0187: training loss 0.697; learning rate 0.000250
Epoch 16 iteration 0010/0187: training loss 0.690; learning rate 0.000250
Epoch 16 iteration 0011/0187: training loss 0.690; learning rate 0.000250
Epoch 16 iteration 0012/0187: training loss 0.705; learning rate 0.000250
Epoch 16 iteration 0013/0187: training loss 0.701; learning rate 0.000250
Epoch 16 iteration 0014/0187: training loss 0.683; learning rate 0.000250
Epoch 16 iteration 0015/0187: training loss 0.685; learning rate 0.000250
Epoch 16 iteration 0016/0187: training loss 0.678; learning rate 0.000250
Epoch 16 iteration 0017/0187: training loss 0.684; learning rate 0.000250
Epoch 16 iteration 0018/0187: training loss 0.674; learning rate 0.000249
Epoch 16 iteration 0019/0187: training loss 0.664; learning rate 0.000249
Epoch 16 iteration 0020/0187: training loss 0.661; learning rate 0.000249
Epoch 16 iteration 0021/0187: training loss 0.659; learning rate 0.000249
Epoch 16 iteration 0022/0187: training loss 0.665; learning rate 0.000249
Epoch 16 iteration 0023/0187: training loss 0.678; learning rate 0.000249
Epoch 16 iteration 0024/0187: training loss 0.679; learning rate 0.000249
Epoch 16 iteration 0025/0187: training loss 0.673; learning rate 0.000249
Epoch 16 iteration 0026/0187: training loss 0.674; learning rate 0.000249
Epoch 16 iteration 0027/0187: training loss 0.673; learning rate 0.000249
Epoch 16 iteration 0028/0187: training loss 0.674; learning rate 0.000249
Epoch 16 iteration 0029/0187: training loss 0.677; learning rate 0.000248
Epoch 16 iteration 0030/0187: training loss 0.670; learning rate 0.000248
Epoch 16 iteration 0031/0187: training loss 0.668; learning rate 0.000248
Epoch 16 iteration 0032/0187: training loss 0.676; learning rate 0.000248
Epoch 16 iteration 0033/0187: training loss 0.676; learning rate 0.000248
Epoch 16 iteration 0034/0187: training loss 0.674; learning rate 0.000248
Epoch 16 iteration 0035/0187: training loss 0.670; learning rate 0.000248
Epoch 16 iteration 0036/0187: training loss 0.672; learning rate 0.000248
Epoch 16 iteration 0037/0187: training loss 0.672; learning rate 0.000248
Epoch 16 iteration 0038/0187: training loss 0.676; learning rate 0.000248
Epoch 16 iteration 0039/0187: training loss 0.679; learning rate 0.000248
Epoch 16 iteration 0040/0187: training loss 0.677; learning rate 0.000248
Epoch 16 iteration 0041/0187: training loss 0.680; learning rate 0.000247
Epoch 16 iteration 0042/0187: training loss 0.680; learning rate 0.000247
Epoch 16 iteration 0043/0187: training loss 0.689; learning rate 0.000247
Epoch 16 iteration 0044/0187: training loss 0.687; learning rate 0.000247
Epoch 16 iteration 0045/0187: training loss 0.687; learning rate 0.000247
Epoch 16 iteration 0046/0187: training loss 0.688; learning rate 0.000247
Epoch 16 iteration 0047/0187: training loss 0.688; learning rate 0.000247
Epoch 16 iteration 0048/0187: training loss 0.686; learning rate 0.000247
Epoch 16 iteration 0049/0187: training loss 0.689; learning rate 0.000247
Epoch 16 iteration 0050/0187: training loss 0.688; learning rate 0.000247
Epoch 16 iteration 0051/0187: training loss 0.684; learning rate 0.000247
Epoch 16 iteration 0052/0187: training loss 0.683; learning rate 0.000246
Epoch 16 iteration 0053/0187: training loss 0.681; learning rate 0.000246
Epoch 16 iteration 0054/0187: training loss 0.683; learning rate 0.000246
Epoch 16 iteration 0055/0187: training loss 0.684; learning rate 0.000246
Epoch 16 iteration 0056/0187: training loss 0.685; learning rate 0.000246
Epoch 16 iteration 0057/0187: training loss 0.686; learning rate 0.000246
Epoch 16 iteration 0058/0187: training loss 0.686; learning rate 0.000246
Epoch 16 iteration 0059/0187: training loss 0.686; learning rate 0.000246
Epoch 16 iteration 0060/0187: training loss 0.687; learning rate 0.000246
Epoch 16 iteration 0061/0187: training loss 0.684; learning rate 0.000246
Epoch 16 iteration 0062/0187: training loss 0.682; learning rate 0.000246
Epoch 16 iteration 0063/0187: training loss 0.684; learning rate 0.000246
Epoch 16 iteration 0064/0187: training loss 0.686; learning rate 0.000245
Epoch 16 iteration 0065/0187: training loss 0.689; learning rate 0.000245
Epoch 16 iteration 0066/0187: training loss 0.688; learning rate 0.000245
Epoch 16 iteration 0067/0187: training loss 0.687; learning rate 0.000245
Epoch 16 iteration 0068/0187: training loss 0.693; learning rate 0.000245
Epoch 16 iteration 0069/0187: training loss 0.692; learning rate 0.000245
Epoch 16 iteration 0070/0187: training loss 0.693; learning rate 0.000245
Epoch 16 iteration 0071/0187: training loss 0.694; learning rate 0.000245
Epoch 16 iteration 0072/0187: training loss 0.691; learning rate 0.000245
Epoch 16 iteration 0073/0187: training loss 0.690; learning rate 0.000245
Epoch 16 iteration 0074/0187: training loss 0.687; learning rate 0.000245
Epoch 16 iteration 0075/0187: training loss 0.687; learning rate 0.000244
Epoch 16 iteration 0076/0187: training loss 0.689; learning rate 0.000244
Epoch 16 iteration 0077/0187: training loss 0.690; learning rate 0.000244
Epoch 16 iteration 0078/0187: training loss 0.690; learning rate 0.000244
Epoch 16 iteration 0079/0187: training loss 0.690; learning rate 0.000244
Epoch 16 iteration 0080/0187: training loss 0.689; learning rate 0.000244
Epoch 16 iteration 0081/0187: training loss 0.691; learning rate 0.000244
Epoch 16 iteration 0082/0187: training loss 0.692; learning rate 0.000244
Epoch 16 iteration 0083/0187: training loss 0.691; learning rate 0.000244
Epoch 16 iteration 0084/0187: training loss 0.697; learning rate 0.000244
Epoch 16 iteration 0085/0187: training loss 0.695; learning rate 0.000244
Epoch 16 iteration 0086/0187: training loss 0.698; learning rate 0.000244
Epoch 16 iteration 0087/0187: training loss 0.696; learning rate 0.000243
Epoch 16 iteration 0088/0187: training loss 0.696; learning rate 0.000243
Epoch 16 iteration 0089/0187: training loss 0.695; learning rate 0.000243
Epoch 16 iteration 0090/0187: training loss 0.695; learning rate 0.000243
Epoch 16 iteration 0091/0188: training loss 0.696; learning rate 0.000243
Epoch 16 iteration 0092/0188: training loss 0.695; learning rate 0.000243
Epoch 16 iteration 0093/0188: training loss 0.695; learning rate 0.000243
Epoch 16 iteration 0094/0188: training loss 0.693; learning rate 0.000243
Epoch 16 iteration 0095/0188: training loss 0.693; learning rate 0.000243
Epoch 16 iteration 0096/0188: training loss 0.695; learning rate 0.000243
Epoch 16 iteration 0097/0188: training loss 0.694; learning rate 0.000243
Epoch 16 iteration 0098/0188: training loss 0.693; learning rate 0.000242
Epoch 16 iteration 0099/0188: training loss 0.693; learning rate 0.000242
Epoch 16 iteration 0100/0188: training loss 0.691; learning rate 0.000242
Epoch 16 iteration 0101/0188: training loss 0.693; learning rate 0.000242
Epoch 16 iteration 0102/0188: training loss 0.694; learning rate 0.000242
Epoch 16 iteration 0103/0188: training loss 0.692; learning rate 0.000242
Epoch 16 iteration 0104/0188: training loss 0.692; learning rate 0.000242
Epoch 16 iteration 0105/0188: training loss 0.692; learning rate 0.000242
Epoch 16 iteration 0106/0188: training loss 0.694; learning rate 0.000242
Epoch 16 iteration 0107/0188: training loss 0.695; learning rate 0.000242
Epoch 16 iteration 0108/0188: training loss 0.695; learning rate 0.000242
Epoch 16 iteration 0109/0188: training loss 0.694; learning rate 0.000242
Epoch 16 iteration 0110/0188: training loss 0.694; learning rate 0.000241
Epoch 16 iteration 0111/0188: training loss 0.696; learning rate 0.000241
Epoch 16 iteration 0112/0188: training loss 0.697; learning rate 0.000241
Epoch 16 iteration 0113/0188: training loss 0.698; learning rate 0.000241
Epoch 16 iteration 0114/0188: training loss 0.697; learning rate 0.000241
Epoch 16 iteration 0115/0188: training loss 0.695; learning rate 0.000241
Epoch 16 iteration 0116/0188: training loss 0.697; learning rate 0.000241
Epoch 16 iteration 0117/0188: training loss 0.697; learning rate 0.000241
Epoch 16 iteration 0118/0188: training loss 0.699; learning rate 0.000241
Epoch 16 iteration 0119/0188: training loss 0.699; learning rate 0.000241
Epoch 16 iteration 0120/0188: training loss 0.699; learning rate 0.000241
Epoch 16 iteration 0121/0188: training loss 0.700; learning rate 0.000240
Epoch 16 iteration 0122/0188: training loss 0.700; learning rate 0.000240
Epoch 16 iteration 0123/0188: training loss 0.702; learning rate 0.000240
Epoch 16 iteration 0124/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0125/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0126/0188: training loss 0.704; learning rate 0.000240
Epoch 16 iteration 0127/0188: training loss 0.702; learning rate 0.000240
Epoch 16 iteration 0128/0188: training loss 0.702; learning rate 0.000240
Epoch 16 iteration 0129/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0130/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0131/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0132/0188: training loss 0.703; learning rate 0.000240
Epoch 16 iteration 0133/0188: training loss 0.703; learning rate 0.000239
Epoch 16 iteration 0134/0188: training loss 0.701; learning rate 0.000239
Epoch 16 iteration 0135/0188: training loss 0.702; learning rate 0.000239
Epoch 16 iteration 0136/0188: training loss 0.702; learning rate 0.000239
Epoch 16 iteration 0137/0188: training loss 0.702; learning rate 0.000239
Epoch 16 iteration 0138/0188: training loss 0.702; learning rate 0.000239
Epoch 16 iteration 0139/0188: training loss 0.701; learning rate 0.000239
Epoch 16 iteration 0140/0188: training loss 0.701; learning rate 0.000239
Epoch 16 iteration 0141/0188: training loss 0.699; learning rate 0.000239
Epoch 16 iteration 0142/0188: training loss 0.699; learning rate 0.000239
Epoch 16 iteration 0143/0188: training loss 0.701; learning rate 0.000239
Epoch 16 iteration 0144/0188: training loss 0.702; learning rate 0.000238
Epoch 16 iteration 0145/0188: training loss 0.701; learning rate 0.000238
Epoch 16 iteration 0146/0188: training loss 0.701; learning rate 0.000238
Epoch 16 iteration 0147/0188: training loss 0.702; learning rate 0.000238
Epoch 16 iteration 0148/0188: training loss 0.701; learning rate 0.000238
Epoch 16 iteration 0149/0188: training loss 0.700; learning rate 0.000238
Epoch 16 iteration 0150/0188: training loss 0.699; learning rate 0.000238
Epoch 16 iteration 0151/0188: training loss 0.700; learning rate 0.000238
Epoch 16 iteration 0152/0188: training loss 0.698; learning rate 0.000238
Epoch 16 iteration 0153/0188: training loss 0.699; learning rate 0.000238
Epoch 16 iteration 0154/0188: training loss 0.698; learning rate 0.000238
Epoch 16 iteration 0155/0188: training loss 0.698; learning rate 0.000238
Epoch 16 iteration 0156/0188: training loss 0.697; learning rate 0.000237
Epoch 16 iteration 0157/0188: training loss 0.697; learning rate 0.000237
Epoch 16 iteration 0158/0188: training loss 0.698; learning rate 0.000237
Epoch 16 iteration 0159/0188: training loss 0.700; learning rate 0.000237
Epoch 16 iteration 0160/0188: training loss 0.701; learning rate 0.000237
Epoch 16 iteration 0161/0188: training loss 0.700; learning rate 0.000237
Epoch 16 iteration 0162/0188: training loss 0.699; learning rate 0.000237
Epoch 16 iteration 0163/0188: training loss 0.699; learning rate 0.000237
Epoch 16 iteration 0164/0188: training loss 0.699; learning rate 0.000237
Epoch 16 iteration 0165/0188: training loss 0.700; learning rate 0.000237
Epoch 16 iteration 0166/0188: training loss 0.700; learning rate 0.000237
Epoch 16 iteration 0167/0188: training loss 0.701; learning rate 0.000236
Epoch 16 iteration 0168/0188: training loss 0.700; learning rate 0.000236
Epoch 16 iteration 0169/0188: training loss 0.699; learning rate 0.000236
Epoch 16 iteration 0170/0188: training loss 0.699; learning rate 0.000236
Epoch 16 iteration 0171/0188: training loss 0.698; learning rate 0.000236
Epoch 16 iteration 0172/0188: training loss 0.697; learning rate 0.000236
Epoch 16 iteration 0173/0188: training loss 0.698; learning rate 0.000236
Epoch 16 iteration 0174/0188: training loss 0.697; learning rate 0.000236
Epoch 16 iteration 0175/0188: training loss 0.697; learning rate 0.000236
Epoch 16 iteration 0176/0188: training loss 0.697; learning rate 0.000236
Epoch 16 iteration 0177/0188: training loss 0.697; learning rate 0.000236
Epoch 16 iteration 0178/0188: training loss 0.696; learning rate 0.000236
Epoch 16 iteration 0179/0188: training loss 0.695; learning rate 0.000235
Epoch 16 iteration 0180/0188: training loss 0.695; learning rate 0.000235
Epoch 16 iteration 0181/0188: training loss 0.695; learning rate 0.000235
Epoch 16 iteration 0182/0188: training loss 0.696; learning rate 0.000235
Epoch 16 iteration 0183/0188: training loss 0.696; learning rate 0.000235
Epoch 16 iteration 0184/0188: training loss 0.695; learning rate 0.000235
Epoch 16 iteration 0185/0188: training loss 0.695; learning rate 0.000235
Epoch 16 iteration 0186/0188: training loss 0.695; learning rate 0.000235
Epoch 16 validation pixAcc: 0.336, mIoU: 0.160
Epoch 17 iteration 0001/0187: training loss 0.634; learning rate 0.000235
Epoch 17 iteration 0002/0187: training loss 0.686; learning rate 0.000235
Epoch 17 iteration 0003/0187: training loss 0.677; learning rate 0.000234
Epoch 17 iteration 0004/0187: training loss 0.656; learning rate 0.000234
Epoch 17 iteration 0005/0187: training loss 0.664; learning rate 0.000234
Epoch 17 iteration 0006/0187: training loss 0.669; learning rate 0.000234
Epoch 17 iteration 0007/0187: training loss 0.709; learning rate 0.000234
Epoch 17 iteration 0008/0187: training loss 0.703; learning rate 0.000234
Epoch 17 iteration 0009/0187: training loss 0.707; learning rate 0.000234
Epoch 17 iteration 0010/0187: training loss 0.766; learning rate 0.000234
Epoch 17 iteration 0011/0187: training loss 0.771; learning rate 0.000234
Epoch 17 iteration 0012/0187: training loss 0.768; learning rate 0.000234
Epoch 17 iteration 0013/0187: training loss 0.748; learning rate 0.000234
Epoch 17 iteration 0014/0187: training loss 0.742; learning rate 0.000234
Epoch 17 iteration 0015/0187: training loss 0.720; learning rate 0.000233
Epoch 17 iteration 0016/0187: training loss 0.735; learning rate 0.000233
Epoch 17 iteration 0017/0187: training loss 0.733; learning rate 0.000233
Epoch 17 iteration 0018/0187: training loss 0.721; learning rate 0.000233
Epoch 17 iteration 0019/0187: training loss 0.721; learning rate 0.000233
Epoch 17 iteration 0020/0187: training loss 0.710; learning rate 0.000233
Epoch 17 iteration 0021/0187: training loss 0.707; learning rate 0.000233
Epoch 17 iteration 0022/0187: training loss 0.702; learning rate 0.000233
Epoch 17 iteration 0023/0187: training loss 0.710; learning rate 0.000233
Epoch 17 iteration 0024/0187: training loss 0.714; learning rate 0.000233
Epoch 17 iteration 0025/0187: training loss 0.710; learning rate 0.000233
Epoch 17 iteration 0026/0187: training loss 0.703; learning rate 0.000232
Epoch 17 iteration 0027/0187: training loss 0.701; learning rate 0.000232
Epoch 17 iteration 0028/0187: training loss 0.700; learning rate 0.000232
Epoch 17 iteration 0029/0187: training loss 0.693; learning rate 0.000232
Epoch 17 iteration 0030/0187: training loss 0.692; learning rate 0.000232
Epoch 17 iteration 0031/0187: training loss 0.691; learning rate 0.000232
Epoch 17 iteration 0032/0187: training loss 0.693; learning rate 0.000232
Epoch 17 iteration 0033/0187: training loss 0.692; learning rate 0.000232
Epoch 17 iteration 0034/0187: training loss 0.687; learning rate 0.000232
Epoch 17 iteration 0035/0187: training loss 0.690; learning rate 0.000232
Epoch 17 iteration 0036/0187: training loss 0.688; learning rate 0.000232
Epoch 17 iteration 0037/0187: training loss 0.688; learning rate 0.000231
Epoch 17 iteration 0038/0187: training loss 0.690; learning rate 0.000231
Epoch 17 iteration 0039/0187: training loss 0.690; learning rate 0.000231
Epoch 17 iteration 0040/0187: training loss 0.692; learning rate 0.000231
Epoch 17 iteration 0041/0187: training loss 0.691; learning rate 0.000231
Epoch 17 iteration 0042/0187: training loss 0.685; learning rate 0.000231
Epoch 17 iteration 0043/0187: training loss 0.684; learning rate 0.000231
Epoch 17 iteration 0044/0187: training loss 0.683; learning rate 0.000231
Epoch 17 iteration 0045/0187: training loss 0.682; learning rate 0.000231
Epoch 17 iteration 0046/0187: training loss 0.682; learning rate 0.000231
Epoch 17 iteration 0047/0187: training loss 0.683; learning rate 0.000231
Epoch 17 iteration 0048/0187: training loss 0.678; learning rate 0.000231
Epoch 17 iteration 0049/0187: training loss 0.676; learning rate 0.000230
Epoch 17 iteration 0050/0187: training loss 0.673; learning rate 0.000230
Epoch 17 iteration 0051/0187: training loss 0.675; learning rate 0.000230
Epoch 17 iteration 0052/0187: training loss 0.673; learning rate 0.000230
Epoch 17 iteration 0053/0187: training loss 0.672; learning rate 0.000230
Epoch 17 iteration 0054/0187: training loss 0.676; learning rate 0.000230
Epoch 17 iteration 0055/0187: training loss 0.674; learning rate 0.000230
Epoch 17 iteration 0056/0187: training loss 0.677; learning rate 0.000230
Epoch 17 iteration 0057/0187: training loss 0.679; learning rate 0.000230
Epoch 17 iteration 0058/0187: training loss 0.679; learning rate 0.000230
Epoch 17 iteration 0059/0187: training loss 0.677; learning rate 0.000230
Epoch 17 iteration 0060/0187: training loss 0.678; learning rate 0.000229
Epoch 17 iteration 0061/0187: training loss 0.679; learning rate 0.000229
Epoch 17 iteration 0062/0187: training loss 0.677; learning rate 0.000229
Epoch 17 iteration 0063/0187: training loss 0.680; learning rate 0.000229
Epoch 17 iteration 0064/0187: training loss 0.678; learning rate 0.000229
Epoch 17 iteration 0065/0187: training loss 0.678; learning rate 0.000229
Epoch 17 iteration 0066/0187: training loss 0.679; learning rate 0.000229
Epoch 17 iteration 0067/0187: training loss 0.678; learning rate 0.000229
Epoch 17 iteration 0068/0187: training loss 0.675; learning rate 0.000229
Epoch 17 iteration 0069/0187: training loss 0.675; learning rate 0.000229
Epoch 17 iteration 0070/0187: training loss 0.676; learning rate 0.000229
Epoch 17 iteration 0071/0187: training loss 0.675; learning rate 0.000229
Epoch 17 iteration 0072/0187: training loss 0.674; learning rate 0.000228
Epoch 17 iteration 0073/0187: training loss 0.676; learning rate 0.000228
Epoch 17 iteration 0074/0187: training loss 0.676; learning rate 0.000228
Epoch 17 iteration 0075/0187: training loss 0.676; learning rate 0.000228
Epoch 17 iteration 0076/0187: training loss 0.677; learning rate 0.000228
Epoch 17 iteration 0077/0187: training loss 0.679; learning rate 0.000228
Epoch 17 iteration 0078/0187: training loss 0.679; learning rate 0.000228
Epoch 17 iteration 0079/0187: training loss 0.680; learning rate 0.000228
Epoch 17 iteration 0080/0187: training loss 0.681; learning rate 0.000228
Epoch 17 iteration 0081/0187: training loss 0.679; learning rate 0.000228
Epoch 17 iteration 0082/0187: training loss 0.676; learning rate 0.000228
Epoch 17 iteration 0083/0187: training loss 0.676; learning rate 0.000227
Epoch 17 iteration 0084/0187: training loss 0.675; learning rate 0.000227
Epoch 17 iteration 0085/0187: training loss 0.673; learning rate 0.000227
Epoch 17 iteration 0086/0187: training loss 0.672; learning rate 0.000227
Epoch 17 iteration 0087/0187: training loss 0.672; learning rate 0.000227
Epoch 17 iteration 0088/0187: training loss 0.670; learning rate 0.000227
Epoch 17 iteration 0089/0187: training loss 0.669; learning rate 0.000227
Epoch 17 iteration 0090/0187: training loss 0.669; learning rate 0.000227
Epoch 17 iteration 0091/0187: training loss 0.668; learning rate 0.000227
Epoch 17 iteration 0092/0187: training loss 0.667; learning rate 0.000227
Epoch 17 iteration 0093/0187: training loss 0.666; learning rate 0.000227
Epoch 17 iteration 0094/0187: training loss 0.665; learning rate 0.000227
Epoch 17 iteration 0095/0187: training loss 0.664; learning rate 0.000226
Epoch 17 iteration 0096/0187: training loss 0.664; learning rate 0.000226
Epoch 17 iteration 0097/0187: training loss 0.668; learning rate 0.000226
Epoch 17 iteration 0098/0187: training loss 0.670; learning rate 0.000226
Epoch 17 iteration 0099/0187: training loss 0.668; learning rate 0.000226
Epoch 17 iteration 0100/0187: training loss 0.669; learning rate 0.000226
Epoch 17 iteration 0101/0187: training loss 0.667; learning rate 0.000226
Epoch 17 iteration 0102/0187: training loss 0.667; learning rate 0.000226
Epoch 17 iteration 0103/0187: training loss 0.666; learning rate 0.000226
Epoch 17 iteration 0104/0187: training loss 0.666; learning rate 0.000226
Epoch 17 iteration 0105/0187: training loss 0.664; learning rate 0.000226
Epoch 17 iteration 0106/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0107/0187: training loss 0.669; learning rate 0.000225
Epoch 17 iteration 0108/0187: training loss 0.667; learning rate 0.000225
Epoch 17 iteration 0109/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0110/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0111/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0112/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0113/0187: training loss 0.668; learning rate 0.000225
Epoch 17 iteration 0114/0187: training loss 0.669; learning rate 0.000225
Epoch 17 iteration 0115/0187: training loss 0.669; learning rate 0.000225
Epoch 17 iteration 0116/0187: training loss 0.669; learning rate 0.000225
Epoch 17 iteration 0117/0187: training loss 0.668; learning rate 0.000224
Epoch 17 iteration 0118/0187: training loss 0.667; learning rate 0.000224
Epoch 17 iteration 0119/0187: training loss 0.667; learning rate 0.000224
Epoch 17 iteration 0120/0187: training loss 0.668; learning rate 0.000224
Epoch 17 iteration 0121/0187: training loss 0.668; learning rate 0.000224
Epoch 17 iteration 0122/0187: training loss 0.670; learning rate 0.000224
Epoch 17 iteration 0123/0187: training loss 0.669; learning rate 0.000224
Epoch 17 iteration 0124/0187: training loss 0.669; learning rate 0.000224
Epoch 17 iteration 0125/0187: training loss 0.670; learning rate 0.000224
Epoch 17 iteration 0126/0187: training loss 0.670; learning rate 0.000224
Epoch 17 iteration 0127/0187: training loss 0.670; learning rate 0.000224
Epoch 17 iteration 0128/0187: training loss 0.673; learning rate 0.000224
Epoch 17 iteration 0129/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0130/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0131/0187: training loss 0.673; learning rate 0.000223
Epoch 17 iteration 0132/0187: training loss 0.671; learning rate 0.000223
Epoch 17 iteration 0133/0187: training loss 0.671; learning rate 0.000223
Epoch 17 iteration 0134/0187: training loss 0.671; learning rate 0.000223
Epoch 17 iteration 0135/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0136/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0137/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0138/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0139/0187: training loss 0.672; learning rate 0.000223
Epoch 17 iteration 0140/0187: training loss 0.673; learning rate 0.000222
Epoch 17 iteration 0141/0187: training loss 0.674; learning rate 0.000222
Epoch 17 iteration 0142/0187: training loss 0.674; learning rate 0.000222
Epoch 17 iteration 0143/0187: training loss 0.675; learning rate 0.000222
Epoch 17 iteration 0144/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0145/0187: training loss 0.677; learning rate 0.000222
Epoch 17 iteration 0146/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0147/0187: training loss 0.677; learning rate 0.000222
Epoch 17 iteration 0148/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0149/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0150/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0151/0187: training loss 0.676; learning rate 0.000222
Epoch 17 iteration 0152/0187: training loss 0.678; learning rate 0.000221
Epoch 17 iteration 0153/0187: training loss 0.677; learning rate 0.000221
Epoch 17 iteration 0154/0187: training loss 0.676; learning rate 0.000221
Epoch 17 iteration 0155/0187: training loss 0.679; learning rate 0.000221
Epoch 17 iteration 0156/0187: training loss 0.679; learning rate 0.000221
Epoch 17 iteration 0157/0187: training loss 0.678; learning rate 0.000221
Epoch 17 iteration 0158/0187: training loss 0.679; learning rate 0.000221
Epoch 17 iteration 0159/0187: training loss 0.680; learning rate 0.000221
Epoch 17 iteration 0160/0187: training loss 0.681; learning rate 0.000221
Epoch 17 iteration 0161/0187: training loss 0.681; learning rate 0.000221
Epoch 17 iteration 0162/0187: training loss 0.681; learning rate 0.000221
Epoch 17 iteration 0163/0187: training loss 0.681; learning rate 0.000220
Epoch 17 iteration 0164/0187: training loss 0.682; learning rate 0.000220
Epoch 17 iteration 0165/0187: training loss 0.682; learning rate 0.000220
Epoch 17 iteration 0166/0187: training loss 0.681; learning rate 0.000220
Epoch 17 iteration 0167/0187: training loss 0.681; learning rate 0.000220
Epoch 17 iteration 0168/0187: training loss 0.681; learning rate 0.000220
Epoch 17 iteration 0169/0187: training loss 0.681; learning rate 0.000220
Epoch 17 iteration 0170/0187: training loss 0.682; learning rate 0.000220
Epoch 17 iteration 0171/0187: training loss 0.682; learning rate 0.000220
Epoch 17 iteration 0172/0187: training loss 0.682; learning rate 0.000220
Epoch 17 iteration 0173/0187: training loss 0.683; learning rate 0.000220
Epoch 17 iteration 0174/0187: training loss 0.683; learning rate 0.000219
Epoch 17 iteration 0175/0187: training loss 0.682; learning rate 0.000219
Epoch 17 iteration 0176/0187: training loss 0.683; learning rate 0.000219
Epoch 17 iteration 0177/0187: training loss 0.683; learning rate 0.000219
Epoch 17 iteration 0178/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0179/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0180/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0181/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0182/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0183/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0184/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0185/0187: training loss 0.684; learning rate 0.000219
Epoch 17 iteration 0186/0187: training loss 0.684; learning rate 0.000218
Epoch 17 iteration 0187/0187: training loss 0.683; learning rate 0.000218
Epoch 17 validation pixAcc: 0.335, mIoU: 0.167
Epoch 18 iteration 0001/0187: training loss 0.853; learning rate 0.000218
Epoch 18 iteration 0002/0187: training loss 0.789; learning rate 0.000218
Epoch 18 iteration 0003/0187: training loss 0.729; learning rate 0.000218
Epoch 18 iteration 0004/0187: training loss 0.769; learning rate 0.000218
Epoch 18 iteration 0005/0187: training loss 0.736; learning rate 0.000218
Epoch 18 iteration 0006/0187: training loss 0.732; learning rate 0.000218
Epoch 18 iteration 0007/0187: training loss 0.728; learning rate 0.000218
Epoch 18 iteration 0008/0187: training loss 0.716; learning rate 0.000218
Epoch 18 iteration 0009/0187: training loss 0.692; learning rate 0.000217
Epoch 18 iteration 0010/0187: training loss 0.677; learning rate 0.000217
Epoch 18 iteration 0011/0187: training loss 0.669; learning rate 0.000217
Epoch 18 iteration 0012/0187: training loss 0.657; learning rate 0.000217
Epoch 18 iteration 0013/0187: training loss 0.672; learning rate 0.000217
Epoch 18 iteration 0014/0187: training loss 0.686; learning rate 0.000217
Epoch 18 iteration 0015/0187: training loss 0.701; learning rate 0.000217
Epoch 18 iteration 0016/0187: training loss 0.730; learning rate 0.000217
Epoch 18 iteration 0017/0187: training loss 0.726; learning rate 0.000217
Epoch 18 iteration 0018/0187: training loss 0.726; learning rate 0.000217
Epoch 18 iteration 0019/0187: training loss 0.720; learning rate 0.000217
Epoch 18 iteration 0020/0187: training loss 0.716; learning rate 0.000216
Epoch 18 iteration 0021/0187: training loss 0.714; learning rate 0.000216
Epoch 18 iteration 0022/0187: training loss 0.717; learning rate 0.000216
Epoch 18 iteration 0023/0187: training loss 0.709; learning rate 0.000216
Epoch 18 iteration 0024/0187: training loss 0.726; learning rate 0.000216
Epoch 18 iteration 0025/0187: training loss 0.719; learning rate 0.000216
Epoch 18 iteration 0026/0187: training loss 0.722; learning rate 0.000216
Epoch 18 iteration 0027/0187: training loss 0.725; learning rate 0.000216
Epoch 18 iteration 0028/0187: training loss 0.719; learning rate 0.000216
Epoch 18 iteration 0029/0187: training loss 0.712; learning rate 0.000216
Epoch 18 iteration 0030/0187: training loss 0.707; learning rate 0.000216
Epoch 18 iteration 0031/0187: training loss 0.714; learning rate 0.000216
Epoch 18 iteration 0032/0187: training loss 0.721; learning rate 0.000215
Epoch 18 iteration 0033/0187: training loss 0.716; learning rate 0.000215
Epoch 18 iteration 0034/0187: training loss 0.718; learning rate 0.000215
Epoch 18 iteration 0035/0187: training loss 0.718; learning rate 0.000215
Epoch 18 iteration 0036/0187: training loss 0.711; learning rate 0.000215
Epoch 18 iteration 0037/0187: training loss 0.712; learning rate 0.000215
Epoch 18 iteration 0038/0187: training loss 0.713; learning rate 0.000215
Epoch 18 iteration 0039/0187: training loss 0.724; learning rate 0.000215
Epoch 18 iteration 0040/0187: training loss 0.726; learning rate 0.000215
Epoch 18 iteration 0041/0187: training loss 0.733; learning rate 0.000215
Epoch 18 iteration 0042/0187: training loss 0.729; learning rate 0.000215
Epoch 18 iteration 0043/0187: training loss 0.723; learning rate 0.000214
Epoch 18 iteration 0044/0187: training loss 0.722; learning rate 0.000214
Epoch 18 iteration 0045/0187: training loss 0.723; learning rate 0.000214
Epoch 18 iteration 0046/0187: training loss 0.723; learning rate 0.000214
Epoch 18 iteration 0047/0187: training loss 0.729; learning rate 0.000214
Epoch 18 iteration 0048/0187: training loss 0.728; learning rate 0.000214
Epoch 18 iteration 0049/0187: training loss 0.723; learning rate 0.000214
Epoch 18 iteration 0050/0187: training loss 0.720; learning rate 0.000214
Epoch 18 iteration 0051/0187: training loss 0.718; learning rate 0.000214
Epoch 18 iteration 0052/0187: training loss 0.715; learning rate 0.000214
Epoch 18 iteration 0053/0187: training loss 0.718; learning rate 0.000214
Epoch 18 iteration 0054/0187: training loss 0.714; learning rate 0.000214
Epoch 18 iteration 0055/0187: training loss 0.719; learning rate 0.000213
Epoch 18 iteration 0056/0187: training loss 0.721; learning rate 0.000213
Epoch 18 iteration 0057/0187: training loss 0.719; learning rate 0.000213
Epoch 18 iteration 0058/0187: training loss 0.720; learning rate 0.000213
Epoch 18 iteration 0059/0187: training loss 0.718; learning rate 0.000213
Epoch 18 iteration 0060/0187: training loss 0.719; learning rate 0.000213
Epoch 18 iteration 0061/0187: training loss 0.714; learning rate 0.000213
Epoch 18 iteration 0062/0187: training loss 0.710; learning rate 0.000213
Epoch 18 iteration 0063/0187: training loss 0.708; learning rate 0.000213
Epoch 18 iteration 0064/0187: training loss 0.708; learning rate 0.000213
Epoch 18 iteration 0065/0187: training loss 0.707; learning rate 0.000213
Epoch 18 iteration 0066/0187: training loss 0.703; learning rate 0.000212
Epoch 18 iteration 0067/0187: training loss 0.703; learning rate 0.000212
Epoch 18 iteration 0068/0187: training loss 0.701; learning rate 0.000212
Epoch 18 iteration 0069/0187: training loss 0.700; learning rate 0.000212
Epoch 18 iteration 0070/0187: training loss 0.702; learning rate 0.000212
Epoch 18 iteration 0071/0187: training loss 0.701; learning rate 0.000212
Epoch 18 iteration 0072/0187: training loss 0.701; learning rate 0.000212
Epoch 18 iteration 0073/0187: training loss 0.703; learning rate 0.000212
Epoch 18 iteration 0074/0187: training loss 0.701; learning rate 0.000212
Epoch 18 iteration 0075/0187: training loss 0.703; learning rate 0.000212
Epoch 18 iteration 0076/0187: training loss 0.703; learning rate 0.000212
Epoch 18 iteration 0077/0187: training loss 0.701; learning rate 0.000211
Epoch 18 iteration 0078/0187: training loss 0.702; learning rate 0.000211
Epoch 18 iteration 0079/0187: training loss 0.703; learning rate 0.000211
Epoch 18 iteration 0080/0187: training loss 0.706; learning rate 0.000211
Epoch 18 iteration 0081/0187: training loss 0.705; learning rate 0.000211
Epoch 18 iteration 0082/0187: training loss 0.707; learning rate 0.000211
Epoch 18 iteration 0083/0187: training loss 0.708; learning rate 0.000211
Epoch 18 iteration 0084/0187: training loss 0.709; learning rate 0.000211
Epoch 18 iteration 0085/0187: training loss 0.710; learning rate 0.000211
Epoch 18 iteration 0086/0187: training loss 0.709; learning rate 0.000211
Epoch 18 iteration 0087/0187: training loss 0.709; learning rate 0.000211
Epoch 18 iteration 0088/0187: training loss 0.709; learning rate 0.000211
Epoch 18 iteration 0089/0187: training loss 0.708; learning rate 0.000210
Epoch 18 iteration 0090/0187: training loss 0.707; learning rate 0.000210
Epoch 18 iteration 0091/0188: training loss 0.707; learning rate 0.000210
Epoch 18 iteration 0092/0188: training loss 0.707; learning rate 0.000210
Epoch 18 iteration 0093/0188: training loss 0.707; learning rate 0.000210
Epoch 18 iteration 0094/0188: training loss 0.706; learning rate 0.000210
Epoch 18 iteration 0095/0188: training loss 0.708; learning rate 0.000210
Epoch 18 iteration 0096/0188: training loss 0.710; learning rate 0.000210
Epoch 18 iteration 0097/0188: training loss 0.711; learning rate 0.000210
Epoch 18 iteration 0098/0188: training loss 0.711; learning rate 0.000210
Epoch 18 iteration 0099/0188: training loss 0.711; learning rate 0.000210
Epoch 18 iteration 0100/0188: training loss 0.711; learning rate 0.000209
Epoch 18 iteration 0101/0188: training loss 0.712; learning rate 0.000209
Epoch 18 iteration 0102/0188: training loss 0.713; learning rate 0.000209
Epoch 18 iteration 0103/0188: training loss 0.712; learning rate 0.000209
Epoch 18 iteration 0104/0188: training loss 0.710; learning rate 0.000209
Epoch 18 iteration 0105/0188: training loss 0.710; learning rate 0.000209
Epoch 18 iteration 0106/0188: training loss 0.710; learning rate 0.000209
Epoch 18 iteration 0107/0188: training loss 0.710; learning rate 0.000209
Epoch 18 iteration 0108/0188: training loss 0.711; learning rate 0.000209
Epoch 18 iteration 0109/0188: training loss 0.713; learning rate 0.000209
Epoch 18 iteration 0110/0188: training loss 0.714; learning rate 0.000209
Epoch 18 iteration 0111/0188: training loss 0.715; learning rate 0.000208
Epoch 18 iteration 0112/0188: training loss 0.714; learning rate 0.000208
Epoch 18 iteration 0113/0188: training loss 0.713; learning rate 0.000208
Epoch 18 iteration 0114/0188: training loss 0.712; learning rate 0.000208
Epoch 18 iteration 0115/0188: training loss 0.713; learning rate 0.000208
Epoch 18 iteration 0116/0188: training loss 0.713; learning rate 0.000208
Epoch 18 iteration 0117/0188: training loss 0.711; learning rate 0.000208
Epoch 18 iteration 0118/0188: training loss 0.713; learning rate 0.000208
Epoch 18 iteration 0119/0188: training loss 0.714; learning rate 0.000208
Epoch 18 iteration 0120/0188: training loss 0.715; learning rate 0.000208
Epoch 18 iteration 0121/0188: training loss 0.714; learning rate 0.000208
Epoch 18 iteration 0122/0188: training loss 0.716; learning rate 0.000207
Epoch 18 iteration 0123/0188: training loss 0.716; learning rate 0.000207
Epoch 18 iteration 0124/0188: training loss 0.716; learning rate 0.000207
Epoch 18 iteration 0125/0188: training loss 0.715; learning rate 0.000207
Epoch 18 iteration 0126/0188: training loss 0.715; learning rate 0.000207
Epoch 18 iteration 0127/0188: training loss 0.713; learning rate 0.000207
Epoch 18 iteration 0128/0188: training loss 0.714; learning rate 0.000207
Epoch 18 iteration 0129/0188: training loss 0.713; learning rate 0.000207
Epoch 18 iteration 0130/0188: training loss 0.712; learning rate 0.000207
Epoch 18 iteration 0131/0188: training loss 0.712; learning rate 0.000207
Epoch 18 iteration 0132/0188: training loss 0.712; learning rate 0.000207
Epoch 18 iteration 0133/0188: training loss 0.711; learning rate 0.000207
Epoch 18 iteration 0134/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0135/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0136/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0137/0188: training loss 0.711; learning rate 0.000206
Epoch 18 iteration 0138/0188: training loss 0.711; learning rate 0.000206
Epoch 18 iteration 0139/0188: training loss 0.711; learning rate 0.000206
Epoch 18 iteration 0140/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0141/0188: training loss 0.710; learning rate 0.000206
Epoch 18 iteration 0142/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0143/0188: training loss 0.712; learning rate 0.000206
Epoch 18 iteration 0144/0188: training loss 0.711; learning rate 0.000206
Epoch 18 iteration 0145/0188: training loss 0.710; learning rate 0.000205
Epoch 18 iteration 0146/0188: training loss 0.710; learning rate 0.000205
Epoch 18 iteration 0147/0188: training loss 0.708; learning rate 0.000205
Epoch 18 iteration 0148/0188: training loss 0.710; learning rate 0.000205
Epoch 18 iteration 0149/0188: training loss 0.709; learning rate 0.000205
Epoch 18 iteration 0150/0188: training loss 0.710; learning rate 0.000205
Epoch 18 iteration 0151/0188: training loss 0.708; learning rate 0.000205
Epoch 18 iteration 0152/0188: training loss 0.708; learning rate 0.000205
Epoch 18 iteration 0153/0188: training loss 0.708; learning rate 0.000205
Epoch 18 iteration 0154/0188: training loss 0.707; learning rate 0.000205
Epoch 18 iteration 0155/0188: training loss 0.706; learning rate 0.000205
Epoch 18 iteration 0156/0188: training loss 0.705; learning rate 0.000204
Epoch 18 iteration 0157/0188: training loss 0.708; learning rate 0.000204
Epoch 18 iteration 0158/0188: training loss 0.709; learning rate 0.000204
Epoch 18 iteration 0159/0188: training loss 0.711; learning rate 0.000204
Epoch 18 iteration 0160/0188: training loss 0.711; learning rate 0.000204
Epoch 18 iteration 0161/0188: training loss 0.711; learning rate 0.000204
Epoch 18 iteration 0162/0188: training loss 0.710; learning rate 0.000204
Epoch 18 iteration 0163/0188: training loss 0.713; learning rate 0.000204
Epoch 18 iteration 0164/0188: training loss 0.712; learning rate 0.000204
Epoch 18 iteration 0165/0188: training loss 0.712; learning rate 0.000204
Epoch 18 iteration 0166/0188: training loss 0.711; learning rate 0.000204
Epoch 18 iteration 0167/0188: training loss 0.712; learning rate 0.000204
Epoch 18 iteration 0168/0188: training loss 0.713; learning rate 0.000203
Epoch 18 iteration 0169/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0170/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0171/0188: training loss 0.715; learning rate 0.000203
Epoch 18 iteration 0172/0188: training loss 0.715; learning rate 0.000203
Epoch 18 iteration 0173/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0174/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0175/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0176/0188: training loss 0.714; learning rate 0.000203
Epoch 18 iteration 0177/0188: training loss 0.713; learning rate 0.000203
Epoch 18 iteration 0178/0188: training loss 0.713; learning rate 0.000203
Epoch 18 iteration 0179/0188: training loss 0.712; learning rate 0.000202
Epoch 18 iteration 0180/0188: training loss 0.711; learning rate 0.000202
Epoch 18 iteration 0181/0188: training loss 0.712; learning rate 0.000202
Epoch 18 iteration 0182/0188: training loss 0.711; learning rate 0.000202
Epoch 18 iteration 0183/0188: training loss 0.710; learning rate 0.000202
Epoch 18 iteration 0184/0188: training loss 0.712; learning rate 0.000202
Epoch 18 iteration 0185/0188: training loss 0.713; learning rate 0.000202
Epoch 18 iteration 0186/0188: training loss 0.715; learning rate 0.000202
Epoch 18 validation pixAcc: 0.331, mIoU: 0.164
Epoch 19 iteration 0001/0187: training loss 0.631; learning rate 0.000202
Epoch 19 iteration 0002/0187: training loss 0.636; learning rate 0.000202
Epoch 19 iteration 0003/0187: training loss 0.607; learning rate 0.000201
Epoch 19 iteration 0004/0187: training loss 0.602; learning rate 0.000201
Epoch 19 iteration 0005/0187: training loss 0.621; learning rate 0.000201
Epoch 19 iteration 0006/0187: training loss 0.653; learning rate 0.000201
Epoch 19 iteration 0007/0187: training loss 0.680; learning rate 0.000201
Epoch 19 iteration 0008/0187: training loss 0.681; learning rate 0.000201
Epoch 19 iteration 0009/0187: training loss 0.693; learning rate 0.000201
Epoch 19 iteration 0010/0187: training loss 0.683; learning rate 0.000201
Epoch 19 iteration 0011/0187: training loss 0.693; learning rate 0.000201
Epoch 19 iteration 0012/0187: training loss 0.709; learning rate 0.000201
Epoch 19 iteration 0013/0187: training loss 0.726; learning rate 0.000201
Epoch 19 iteration 0014/0187: training loss 0.728; learning rate 0.000200
Epoch 19 iteration 0015/0187: training loss 0.732; learning rate 0.000200
Epoch 19 iteration 0016/0187: training loss 0.732; learning rate 0.000200
Epoch 19 iteration 0017/0187: training loss 0.719; learning rate 0.000200
Epoch 19 iteration 0018/0187: training loss 0.721; learning rate 0.000200
Epoch 19 iteration 0019/0187: training loss 0.712; learning rate 0.000200
Epoch 19 iteration 0020/0187: training loss 0.721; learning rate 0.000200
Epoch 19 iteration 0021/0187: training loss 0.721; learning rate 0.000200
Epoch 19 iteration 0022/0187: training loss 0.712; learning rate 0.000200
Epoch 19 iteration 0023/0187: training loss 0.714; learning rate 0.000200
Epoch 19 iteration 0024/0187: training loss 0.715; learning rate 0.000200
Epoch 19 iteration 0025/0187: training loss 0.712; learning rate 0.000200
Epoch 19 iteration 0026/0187: training loss 0.711; learning rate 0.000199
Epoch 19 iteration 0027/0187: training loss 0.709; learning rate 0.000199
Epoch 19 iteration 0028/0187: training loss 0.713; learning rate 0.000199
Epoch 19 iteration 0029/0187: training loss 0.705; learning rate 0.000199
Epoch 19 iteration 0030/0187: training loss 0.706; learning rate 0.000199
Epoch 19 iteration 0031/0187: training loss 0.711; learning rate 0.000199
Epoch 19 iteration 0032/0187: training loss 0.707; learning rate 0.000199
Epoch 19 iteration 0033/0187: training loss 0.699; learning rate 0.000199
Epoch 19 iteration 0034/0187: training loss 0.699; learning rate 0.000199
Epoch 19 iteration 0035/0187: training loss 0.693; learning rate 0.000199
Epoch 19 iteration 0036/0187: training loss 0.689; learning rate 0.000199
Epoch 19 iteration 0037/0187: training loss 0.698; learning rate 0.000198
Epoch 19 iteration 0038/0187: training loss 0.697; learning rate 0.000198
Epoch 19 iteration 0039/0187: training loss 0.697; learning rate 0.000198
Epoch 19 iteration 0040/0187: training loss 0.696; learning rate 0.000198
Epoch 19 iteration 0041/0187: training loss 0.695; learning rate 0.000198
Epoch 19 iteration 0042/0187: training loss 0.699; learning rate 0.000198
Epoch 19 iteration 0043/0187: training loss 0.696; learning rate 0.000198
Epoch 19 iteration 0044/0187: training loss 0.695; learning rate 0.000198
Epoch 19 iteration 0045/0187: training loss 0.695; learning rate 0.000198
Epoch 19 iteration 0046/0187: training loss 0.696; learning rate 0.000198
Epoch 19 iteration 0047/0187: training loss 0.700; learning rate 0.000198
Epoch 19 iteration 0048/0187: training loss 0.695; learning rate 0.000197
Epoch 19 iteration 0049/0187: training loss 0.692; learning rate 0.000197
Epoch 19 iteration 0050/0187: training loss 0.693; learning rate 0.000197
Epoch 19 iteration 0051/0187: training loss 0.692; learning rate 0.000197
Epoch 19 iteration 0052/0187: training loss 0.703; learning rate 0.000197
Epoch 19 iteration 0053/0187: training loss 0.702; learning rate 0.000197
Epoch 19 iteration 0054/0187: training loss 0.699; learning rate 0.000197
Epoch 19 iteration 0055/0187: training loss 0.697; learning rate 0.000197
Epoch 19 iteration 0056/0187: training loss 0.698; learning rate 0.000197
Epoch 19 iteration 0057/0187: training loss 0.699; learning rate 0.000197
Epoch 19 iteration 0058/0187: training loss 0.701; learning rate 0.000197
Epoch 19 iteration 0059/0187: training loss 0.709; learning rate 0.000196
Epoch 19 iteration 0060/0187: training loss 0.711; learning rate 0.000196
Epoch 19 iteration 0061/0187: training loss 0.717; learning rate 0.000196
Epoch 19 iteration 0062/0187: training loss 0.715; learning rate 0.000196
Epoch 19 iteration 0063/0187: training loss 0.714; learning rate 0.000196
Epoch 19 iteration 0064/0187: training loss 0.713; learning rate 0.000196
Epoch 19 iteration 0065/0187: training loss 0.712; learning rate 0.000196
Epoch 19 iteration 0066/0187: training loss 0.712; learning rate 0.000196
Epoch 19 iteration 0067/0187: training loss 0.713; learning rate 0.000196
Epoch 19 iteration 0068/0187: training loss 0.717; learning rate 0.000196
Epoch 19 iteration 0069/0187: training loss 0.716; learning rate 0.000196
Epoch 19 iteration 0070/0187: training loss 0.715; learning rate 0.000196
Epoch 19 iteration 0071/0187: training loss 0.713; learning rate 0.000195
Epoch 19 iteration 0072/0187: training loss 0.716; learning rate 0.000195
Epoch 19 iteration 0073/0187: training loss 0.714; learning rate 0.000195
Epoch 19 iteration 0074/0187: training loss 0.713; learning rate 0.000195
Epoch 19 iteration 0075/0187: training loss 0.710; learning rate 0.000195
Epoch 19 iteration 0076/0187: training loss 0.710; learning rate 0.000195
Epoch 19 iteration 0077/0187: training loss 0.708; learning rate 0.000195
Epoch 19 iteration 0078/0187: training loss 0.706; learning rate 0.000195
Epoch 19 iteration 0079/0187: training loss 0.706; learning rate 0.000195
Epoch 19 iteration 0080/0187: training loss 0.704; learning rate 0.000195
Epoch 19 iteration 0081/0187: training loss 0.704; learning rate 0.000195
Epoch 19 iteration 0082/0187: training loss 0.704; learning rate 0.000194
Epoch 19 iteration 0083/0187: training loss 0.702; learning rate 0.000194
Epoch 19 iteration 0084/0187: training loss 0.704; learning rate 0.000194
Epoch 19 iteration 0085/0187: training loss 0.701; learning rate 0.000194
Epoch 19 iteration 0086/0187: training loss 0.702; learning rate 0.000194
Epoch 19 iteration 0087/0187: training loss 0.706; learning rate 0.000194
Epoch 19 iteration 0088/0187: training loss 0.704; learning rate 0.000194
Epoch 19 iteration 0089/0187: training loss 0.705; learning rate 0.000194
Epoch 19 iteration 0090/0187: training loss 0.708; learning rate 0.000194
Epoch 19 iteration 0091/0187: training loss 0.707; learning rate 0.000194
Epoch 19 iteration 0092/0187: training loss 0.706; learning rate 0.000194
Epoch 19 iteration 0093/0187: training loss 0.705; learning rate 0.000193
Epoch 19 iteration 0094/0187: training loss 0.704; learning rate 0.000193
Epoch 19 iteration 0095/0187: training loss 0.704; learning rate 0.000193
Epoch 19 iteration 0096/0187: training loss 0.705; learning rate 0.000193
Epoch 19 iteration 0097/0187: training loss 0.706; learning rate 0.000193
Epoch 19 iteration 0098/0187: training loss 0.705; learning rate 0.000193
Epoch 19 iteration 0099/0187: training loss 0.704; learning rate 0.000193
Epoch 19 iteration 0100/0187: training loss 0.705; learning rate 0.000193
Epoch 19 iteration 0101/0187: training loss 0.705; learning rate 0.000193
Epoch 19 iteration 0102/0187: training loss 0.704; learning rate 0.000193
Epoch 19 iteration 0103/0187: training loss 0.703; learning rate 0.000193
Epoch 19 iteration 0104/0187: training loss 0.706; learning rate 0.000192
Epoch 19 iteration 0105/0187: training loss 0.705; learning rate 0.000192
Epoch 19 iteration 0106/0187: training loss 0.705; learning rate 0.000192
Epoch 19 iteration 0107/0187: training loss 0.704; learning rate 0.000192
Epoch 19 iteration 0108/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0109/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0110/0187: training loss 0.702; learning rate 0.000192
Epoch 19 iteration 0111/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0112/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0113/0187: training loss 0.702; learning rate 0.000192
Epoch 19 iteration 0114/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0115/0187: training loss 0.703; learning rate 0.000192
Epoch 19 iteration 0116/0187: training loss 0.704; learning rate 0.000191
Epoch 19 iteration 0117/0187: training loss 0.704; learning rate 0.000191
Epoch 19 iteration 0118/0187: training loss 0.703; learning rate 0.000191
Epoch 19 iteration 0119/0187: training loss 0.703; learning rate 0.000191
Epoch 19 iteration 0120/0187: training loss 0.704; learning rate 0.000191
Epoch 19 iteration 0121/0187: training loss 0.704; learning rate 0.000191
Epoch 19 iteration 0122/0187: training loss 0.706; learning rate 0.000191
Epoch 19 iteration 0123/0187: training loss 0.706; learning rate 0.000191
Epoch 19 iteration 0124/0187: training loss 0.707; learning rate 0.000191
Epoch 19 iteration 0125/0187: training loss 0.708; learning rate 0.000191
Epoch 19 iteration 0126/0187: training loss 0.706; learning rate 0.000191
Epoch 19 iteration 0127/0187: training loss 0.707; learning rate 0.000190
Epoch 19 iteration 0128/0187: training loss 0.706; learning rate 0.000190
Epoch 19 iteration 0129/0187: training loss 0.707; learning rate 0.000190
Epoch 19 iteration 0130/0187: training loss 0.706; learning rate 0.000190
Epoch 19 iteration 0131/0187: training loss 0.706; learning rate 0.000190
Epoch 19 iteration 0132/0187: training loss 0.705; learning rate 0.000190
Epoch 19 iteration 0133/0187: training loss 0.705; learning rate 0.000190
Epoch 19 iteration 0134/0187: training loss 0.706; learning rate 0.000190
Epoch 19 iteration 0135/0187: training loss 0.707; learning rate 0.000190
Epoch 19 iteration 0136/0187: training loss 0.707; learning rate 0.000190
Epoch 19 iteration 0137/0187: training loss 0.708; learning rate 0.000190
Epoch 19 iteration 0138/0187: training loss 0.709; learning rate 0.000189
Epoch 19 iteration 0139/0187: training loss 0.707; learning rate 0.000189
Epoch 19 iteration 0140/0187: training loss 0.707; learning rate 0.000189
Epoch 19 iteration 0141/0187: training loss 0.708; learning rate 0.000189
Epoch 19 iteration 0142/0187: training loss 0.707; learning rate 0.000189
Epoch 19 iteration 0143/0187: training loss 0.708; learning rate 0.000189
Epoch 19 iteration 0144/0187: training loss 0.709; learning rate 0.000189
Epoch 19 iteration 0145/0187: training loss 0.708; learning rate 0.000189
Epoch 19 iteration 0146/0187: training loss 0.709; learning rate 0.000189
Epoch 19 iteration 0147/0187: training loss 0.710; learning rate 0.000189
Epoch 19 iteration 0148/0187: training loss 0.710; learning rate 0.000189
Epoch 19 iteration 0149/0187: training loss 0.711; learning rate 0.000188
Epoch 19 iteration 0150/0187: training loss 0.712; learning rate 0.000188
Epoch 19 iteration 0151/0187: training loss 0.713; learning rate 0.000188
Epoch 19 iteration 0152/0187: training loss 0.715; learning rate 0.000188
Epoch 19 iteration 0153/0187: training loss 0.715; learning rate 0.000188
Epoch 19 iteration 0154/0187: training loss 0.715; learning rate 0.000188
Epoch 19 iteration 0155/0187: training loss 0.716; learning rate 0.000188
Epoch 19 iteration 0156/0187: training loss 0.717; learning rate 0.000188
Epoch 19 iteration 0157/0187: training loss 0.716; learning rate 0.000188
Epoch 19 iteration 0158/0187: training loss 0.717; learning rate 0.000188
Epoch 19 iteration 0159/0187: training loss 0.718; learning rate 0.000188
Epoch 19 iteration 0160/0187: training loss 0.717; learning rate 0.000187
Epoch 19 iteration 0161/0187: training loss 0.717; learning rate 0.000187
Epoch 19 iteration 0162/0187: training loss 0.717; learning rate 0.000187
Epoch 19 iteration 0163/0187: training loss 0.718; learning rate 0.000187
Epoch 19 iteration 0164/0187: training loss 0.717; learning rate 0.000187
Epoch 19 iteration 0165/0187: training loss 0.716; learning rate 0.000187
Epoch 19 iteration 0166/0187: training loss 0.717; learning rate 0.000187
Epoch 19 iteration 0167/0187: training loss 0.718; learning rate 0.000187
Epoch 19 iteration 0168/0187: training loss 0.719; learning rate 0.000187
Epoch 19 iteration 0169/0187: training loss 0.720; learning rate 0.000187
Epoch 19 iteration 0170/0187: training loss 0.721; learning rate 0.000187
Epoch 19 iteration 0171/0187: training loss 0.721; learning rate 0.000186
Epoch 19 iteration 0172/0187: training loss 0.721; learning rate 0.000186
Epoch 19 iteration 0173/0187: training loss 0.720; learning rate 0.000186
Epoch 19 iteration 0174/0187: training loss 0.720; learning rate 0.000186
Epoch 19 iteration 0175/0187: training loss 0.720; learning rate 0.000186
Epoch 19 iteration 0176/0187: training loss 0.720; learning rate 0.000186
Epoch 19 iteration 0177/0187: training loss 0.721; learning rate 0.000186
Epoch 19 iteration 0178/0187: training loss 0.721; learning rate 0.000186
Epoch 19 iteration 0179/0187: training loss 0.722; learning rate 0.000186
Epoch 19 iteration 0180/0187: training loss 0.723; learning rate 0.000186
Epoch 19 iteration 0181/0187: training loss 0.723; learning rate 0.000186
Epoch 19 iteration 0182/0187: training loss 0.723; learning rate 0.000186
Epoch 19 iteration 0183/0187: training loss 0.723; learning rate 0.000185
Epoch 19 iteration 0184/0187: training loss 0.724; learning rate 0.000185
Epoch 19 iteration 0185/0187: training loss 0.723; learning rate 0.000185
Epoch 19 iteration 0186/0187: training loss 0.723; learning rate 0.000185
Epoch 19 iteration 0187/0187: training loss 0.722; learning rate 0.000185
Epoch 19 validation pixAcc: 0.335, mIoU: 0.167
Epoch 20 iteration 0001/0187: training loss 0.686; learning rate 0.000185
Epoch 20 iteration 0002/0187: training loss 0.639; learning rate 0.000185
Epoch 20 iteration 0003/0187: training loss 0.633; learning rate 0.000185
Epoch 20 iteration 0004/0187: training loss 0.632; learning rate 0.000185
Epoch 20 iteration 0005/0187: training loss 0.633; learning rate 0.000185
Epoch 20 iteration 0006/0187: training loss 0.633; learning rate 0.000184
Epoch 20 iteration 0007/0187: training loss 0.638; learning rate 0.000184
Epoch 20 iteration 0008/0187: training loss 0.684; learning rate 0.000184
Epoch 20 iteration 0009/0187: training loss 0.706; learning rate 0.000184
Epoch 20 iteration 0010/0187: training loss 0.699; learning rate 0.000184
Epoch 20 iteration 0011/0187: training loss 0.705; learning rate 0.000184
Epoch 20 iteration 0012/0187: training loss 0.734; learning rate 0.000184
Epoch 20 iteration 0013/0187: training loss 0.723; learning rate 0.000184
Epoch 20 iteration 0014/0187: training loss 0.710; learning rate 0.000184
Epoch 20 iteration 0015/0187: training loss 0.724; learning rate 0.000184
Epoch 20 iteration 0016/0187: training loss 0.738; learning rate 0.000184
Epoch 20 iteration 0017/0187: training loss 0.719; learning rate 0.000183
Epoch 20 iteration 0018/0187: training loss 0.713; learning rate 0.000183
Epoch 20 iteration 0019/0187: training loss 0.717; learning rate 0.000183
Epoch 20 iteration 0020/0187: training loss 0.723; learning rate 0.000183
Epoch 20 iteration 0021/0187: training loss 0.723; learning rate 0.000183
Epoch 20 iteration 0022/0187: training loss 0.715; learning rate 0.000183
Epoch 20 iteration 0023/0187: training loss 0.715; learning rate 0.000183
Epoch 20 iteration 0024/0187: training loss 0.715; learning rate 0.000183
Epoch 20 iteration 0025/0187: training loss 0.716; learning rate 0.000183
Epoch 20 iteration 0026/0187: training loss 0.720; learning rate 0.000183
Epoch 20 iteration 0027/0187: training loss 0.713; learning rate 0.000183
Epoch 20 iteration 0028/0187: training loss 0.718; learning rate 0.000182
Epoch 20 iteration 0029/0187: training loss 0.715; learning rate 0.000182
Epoch 20 iteration 0030/0187: training loss 0.728; learning rate 0.000182
Epoch 20 iteration 0031/0187: training loss 0.722; learning rate 0.000182
Epoch 20 iteration 0032/0187: training loss 0.718; learning rate 0.000182
Epoch 20 iteration 0033/0187: training loss 0.714; learning rate 0.000182
Epoch 20 iteration 0034/0187: training loss 0.711; learning rate 0.000182
Epoch 20 iteration 0035/0187: training loss 0.710; learning rate 0.000182
Epoch 20 iteration 0036/0187: training loss 0.708; learning rate 0.000182
Epoch 20 iteration 0037/0187: training loss 0.718; learning rate 0.000182
Epoch 20 iteration 0038/0187: training loss 0.715; learning rate 0.000182
Epoch 20 iteration 0039/0187: training loss 0.716; learning rate 0.000181
Epoch 20 iteration 0040/0187: training loss 0.719; learning rate 0.000181
Epoch 20 iteration 0041/0187: training loss 0.716; learning rate 0.000181
Epoch 20 iteration 0042/0187: training loss 0.720; learning rate 0.000181
Epoch 20 iteration 0043/0187: training loss 0.719; learning rate 0.000181
Epoch 20 iteration 0044/0187: training loss 0.718; learning rate 0.000181
Epoch 20 iteration 0045/0187: training loss 0.712; learning rate 0.000181
Epoch 20 iteration 0046/0187: training loss 0.714; learning rate 0.000181
Epoch 20 iteration 0047/0187: training loss 0.711; learning rate 0.000181
Epoch 20 iteration 0048/0187: training loss 0.706; learning rate 0.000181
Epoch 20 iteration 0049/0187: training loss 0.704; learning rate 0.000181
Epoch 20 iteration 0050/0187: training loss 0.710; learning rate 0.000180
Epoch 20 iteration 0051/0187: training loss 0.709; learning rate 0.000180
Epoch 20 iteration 0052/0187: training loss 0.710; learning rate 0.000180
Epoch 20 iteration 0053/0187: training loss 0.704; learning rate 0.000180
Epoch 20 iteration 0054/0187: training loss 0.705; learning rate 0.000180
Epoch 20 iteration 0055/0187: training loss 0.705; learning rate 0.000180
Epoch 20 iteration 0056/0187: training loss 0.704; learning rate 0.000180
Epoch 20 iteration 0057/0187: training loss 0.704; learning rate 0.000180
Epoch 20 iteration 0058/0187: training loss 0.700; learning rate 0.000180
Epoch 20 iteration 0059/0187: training loss 0.700; learning rate 0.000180
Epoch 20 iteration 0060/0187: training loss 0.700; learning rate 0.000180
Epoch 20 iteration 0061/0187: training loss 0.702; learning rate 0.000180
Epoch 20 iteration 0062/0187: training loss 0.700; learning rate 0.000179
Epoch 20 iteration 0063/0187: training loss 0.699; learning rate 0.000179
Epoch 20 iteration 0064/0187: training loss 0.701; learning rate 0.000179
Epoch 20 iteration 0065/0187: training loss 0.701; learning rate 0.000179
Epoch 20 iteration 0066/0187: training loss 0.705; learning rate 0.000179
Epoch 20 iteration 0067/0187: training loss 0.709; learning rate 0.000179
Epoch 20 iteration 0068/0187: training loss 0.708; learning rate 0.000179
Epoch 20 iteration 0069/0187: training loss 0.710; learning rate 0.000179
Epoch 20 iteration 0070/0187: training loss 0.712; learning rate 0.000179
Epoch 20 iteration 0071/0187: training loss 0.711; learning rate 0.000179
Epoch 20 iteration 0072/0187: training loss 0.715; learning rate 0.000179
Epoch 20 iteration 0073/0187: training loss 0.718; learning rate 0.000178
Epoch 20 iteration 0074/0187: training loss 0.716; learning rate 0.000178
Epoch 20 iteration 0075/0187: training loss 0.716; learning rate 0.000178
Epoch 20 iteration 0076/0187: training loss 0.714; learning rate 0.000178
Epoch 20 iteration 0077/0187: training loss 0.715; learning rate 0.000178
Epoch 20 iteration 0078/0187: training loss 0.718; learning rate 0.000178
Epoch 20 iteration 0079/0187: training loss 0.719; learning rate 0.000178
Epoch 20 iteration 0080/0187: training loss 0.719; learning rate 0.000178
Epoch 20 iteration 0081/0187: training loss 0.717; learning rate 0.000178
Epoch 20 iteration 0082/0187: training loss 0.716; learning rate 0.000178
Epoch 20 iteration 0083/0187: training loss 0.717; learning rate 0.000178
Epoch 20 iteration 0084/0187: training loss 0.718; learning rate 0.000177
Epoch 20 iteration 0085/0187: training loss 0.718; learning rate 0.000177
Epoch 20 iteration 0086/0187: training loss 0.717; learning rate 0.000177
Epoch 20 iteration 0087/0187: training loss 0.718; learning rate 0.000177
Epoch 20 iteration 0088/0187: training loss 0.721; learning rate 0.000177
Epoch 20 iteration 0089/0187: training loss 0.720; learning rate 0.000177
Epoch 20 iteration 0090/0187: training loss 0.722; learning rate 0.000177
Epoch 20 iteration 0091/0188: training loss 0.722; learning rate 0.000177
Epoch 20 iteration 0092/0188: training loss 0.722; learning rate 0.000177
Epoch 20 iteration 0093/0188: training loss 0.724; learning rate 0.000177
Epoch 20 iteration 0094/0188: training loss 0.722; learning rate 0.000177
Epoch 20 iteration 0095/0188: training loss 0.721; learning rate 0.000176
Epoch 20 iteration 0096/0188: training loss 0.721; learning rate 0.000176
Epoch 20 iteration 0097/0188: training loss 0.722; learning rate 0.000176
Epoch 20 iteration 0098/0188: training loss 0.722; learning rate 0.000176
Epoch 20 iteration 0099/0188: training loss 0.723; learning rate 0.000176
Epoch 20 iteration 0100/0188: training loss 0.723; learning rate 0.000176
Epoch 20 iteration 0101/0188: training loss 0.723; learning rate 0.000176
Epoch 20 iteration 0102/0188: training loss 0.724; learning rate 0.000176
Epoch 20 iteration 0103/0188: training loss 0.725; learning rate 0.000176
Epoch 20 iteration 0104/0188: training loss 0.723; learning rate 0.000176
Epoch 20 iteration 0105/0188: training loss 0.724; learning rate 0.000176
Epoch 20 iteration 0106/0188: training loss 0.723; learning rate 0.000175
Epoch 20 iteration 0107/0188: training loss 0.723; learning rate 0.000175
Epoch 20 iteration 0108/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0109/0188: training loss 0.721; learning rate 0.000175
Epoch 20 iteration 0110/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0111/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0112/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0113/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0114/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0115/0188: training loss 0.722; learning rate 0.000175
Epoch 20 iteration 0116/0188: training loss 0.723; learning rate 0.000175
Epoch 20 iteration 0117/0188: training loss 0.723; learning rate 0.000174
Epoch 20 iteration 0118/0188: training loss 0.724; learning rate 0.000174
Epoch 20 iteration 0119/0188: training loss 0.726; learning rate 0.000174
Epoch 20 iteration 0120/0188: training loss 0.725; learning rate 0.000174
Epoch 20 iteration 0121/0188: training loss 0.723; learning rate 0.000174
Epoch 20 iteration 0122/0188: training loss 0.729; learning rate 0.000174
Epoch 20 iteration 0123/0188: training loss 0.729; learning rate 0.000174
Epoch 20 iteration 0124/0188: training loss 0.728; learning rate 0.000174
Epoch 20 iteration 0125/0188: training loss 0.729; learning rate 0.000174
Epoch 20 iteration 0126/0188: training loss 0.729; learning rate 0.000174
Epoch 20 iteration 0127/0188: training loss 0.728; learning rate 0.000174
Epoch 20 iteration 0128/0188: training loss 0.730; learning rate 0.000173
Epoch 20 iteration 0129/0188: training loss 0.729; learning rate 0.000173
Epoch 20 iteration 0130/0188: training loss 0.732; learning rate 0.000173
Epoch 20 iteration 0131/0188: training loss 0.732; learning rate 0.000173
Epoch 20 iteration 0132/0188: training loss 0.735; learning rate 0.000173
Epoch 20 iteration 0133/0188: training loss 0.734; learning rate 0.000173
Epoch 20 iteration 0134/0188: training loss 0.732; learning rate 0.000173
Epoch 20 iteration 0135/0188: training loss 0.731; learning rate 0.000173
Epoch 20 iteration 0136/0188: training loss 0.731; learning rate 0.000173
Epoch 20 iteration 0137/0188: training loss 0.730; learning rate 0.000173
Epoch 20 iteration 0138/0188: training loss 0.731; learning rate 0.000173
Epoch 20 iteration 0139/0188: training loss 0.731; learning rate 0.000172
Epoch 20 iteration 0140/0188: training loss 0.732; learning rate 0.000172
Epoch 20 iteration 0141/0188: training loss 0.734; learning rate 0.000172
Epoch 20 iteration 0142/0188: training loss 0.736; learning rate 0.000172
Epoch 20 iteration 0143/0188: training loss 0.737; learning rate 0.000172
Epoch 20 iteration 0144/0188: training loss 0.736; learning rate 0.000172
Epoch 20 iteration 0145/0188: training loss 0.737; learning rate 0.000172
Epoch 20 iteration 0146/0188: training loss 0.735; learning rate 0.000172
Epoch 20 iteration 0147/0188: training loss 0.735; learning rate 0.000172
Epoch 20 iteration 0148/0188: training loss 0.736; learning rate 0.000172
Epoch 20 iteration 0149/0188: training loss 0.735; learning rate 0.000172
Epoch 20 iteration 0150/0188: training loss 0.738; learning rate 0.000171
Epoch 20 iteration 0151/0188: training loss 0.738; learning rate 0.000171
Epoch 20 iteration 0152/0188: training loss 0.738; learning rate 0.000171
Epoch 20 iteration 0153/0188: training loss 0.738; learning rate 0.000171
Epoch 20 iteration 0154/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0155/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0156/0188: training loss 0.736; learning rate 0.000171
Epoch 20 iteration 0157/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0158/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0159/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0160/0188: training loss 0.737; learning rate 0.000171
Epoch 20 iteration 0161/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0162/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0163/0188: training loss 0.737; learning rate 0.000170
Epoch 20 iteration 0164/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0165/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0166/0188: training loss 0.739; learning rate 0.000170
Epoch 20 iteration 0167/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0168/0188: training loss 0.739; learning rate 0.000170
Epoch 20 iteration 0169/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0170/0188: training loss 0.737; learning rate 0.000170
Epoch 20 iteration 0171/0188: training loss 0.738; learning rate 0.000170
Epoch 20 iteration 0172/0188: training loss 0.736; learning rate 0.000169
Epoch 20 iteration 0173/0188: training loss 0.737; learning rate 0.000169
Epoch 20 iteration 0174/0188: training loss 0.735; learning rate 0.000169
Epoch 20 iteration 0175/0188: training loss 0.738; learning rate 0.000169
Epoch 20 iteration 0176/0188: training loss 0.738; learning rate 0.000169
Epoch 20 iteration 0177/0188: training loss 0.738; learning rate 0.000169
Epoch 20 iteration 0178/0188: training loss 0.739; learning rate 0.000169
Epoch 20 iteration 0179/0188: training loss 0.739; learning rate 0.000169
Epoch 20 iteration 0180/0188: training loss 0.739; learning rate 0.000169
Epoch 20 iteration 0181/0188: training loss 0.739; learning rate 0.000169
Epoch 20 iteration 0182/0188: training loss 0.738; learning rate 0.000169
Epoch 20 iteration 0183/0188: training loss 0.741; learning rate 0.000168
Epoch 20 iteration 0184/0188: training loss 0.740; learning rate 0.000168
Epoch 20 iteration 0185/0188: training loss 0.740; learning rate 0.000168
Epoch 20 iteration 0186/0188: training loss 0.740; learning rate 0.000168
Epoch 20 validation pixAcc: 0.333, mIoU: 0.168
Epoch 21 iteration 0001/0187: training loss 0.823; learning rate 0.000168
Epoch 21 iteration 0002/0187: training loss 0.776; learning rate 0.000168
Epoch 21 iteration 0003/0187: training loss 0.698; learning rate 0.000168
Epoch 21 iteration 0004/0187: training loss 0.671; learning rate 0.000168
Epoch 21 iteration 0005/0187: training loss 0.723; learning rate 0.000168
Epoch 21 iteration 0006/0187: training loss 0.706; learning rate 0.000168
Epoch 21 iteration 0007/0187: training loss 0.682; learning rate 0.000167
Epoch 21 iteration 0008/0187: training loss 0.671; learning rate 0.000167
Epoch 21 iteration 0009/0187: training loss 0.667; learning rate 0.000167
Epoch 21 iteration 0010/0187: training loss 0.663; learning rate 0.000167
Epoch 21 iteration 0011/0187: training loss 0.666; learning rate 0.000167
Epoch 21 iteration 0012/0187: training loss 0.668; learning rate 0.000167
Epoch 21 iteration 0013/0187: training loss 0.685; learning rate 0.000167
Epoch 21 iteration 0014/0187: training loss 0.677; learning rate 0.000167
Epoch 21 iteration 0015/0187: training loss 0.669; learning rate 0.000167
Epoch 21 iteration 0016/0187: training loss 0.694; learning rate 0.000167
Epoch 21 iteration 0017/0187: training loss 0.686; learning rate 0.000167
Epoch 21 iteration 0018/0187: training loss 0.686; learning rate 0.000167
Epoch 21 iteration 0019/0187: training loss 0.693; learning rate 0.000166
Epoch 21 iteration 0020/0187: training loss 0.693; learning rate 0.000166
Epoch 21 iteration 0021/0187: training loss 0.689; learning rate 0.000166
Epoch 21 iteration 0022/0187: training loss 0.688; learning rate 0.000166
Epoch 21 iteration 0023/0187: training loss 0.687; learning rate 0.000166
Epoch 21 iteration 0024/0187: training loss 0.681; learning rate 0.000166
Epoch 21 iteration 0025/0187: training loss 0.679; learning rate 0.000166
Epoch 21 iteration 0026/0187: training loss 0.678; learning rate 0.000166
Epoch 21 iteration 0027/0187: training loss 0.673; learning rate 0.000166
Epoch 21 iteration 0028/0187: training loss 0.671; learning rate 0.000166
Epoch 21 iteration 0029/0187: training loss 0.674; learning rate 0.000166
Epoch 21 iteration 0030/0187: training loss 0.674; learning rate 0.000165
Epoch 21 iteration 0031/0187: training loss 0.679; learning rate 0.000165
Epoch 21 iteration 0032/0187: training loss 0.681; learning rate 0.000165
Epoch 21 iteration 0033/0187: training loss 0.682; learning rate 0.000165
Epoch 21 iteration 0034/0187: training loss 0.676; learning rate 0.000165
Epoch 21 iteration 0035/0187: training loss 0.677; learning rate 0.000165
Epoch 21 iteration 0036/0187: training loss 0.675; learning rate 0.000165
Epoch 21 iteration 0037/0187: training loss 0.674; learning rate 0.000165
Epoch 21 iteration 0038/0187: training loss 0.676; learning rate 0.000165
Epoch 21 iteration 0039/0187: training loss 0.673; learning rate 0.000165
Epoch 21 iteration 0040/0187: training loss 0.674; learning rate 0.000165
Epoch 21 iteration 0041/0187: training loss 0.674; learning rate 0.000164
Epoch 21 iteration 0042/0187: training loss 0.673; learning rate 0.000164
Epoch 21 iteration 0043/0187: training loss 0.680; learning rate 0.000164
Epoch 21 iteration 0044/0187: training loss 0.678; learning rate 0.000164
Epoch 21 iteration 0045/0187: training loss 0.675; learning rate 0.000164
Epoch 21 iteration 0046/0187: training loss 0.677; learning rate 0.000164
Epoch 21 iteration 0047/0187: training loss 0.681; learning rate 0.000164
Epoch 21 iteration 0048/0187: training loss 0.680; learning rate 0.000164
Epoch 21 iteration 0049/0187: training loss 0.689; learning rate 0.000164
Epoch 21 iteration 0050/0187: training loss 0.694; learning rate 0.000164
Epoch 21 iteration 0051/0187: training loss 0.696; learning rate 0.000164
Epoch 21 iteration 0052/0187: training loss 0.694; learning rate 0.000163
Epoch 21 iteration 0053/0187: training loss 0.697; learning rate 0.000163
Epoch 21 iteration 0054/0187: training loss 0.698; learning rate 0.000163
Epoch 21 iteration 0055/0187: training loss 0.696; learning rate 0.000163
Epoch 21 iteration 0056/0187: training loss 0.696; learning rate 0.000163
Epoch 21 iteration 0057/0187: training loss 0.698; learning rate 0.000163
Epoch 21 iteration 0058/0187: training loss 0.696; learning rate 0.000163
Epoch 21 iteration 0059/0187: training loss 0.701; learning rate 0.000163
Epoch 21 iteration 0060/0187: training loss 0.703; learning rate 0.000163
Epoch 21 iteration 0061/0187: training loss 0.705; learning rate 0.000163
Epoch 21 iteration 0062/0187: training loss 0.704; learning rate 0.000163
Epoch 21 iteration 0063/0187: training loss 0.704; learning rate 0.000162
Epoch 21 iteration 0064/0187: training loss 0.704; learning rate 0.000162
Epoch 21 iteration 0065/0187: training loss 0.703; learning rate 0.000162
Epoch 21 iteration 0066/0187: training loss 0.706; learning rate 0.000162
Epoch 21 iteration 0067/0187: training loss 0.707; learning rate 0.000162
Epoch 21 iteration 0068/0187: training loss 0.705; learning rate 0.000162
Epoch 21 iteration 0069/0187: training loss 0.710; learning rate 0.000162
Epoch 21 iteration 0070/0187: training loss 0.707; learning rate 0.000162
Epoch 21 iteration 0071/0187: training loss 0.708; learning rate 0.000162
Epoch 21 iteration 0072/0187: training loss 0.708; learning rate 0.000162
Epoch 21 iteration 0073/0187: training loss 0.706; learning rate 0.000162
Epoch 21 iteration 0074/0187: training loss 0.712; learning rate 0.000161
Epoch 21 iteration 0075/0187: training loss 0.712; learning rate 0.000161
Epoch 21 iteration 0076/0187: training loss 0.714; learning rate 0.000161
Epoch 21 iteration 0077/0187: training loss 0.714; learning rate 0.000161
Epoch 21 iteration 0078/0187: training loss 0.713; learning rate 0.000161
Epoch 21 iteration 0079/0187: training loss 0.714; learning rate 0.000161
Epoch 21 iteration 0080/0187: training loss 0.715; learning rate 0.000161
Epoch 21 iteration 0081/0187: training loss 0.714; learning rate 0.000161
Epoch 21 iteration 0082/0187: training loss 0.717; learning rate 0.000161
Epoch 21 iteration 0083/0187: training loss 0.717; learning rate 0.000161
Epoch 21 iteration 0084/0187: training loss 0.717; learning rate 0.000161
Epoch 21 iteration 0085/0187: training loss 0.720; learning rate 0.000160
Epoch 21 iteration 0086/0187: training loss 0.720; learning rate 0.000160
Epoch 21 iteration 0087/0187: training loss 0.720; learning rate 0.000160
Epoch 21 iteration 0088/0187: training loss 0.718; learning rate 0.000160
Epoch 21 iteration 0089/0187: training loss 0.720; learning rate 0.000160
Epoch 21 iteration 0090/0187: training loss 0.719; learning rate 0.000160
Epoch 21 iteration 0091/0187: training loss 0.720; learning rate 0.000160
Epoch 21 iteration 0092/0187: training loss 0.719; learning rate 0.000160
Epoch 21 iteration 0093/0187: training loss 0.719; learning rate 0.000160
Epoch 21 iteration 0094/0187: training loss 0.718; learning rate 0.000160
Epoch 21 iteration 0095/0187: training loss 0.718; learning rate 0.000160
Epoch 21 iteration 0096/0187: training loss 0.720; learning rate 0.000159
Epoch 21 iteration 0097/0187: training loss 0.719; learning rate 0.000159
Epoch 21 iteration 0098/0187: training loss 0.719; learning rate 0.000159
Epoch 21 iteration 0099/0187: training loss 0.719; learning rate 0.000159
Epoch 21 iteration 0100/0187: training loss 0.718; learning rate 0.000159
Epoch 21 iteration 0101/0187: training loss 0.717; learning rate 0.000159
Epoch 21 iteration 0102/0187: training loss 0.720; learning rate 0.000159
Epoch 21 iteration 0103/0187: training loss 0.719; learning rate 0.000159
Epoch 21 iteration 0104/0187: training loss 0.722; learning rate 0.000159
Epoch 21 iteration 0105/0187: training loss 0.720; learning rate 0.000159
Epoch 21 iteration 0106/0187: training loss 0.722; learning rate 0.000159
Epoch 21 iteration 0107/0187: training loss 0.721; learning rate 0.000158
Epoch 21 iteration 0108/0187: training loss 0.720; learning rate 0.000158
Epoch 21 iteration 0109/0187: training loss 0.721; learning rate 0.000158
Epoch 21 iteration 0110/0187: training loss 0.723; learning rate 0.000158
Epoch 21 iteration 0111/0187: training loss 0.724; learning rate 0.000158
Epoch 21 iteration 0112/0187: training loss 0.725; learning rate 0.000158
Epoch 21 iteration 0113/0187: training loss 0.723; learning rate 0.000158
Epoch 21 iteration 0114/0187: training loss 0.723; learning rate 0.000158
Epoch 21 iteration 0115/0187: training loss 0.722; learning rate 0.000158
Epoch 21 iteration 0116/0187: training loss 0.722; learning rate 0.000158
Epoch 21 iteration 0117/0187: training loss 0.721; learning rate 0.000157
Epoch 21 iteration 0118/0187: training loss 0.720; learning rate 0.000157
Epoch 21 iteration 0119/0187: training loss 0.719; learning rate 0.000157
Epoch 21 iteration 0120/0187: training loss 0.718; learning rate 0.000157
Epoch 21 iteration 0121/0187: training loss 0.718; learning rate 0.000157
Epoch 21 iteration 0122/0187: training loss 0.718; learning rate 0.000157
Epoch 21 iteration 0123/0187: training loss 0.716; learning rate 0.000157
Epoch 21 iteration 0124/0187: training loss 0.717; learning rate 0.000157
Epoch 21 iteration 0125/0187: training loss 0.717; learning rate 0.000157
Epoch 21 iteration 0126/0187: training loss 0.718; learning rate 0.000157
Epoch 21 iteration 0127/0187: training loss 0.721; learning rate 0.000157
Epoch 21 iteration 0128/0187: training loss 0.722; learning rate 0.000156
Epoch 21 iteration 0129/0187: training loss 0.722; learning rate 0.000156
Epoch 21 iteration 0130/0187: training loss 0.721; learning rate 0.000156
Epoch 21 iteration 0131/0187: training loss 0.723; learning rate 0.000156
Epoch 21 iteration 0132/0187: training loss 0.724; learning rate 0.000156
Epoch 21 iteration 0133/0187: training loss 0.724; learning rate 0.000156
Epoch 21 iteration 0134/0187: training loss 0.725; learning rate 0.000156
Epoch 21 iteration 0135/0187: training loss 0.726; learning rate 0.000156
Epoch 21 iteration 0136/0187: training loss 0.728; learning rate 0.000156
Epoch 21 iteration 0137/0187: training loss 0.728; learning rate 0.000156
Epoch 21 iteration 0138/0187: training loss 0.729; learning rate 0.000156
Epoch 21 iteration 0139/0187: training loss 0.728; learning rate 0.000155
Epoch 21 iteration 0140/0187: training loss 0.731; learning rate 0.000155
Epoch 21 iteration 0141/0187: training loss 0.731; learning rate 0.000155
Epoch 21 iteration 0142/0187: training loss 0.731; learning rate 0.000155
Epoch 21 iteration 0143/0187: training loss 0.733; learning rate 0.000155
Epoch 21 iteration 0144/0187: training loss 0.733; learning rate 0.000155
Epoch 21 iteration 0145/0187: training loss 0.733; learning rate 0.000155
Epoch 21 iteration 0146/0187: training loss 0.732; learning rate 0.000155
Epoch 21 iteration 0147/0187: training loss 0.732; learning rate 0.000155
Epoch 21 iteration 0148/0187: training loss 0.731; learning rate 0.000155
Epoch 21 iteration 0149/0187: training loss 0.733; learning rate 0.000155
Epoch 21 iteration 0150/0187: training loss 0.733; learning rate 0.000154
Epoch 21 iteration 0151/0187: training loss 0.733; learning rate 0.000154
Epoch 21 iteration 0152/0187: training loss 0.733; learning rate 0.000154
Epoch 21 iteration 0153/0187: training loss 0.734; learning rate 0.000154
Epoch 21 iteration 0154/0187: training loss 0.733; learning rate 0.000154
Epoch 21 iteration 0155/0187: training loss 0.734; learning rate 0.000154
Epoch 21 iteration 0156/0187: training loss 0.733; learning rate 0.000154
Epoch 21 iteration 0157/0187: training loss 0.732; learning rate 0.000154
Epoch 21 iteration 0158/0187: training loss 0.732; learning rate 0.000154
Epoch 21 iteration 0159/0187: training loss 0.734; learning rate 0.000154
Epoch 21 iteration 0160/0187: training loss 0.734; learning rate 0.000154
Epoch 21 iteration 0161/0187: training loss 0.736; learning rate 0.000153
Epoch 21 iteration 0162/0187: training loss 0.736; learning rate 0.000153
Epoch 21 iteration 0163/0187: training loss 0.736; learning rate 0.000153
Epoch 21 iteration 0164/0187: training loss 0.736; learning rate 0.000153
Epoch 21 iteration 0165/0187: training loss 0.737; learning rate 0.000153
Epoch 21 iteration 0166/0187: training loss 0.737; learning rate 0.000153
Epoch 21 iteration 0167/0187: training loss 0.738; learning rate 0.000153
Epoch 21 iteration 0168/0187: training loss 0.739; learning rate 0.000153
Epoch 21 iteration 0169/0187: training loss 0.738; learning rate 0.000153
Epoch 21 iteration 0170/0187: training loss 0.738; learning rate 0.000153
Epoch 21 iteration 0171/0187: training loss 0.738; learning rate 0.000153
Epoch 21 iteration 0172/0187: training loss 0.738; learning rate 0.000152
Epoch 21 iteration 0173/0187: training loss 0.737; learning rate 0.000152
Epoch 21 iteration 0174/0187: training loss 0.737; learning rate 0.000152
Epoch 21 iteration 0175/0187: training loss 0.738; learning rate 0.000152
Epoch 21 iteration 0176/0187: training loss 0.737; learning rate 0.000152
Epoch 21 iteration 0177/0187: training loss 0.736; learning rate 0.000152
Epoch 21 iteration 0178/0187: training loss 0.734; learning rate 0.000152
Epoch 21 iteration 0179/0187: training loss 0.735; learning rate 0.000152
Epoch 21 iteration 0180/0187: training loss 0.735; learning rate 0.000152
Epoch 21 iteration 0181/0187: training loss 0.735; learning rate 0.000152
Epoch 21 iteration 0182/0187: training loss 0.735; learning rate 0.000152
Epoch 21 iteration 0183/0187: training loss 0.734; learning rate 0.000151
Epoch 21 iteration 0184/0187: training loss 0.737; learning rate 0.000151
Epoch 21 iteration 0185/0187: training loss 0.737; learning rate 0.000151
Epoch 21 iteration 0186/0187: training loss 0.737; learning rate 0.000151
Epoch 21 iteration 0187/0187: training loss 0.738; learning rate 0.000151
Epoch 21 validation pixAcc: 0.332, mIoU: 0.168
Epoch 22 iteration 0001/0187: training loss 0.705; learning rate 0.000151
Epoch 22 iteration 0002/0187: training loss 0.687; learning rate 0.000151
Epoch 22 iteration 0003/0187: training loss 0.648; learning rate 0.000151
Epoch 22 iteration 0004/0187: training loss 0.672; learning rate 0.000151
Epoch 22 iteration 0005/0187: training loss 0.702; learning rate 0.000151
Epoch 22 iteration 0006/0187: training loss 0.694; learning rate 0.000150
Epoch 22 iteration 0007/0187: training loss 0.726; learning rate 0.000150
Epoch 22 iteration 0008/0187: training loss 0.726; learning rate 0.000150
Epoch 22 iteration 0009/0187: training loss 0.731; learning rate 0.000150
Epoch 22 iteration 0010/0187: training loss 0.737; learning rate 0.000150
Epoch 22 iteration 0011/0187: training loss 0.744; learning rate 0.000150
Epoch 22 iteration 0012/0187: training loss 0.740; learning rate 0.000150
Epoch 22 iteration 0013/0187: training loss 0.755; learning rate 0.000150
Epoch 22 iteration 0014/0187: training loss 0.747; learning rate 0.000150
Epoch 22 iteration 0015/0187: training loss 0.760; learning rate 0.000150
Epoch 22 iteration 0016/0187: training loss 0.761; learning rate 0.000150
Epoch 22 iteration 0017/0187: training loss 0.752; learning rate 0.000149
Epoch 22 iteration 0018/0187: training loss 0.758; learning rate 0.000149
Epoch 22 iteration 0019/0187: training loss 0.746; learning rate 0.000149
Epoch 22 iteration 0020/0187: training loss 0.745; learning rate 0.000149
Epoch 22 iteration 0021/0187: training loss 0.750; learning rate 0.000149
Epoch 22 iteration 0022/0187: training loss 0.748; learning rate 0.000149
Epoch 22 iteration 0023/0187: training loss 0.738; learning rate 0.000149
Epoch 22 iteration 0024/0187: training loss 0.738; learning rate 0.000149
Epoch 22 iteration 0025/0187: training loss 0.734; learning rate 0.000149
Epoch 22 iteration 0026/0187: training loss 0.737; learning rate 0.000149
Epoch 22 iteration 0027/0187: training loss 0.728; learning rate 0.000149
Epoch 22 iteration 0028/0187: training loss 0.724; learning rate 0.000148
Epoch 22 iteration 0029/0187: training loss 0.720; learning rate 0.000148
Epoch 22 iteration 0030/0187: training loss 0.717; learning rate 0.000148
Epoch 22 iteration 0031/0187: training loss 0.716; learning rate 0.000148
Epoch 22 iteration 0032/0187: training loss 0.718; learning rate 0.000148
Epoch 22 iteration 0033/0187: training loss 0.722; learning rate 0.000148
Epoch 22 iteration 0034/0187: training loss 0.723; learning rate 0.000148
Epoch 22 iteration 0035/0187: training loss 0.724; learning rate 0.000148
Epoch 22 iteration 0036/0187: training loss 0.725; learning rate 0.000148
Epoch 22 iteration 0037/0187: training loss 0.727; learning rate 0.000148
Epoch 22 iteration 0038/0187: training loss 0.727; learning rate 0.000148
Epoch 22 iteration 0039/0187: training loss 0.735; learning rate 0.000147
Epoch 22 iteration 0040/0187: training loss 0.732; learning rate 0.000147
Epoch 22 iteration 0041/0187: training loss 0.732; learning rate 0.000147
Epoch 22 iteration 0042/0187: training loss 0.726; learning rate 0.000147
Epoch 22 iteration 0043/0187: training loss 0.729; learning rate 0.000147
Epoch 22 iteration 0044/0187: training loss 0.730; learning rate 0.000147
Epoch 22 iteration 0045/0187: training loss 0.726; learning rate 0.000147
Epoch 22 iteration 0046/0187: training loss 0.730; learning rate 0.000147
Epoch 22 iteration 0047/0187: training loss 0.729; learning rate 0.000147
Epoch 22 iteration 0048/0187: training loss 0.729; learning rate 0.000147
Epoch 22 iteration 0049/0187: training loss 0.725; learning rate 0.000147
Epoch 22 iteration 0050/0187: training loss 0.727; learning rate 0.000146
Epoch 22 iteration 0051/0187: training loss 0.724; learning rate 0.000146
Epoch 22 iteration 0052/0187: training loss 0.727; learning rate 0.000146
Epoch 22 iteration 0053/0187: training loss 0.728; learning rate 0.000146
Epoch 22 iteration 0054/0187: training loss 0.727; learning rate 0.000146
Epoch 22 iteration 0055/0187: training loss 0.723; learning rate 0.000146
Epoch 22 iteration 0056/0187: training loss 0.725; learning rate 0.000146
Epoch 22 iteration 0057/0187: training loss 0.726; learning rate 0.000146
Epoch 22 iteration 0058/0187: training loss 0.725; learning rate 0.000146
Epoch 22 iteration 0059/0187: training loss 0.725; learning rate 0.000146
Epoch 22 iteration 0060/0187: training loss 0.725; learning rate 0.000145
Epoch 22 iteration 0061/0187: training loss 0.724; learning rate 0.000145
Epoch 22 iteration 0062/0187: training loss 0.723; learning rate 0.000145
Epoch 22 iteration 0063/0187: training loss 0.723; learning rate 0.000145
Epoch 22 iteration 0064/0187: training loss 0.724; learning rate 0.000145
Epoch 22 iteration 0065/0187: training loss 0.724; learning rate 0.000145
Epoch 22 iteration 0066/0187: training loss 0.726; learning rate 0.000145
Epoch 22 iteration 0067/0187: training loss 0.728; learning rate 0.000145
Epoch 22 iteration 0068/0187: training loss 0.729; learning rate 0.000145
Epoch 22 iteration 0069/0187: training loss 0.733; learning rate 0.000145
Epoch 22 iteration 0070/0187: training loss 0.732; learning rate 0.000145
Epoch 22 iteration 0071/0187: training loss 0.732; learning rate 0.000144
Epoch 22 iteration 0072/0187: training loss 0.733; learning rate 0.000144
Epoch 22 iteration 0073/0187: training loss 0.734; learning rate 0.000144
Epoch 22 iteration 0074/0187: training loss 0.733; learning rate 0.000144
Epoch 22 iteration 0075/0187: training loss 0.734; learning rate 0.000144
Epoch 22 iteration 0076/0187: training loss 0.734; learning rate 0.000144
Epoch 22 iteration 0077/0187: training loss 0.735; learning rate 0.000144
Epoch 22 iteration 0078/0187: training loss 0.736; learning rate 0.000144
Epoch 22 iteration 0079/0187: training loss 0.734; learning rate 0.000144
Epoch 22 iteration 0080/0187: training loss 0.732; learning rate 0.000144
Epoch 22 iteration 0081/0187: training loss 0.732; learning rate 0.000144
Epoch 22 iteration 0082/0187: training loss 0.732; learning rate 0.000143
Epoch 22 iteration 0083/0187: training loss 0.734; learning rate 0.000143
Epoch 22 iteration 0084/0187: training loss 0.732; learning rate 0.000143
Epoch 22 iteration 0085/0187: training loss 0.733; learning rate 0.000143
Epoch 22 iteration 0086/0187: training loss 0.733; learning rate 0.000143
Epoch 22 iteration 0087/0187: training loss 0.733; learning rate 0.000143
Epoch 22 iteration 0088/0187: training loss 0.733; learning rate 0.000143
Epoch 22 iteration 0089/0187: training loss 0.732; learning rate 0.000143
Epoch 22 iteration 0090/0187: training loss 0.733; learning rate 0.000143
Epoch 22 iteration 0091/0188: training loss 0.731; learning rate 0.000143
Epoch 22 iteration 0092/0188: training loss 0.732; learning rate 0.000143
Epoch 22 iteration 0093/0188: training loss 0.732; learning rate 0.000142
Epoch 22 iteration 0094/0188: training loss 0.731; learning rate 0.000142
Epoch 22 iteration 0095/0188: training loss 0.728; learning rate 0.000142
Epoch 22 iteration 0096/0188: training loss 0.730; learning rate 0.000142
Epoch 22 iteration 0097/0188: training loss 0.729; learning rate 0.000142
Epoch 22 iteration 0098/0188: training loss 0.729; learning rate 0.000142
Epoch 22 iteration 0099/0188: training loss 0.730; learning rate 0.000142
Epoch 22 iteration 0100/0188: training loss 0.730; learning rate 0.000142
Epoch 22 iteration 0101/0188: training loss 0.731; learning rate 0.000142
Epoch 22 iteration 0102/0188: training loss 0.730; learning rate 0.000142
Epoch 22 iteration 0103/0188: training loss 0.728; learning rate 0.000142
Epoch 22 iteration 0104/0188: training loss 0.727; learning rate 0.000141
Epoch 22 iteration 0105/0188: training loss 0.728; learning rate 0.000141
Epoch 22 iteration 0106/0188: training loss 0.728; learning rate 0.000141
Epoch 22 iteration 0107/0188: training loss 0.728; learning rate 0.000141
Epoch 22 iteration 0108/0188: training loss 0.730; learning rate 0.000141
Epoch 22 iteration 0109/0188: training loss 0.729; learning rate 0.000141
Epoch 22 iteration 0110/0188: training loss 0.729; learning rate 0.000141
Epoch 22 iteration 0111/0188: training loss 0.728; learning rate 0.000141
Epoch 22 iteration 0112/0188: training loss 0.726; learning rate 0.000141
Epoch 22 iteration 0113/0188: training loss 0.728; learning rate 0.000141
Epoch 22 iteration 0114/0188: training loss 0.729; learning rate 0.000141
Epoch 22 iteration 0115/0188: training loss 0.729; learning rate 0.000140
Epoch 22 iteration 0116/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0117/0188: training loss 0.726; learning rate 0.000140
Epoch 22 iteration 0118/0188: training loss 0.728; learning rate 0.000140
Epoch 22 iteration 0119/0188: training loss 0.726; learning rate 0.000140
Epoch 22 iteration 0120/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0121/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0122/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0123/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0124/0188: training loss 0.728; learning rate 0.000140
Epoch 22 iteration 0125/0188: training loss 0.727; learning rate 0.000140
Epoch 22 iteration 0126/0188: training loss 0.724; learning rate 0.000139
Epoch 22 iteration 0127/0188: training loss 0.724; learning rate 0.000139
Epoch 22 iteration 0128/0188: training loss 0.724; learning rate 0.000139
Epoch 22 iteration 0129/0188: training loss 0.722; learning rate 0.000139
Epoch 22 iteration 0130/0188: training loss 0.724; learning rate 0.000139
Epoch 22 iteration 0131/0188: training loss 0.724; learning rate 0.000139
Epoch 22 iteration 0132/0188: training loss 0.723; learning rate 0.000139
Epoch 22 iteration 0133/0188: training loss 0.723; learning rate 0.000139
Epoch 22 iteration 0134/0188: training loss 0.725; learning rate 0.000139
Epoch 22 iteration 0135/0188: training loss 0.729; learning rate 0.000139
Epoch 22 iteration 0136/0188: training loss 0.729; learning rate 0.000138
Epoch 22 iteration 0137/0188: training loss 0.730; learning rate 0.000138
Epoch 22 iteration 0138/0188: training loss 0.731; learning rate 0.000138
Epoch 22 iteration 0139/0188: training loss 0.730; learning rate 0.000138
Epoch 22 iteration 0140/0188: training loss 0.730; learning rate 0.000138
Epoch 22 iteration 0141/0188: training loss 0.729; learning rate 0.000138
Epoch 22 iteration 0142/0188: training loss 0.728; learning rate 0.000138
Epoch 22 iteration 0143/0188: training loss 0.728; learning rate 0.000138
Epoch 22 iteration 0144/0188: training loss 0.727; learning rate 0.000138
Epoch 22 iteration 0145/0188: training loss 0.728; learning rate 0.000138
Epoch 22 iteration 0146/0188: training loss 0.729; learning rate 0.000138
Epoch 22 iteration 0147/0188: training loss 0.730; learning rate 0.000137
Epoch 22 iteration 0148/0188: training loss 0.730; learning rate 0.000137
Epoch 22 iteration 0149/0188: training loss 0.731; learning rate 0.000137
Epoch 22 iteration 0150/0188: training loss 0.729; learning rate 0.000137
Epoch 22 iteration 0151/0188: training loss 0.730; learning rate 0.000137
Epoch 22 iteration 0152/0188: training loss 0.728; learning rate 0.000137
Epoch 22 iteration 0153/0188: training loss 0.727; learning rate 0.000137
Epoch 22 iteration 0154/0188: training loss 0.729; learning rate 0.000137
Epoch 22 iteration 0155/0188: training loss 0.730; learning rate 0.000137
Epoch 22 iteration 0156/0188: training loss 0.731; learning rate 0.000137
Epoch 22 iteration 0157/0188: training loss 0.731; learning rate 0.000137
Epoch 22 iteration 0158/0188: training loss 0.731; learning rate 0.000136
Epoch 22 iteration 0159/0188: training loss 0.731; learning rate 0.000136
Epoch 22 iteration 0160/0188: training loss 0.732; learning rate 0.000136
Epoch 22 iteration 0161/0188: training loss 0.732; learning rate 0.000136
Epoch 22 iteration 0162/0188: training loss 0.731; learning rate 0.000136
Epoch 22 iteration 0163/0188: training loss 0.731; learning rate 0.000136
Epoch 22 iteration 0164/0188: training loss 0.730; learning rate 0.000136
Epoch 22 iteration 0165/0188: training loss 0.729; learning rate 0.000136
Epoch 22 iteration 0166/0188: training loss 0.729; learning rate 0.000136
Epoch 22 iteration 0167/0188: training loss 0.729; learning rate 0.000136
Epoch 22 iteration 0168/0188: training loss 0.729; learning rate 0.000136
Epoch 22 iteration 0169/0188: training loss 0.730; learning rate 0.000135
Epoch 22 iteration 0170/0188: training loss 0.731; learning rate 0.000135
Epoch 22 iteration 0171/0188: training loss 0.732; learning rate 0.000135
Epoch 22 iteration 0172/0188: training loss 0.733; learning rate 0.000135
Epoch 22 iteration 0173/0188: training loss 0.733; learning rate 0.000135
Epoch 22 iteration 0174/0188: training loss 0.734; learning rate 0.000135
Epoch 22 iteration 0175/0188: training loss 0.733; learning rate 0.000135
Epoch 22 iteration 0176/0188: training loss 0.733; learning rate 0.000135
Epoch 22 iteration 0177/0188: training loss 0.734; learning rate 0.000135
Epoch 22 iteration 0178/0188: training loss 0.735; learning rate 0.000135
Epoch 22 iteration 0179/0188: training loss 0.734; learning rate 0.000135
Epoch 22 iteration 0180/0188: training loss 0.734; learning rate 0.000134
Epoch 22 iteration 0181/0188: training loss 0.735; learning rate 0.000134
Epoch 22 iteration 0182/0188: training loss 0.735; learning rate 0.000134
Epoch 22 iteration 0183/0188: training loss 0.734; learning rate 0.000134
Epoch 22 iteration 0184/0188: training loss 0.734; learning rate 0.000134
Epoch 22 iteration 0185/0188: training loss 0.733; learning rate 0.000134
Epoch 22 iteration 0186/0188: training loss 0.733; learning rate 0.000134
Epoch 22 validation pixAcc: 0.333, mIoU: 0.167
Epoch 23 iteration 0001/0187: training loss 0.795; learning rate 0.000134
Epoch 23 iteration 0002/0187: training loss 0.781; learning rate 0.000134
Epoch 23 iteration 0003/0187: training loss 0.725; learning rate 0.000133
Epoch 23 iteration 0004/0187: training loss 0.720; learning rate 0.000133
Epoch 23 iteration 0005/0187: training loss 0.756; learning rate 0.000133
Epoch 23 iteration 0006/0187: training loss 0.771; learning rate 0.000133
Epoch 23 iteration 0007/0187: training loss 0.752; learning rate 0.000133
Epoch 23 iteration 0008/0187: training loss 0.758; learning rate 0.000133
Epoch 23 iteration 0009/0187: training loss 0.755; learning rate 0.000133
Epoch 23 iteration 0010/0187: training loss 0.750; learning rate 0.000133
Epoch 23 iteration 0011/0187: training loss 0.760; learning rate 0.000133
Epoch 23 iteration 0012/0187: training loss 0.753; learning rate 0.000133
Epoch 23 iteration 0013/0187: training loss 0.749; learning rate 0.000133
Epoch 23 iteration 0014/0187: training loss 0.749; learning rate 0.000132
Epoch 23 iteration 0015/0187: training loss 0.753; learning rate 0.000132
Epoch 23 iteration 0016/0187: training loss 0.759; learning rate 0.000132
Epoch 23 iteration 0017/0187: training loss 0.751; learning rate 0.000132
Epoch 23 iteration 0018/0187: training loss 0.765; learning rate 0.000132
Epoch 23 iteration 0019/0187: training loss 0.758; learning rate 0.000132
Epoch 23 iteration 0020/0187: training loss 0.752; learning rate 0.000132
Epoch 23 iteration 0021/0187: training loss 0.752; learning rate 0.000132
Epoch 23 iteration 0022/0187: training loss 0.750; learning rate 0.000132
Epoch 23 iteration 0023/0187: training loss 0.753; learning rate 0.000132
Epoch 23 iteration 0024/0187: training loss 0.745; learning rate 0.000132
Epoch 23 iteration 0025/0187: training loss 0.753; learning rate 0.000131
Epoch 23 iteration 0026/0187: training loss 0.749; learning rate 0.000131
Epoch 23 iteration 0027/0187: training loss 0.747; learning rate 0.000131
Epoch 23 iteration 0028/0187: training loss 0.744; learning rate 0.000131
Epoch 23 iteration 0029/0187: training loss 0.752; learning rate 0.000131
Epoch 23 iteration 0030/0187: training loss 0.748; learning rate 0.000131
Epoch 23 iteration 0031/0187: training loss 0.748; learning rate 0.000131
Epoch 23 iteration 0032/0187: training loss 0.748; learning rate 0.000131
Epoch 23 iteration 0033/0187: training loss 0.749; learning rate 0.000131
Epoch 23 iteration 0034/0187: training loss 0.746; learning rate 0.000131
Epoch 23 iteration 0035/0187: training loss 0.741; learning rate 0.000131
Epoch 23 iteration 0036/0187: training loss 0.745; learning rate 0.000130
Epoch 23 iteration 0037/0187: training loss 0.744; learning rate 0.000130
Epoch 23 iteration 0038/0187: training loss 0.743; learning rate 0.000130
Epoch 23 iteration 0039/0187: training loss 0.745; learning rate 0.000130
Epoch 23 iteration 0040/0187: training loss 0.743; learning rate 0.000130
Epoch 23 iteration 0041/0187: training loss 0.742; learning rate 0.000130
Epoch 23 iteration 0042/0187: training loss 0.742; learning rate 0.000130
Epoch 23 iteration 0043/0187: training loss 0.743; learning rate 0.000130
Epoch 23 iteration 0044/0187: training loss 0.738; learning rate 0.000130
Epoch 23 iteration 0045/0187: training loss 0.738; learning rate 0.000130
Epoch 23 iteration 0046/0187: training loss 0.742; learning rate 0.000129
Epoch 23 iteration 0047/0187: training loss 0.742; learning rate 0.000129
Epoch 23 iteration 0048/0187: training loss 0.739; learning rate 0.000129
Epoch 23 iteration 0049/0187: training loss 0.739; learning rate 0.000129
Epoch 23 iteration 0050/0187: training loss 0.741; learning rate 0.000129
Epoch 23 iteration 0051/0187: training loss 0.740; learning rate 0.000129
Epoch 23 iteration 0052/0187: training loss 0.752; learning rate 0.000129
Epoch 23 iteration 0053/0187: training loss 0.755; learning rate 0.000129
Epoch 23 iteration 0054/0187: training loss 0.756; learning rate 0.000129
Epoch 23 iteration 0055/0187: training loss 0.753; learning rate 0.000129
Epoch 23 iteration 0056/0187: training loss 0.758; learning rate 0.000129
Epoch 23 iteration 0057/0187: training loss 0.758; learning rate 0.000128
Epoch 23 iteration 0058/0187: training loss 0.755; learning rate 0.000128
Epoch 23 iteration 0059/0187: training loss 0.759; learning rate 0.000128
Epoch 23 iteration 0060/0187: training loss 0.753; learning rate 0.000128
Epoch 23 iteration 0061/0187: training loss 0.754; learning rate 0.000128
Epoch 23 iteration 0062/0187: training loss 0.757; learning rate 0.000128
Epoch 23 iteration 0063/0187: training loss 0.757; learning rate 0.000128
Epoch 23 iteration 0064/0187: training loss 0.758; learning rate 0.000128
Epoch 23 iteration 0065/0187: training loss 0.763; learning rate 0.000128
Epoch 23 iteration 0066/0187: training loss 0.766; learning rate 0.000128
Epoch 23 iteration 0067/0187: training loss 0.765; learning rate 0.000128
Epoch 23 iteration 0068/0187: training loss 0.766; learning rate 0.000127
Epoch 23 iteration 0069/0187: training loss 0.768; learning rate 0.000127
Epoch 23 iteration 0070/0187: training loss 0.765; learning rate 0.000127
Epoch 23 iteration 0071/0187: training loss 0.769; learning rate 0.000127
Epoch 23 iteration 0072/0187: training loss 0.768; learning rate 0.000127
Epoch 23 iteration 0073/0187: training loss 0.766; learning rate 0.000127
Epoch 23 iteration 0074/0187: training loss 0.769; learning rate 0.000127
Epoch 23 iteration 0075/0187: training loss 0.771; learning rate 0.000127
Epoch 23 iteration 0076/0187: training loss 0.769; learning rate 0.000127
Epoch 23 iteration 0077/0187: training loss 0.769; learning rate 0.000127
Epoch 23 iteration 0078/0187: training loss 0.769; learning rate 0.000126
Epoch 23 iteration 0079/0187: training loss 0.772; learning rate 0.000126
Epoch 23 iteration 0080/0187: training loss 0.775; learning rate 0.000126
Epoch 23 iteration 0081/0187: training loss 0.773; learning rate 0.000126
Epoch 23 iteration 0082/0187: training loss 0.773; learning rate 0.000126
Epoch 23 iteration 0083/0187: training loss 0.771; learning rate 0.000126
Epoch 23 iteration 0084/0187: training loss 0.769; learning rate 0.000126
Epoch 23 iteration 0085/0187: training loss 0.767; learning rate 0.000126
Epoch 23 iteration 0086/0187: training loss 0.770; learning rate 0.000126
Epoch 23 iteration 0087/0187: training loss 0.771; learning rate 0.000126
Epoch 23 iteration 0088/0187: training loss 0.778; learning rate 0.000126
Epoch 23 iteration 0089/0187: training loss 0.775; learning rate 0.000125
Epoch 23 iteration 0090/0187: training loss 0.776; learning rate 0.000125
Epoch 23 iteration 0091/0187: training loss 0.776; learning rate 0.000125
Epoch 23 iteration 0092/0187: training loss 0.774; learning rate 0.000125
Epoch 23 iteration 0093/0187: training loss 0.776; learning rate 0.000125
Epoch 23 iteration 0094/0187: training loss 0.775; learning rate 0.000125
Epoch 23 iteration 0095/0187: training loss 0.773; learning rate 0.000125
Epoch 23 iteration 0096/0187: training loss 0.772; learning rate 0.000125
Epoch 23 iteration 0097/0187: training loss 0.771; learning rate 0.000125
Epoch 23 iteration 0098/0187: training loss 0.770; learning rate 0.000125
Epoch 23 iteration 0099/0187: training loss 0.769; learning rate 0.000125
Epoch 23 iteration 0100/0187: training loss 0.767; learning rate 0.000124
Epoch 23 iteration 0101/0187: training loss 0.768; learning rate 0.000124
Epoch 23 iteration 0102/0187: training loss 0.767; learning rate 0.000124
Epoch 23 iteration 0103/0187: training loss 0.768; learning rate 0.000124
Epoch 23 iteration 0104/0187: training loss 0.768; learning rate 0.000124
Epoch 23 iteration 0105/0187: training loss 0.770; learning rate 0.000124
Epoch 23 iteration 0106/0187: training loss 0.772; learning rate 0.000124
Epoch 23 iteration 0107/0187: training loss 0.771; learning rate 0.000124
Epoch 23 iteration 0108/0187: training loss 0.769; learning rate 0.000124
Epoch 23 iteration 0109/0187: training loss 0.770; learning rate 0.000124
Epoch 23 iteration 0110/0187: training loss 0.767; learning rate 0.000123
Epoch 23 iteration 0111/0187: training loss 0.766; learning rate 0.000123
Epoch 23 iteration 0112/0187: training loss 0.765; learning rate 0.000123
Epoch 23 iteration 0113/0187: training loss 0.766; learning rate 0.000123
Epoch 23 iteration 0114/0187: training loss 0.765; learning rate 0.000123
Epoch 23 iteration 0115/0187: training loss 0.765; learning rate 0.000123
Epoch 23 iteration 0116/0187: training loss 0.764; learning rate 0.000123
Epoch 23 iteration 0117/0187: training loss 0.764; learning rate 0.000123
Epoch 23 iteration 0118/0187: training loss 0.763; learning rate 0.000123
Epoch 23 iteration 0119/0187: training loss 0.763; learning rate 0.000123
Epoch 23 iteration 0120/0187: training loss 0.763; learning rate 0.000123
Epoch 23 iteration 0121/0187: training loss 0.764; learning rate 0.000122
Epoch 23 iteration 0122/0187: training loss 0.764; learning rate 0.000122
Epoch 23 iteration 0123/0187: training loss 0.763; learning rate 0.000122
Epoch 23 iteration 0124/0187: training loss 0.762; learning rate 0.000122
Epoch 23 iteration 0125/0187: training loss 0.761; learning rate 0.000122
Epoch 23 iteration 0126/0187: training loss 0.759; learning rate 0.000122
Epoch 23 iteration 0127/0187: training loss 0.758; learning rate 0.000122
Epoch 23 iteration 0128/0187: training loss 0.761; learning rate 0.000122
Epoch 23 iteration 0129/0187: training loss 0.761; learning rate 0.000122
Epoch 23 iteration 0130/0187: training loss 0.760; learning rate 0.000122
Epoch 23 iteration 0131/0187: training loss 0.758; learning rate 0.000122
Epoch 23 iteration 0132/0187: training loss 0.757; learning rate 0.000121
Epoch 23 iteration 0133/0187: training loss 0.757; learning rate 0.000121
Epoch 23 iteration 0134/0187: training loss 0.759; learning rate 0.000121
Epoch 23 iteration 0135/0187: training loss 0.758; learning rate 0.000121
Epoch 23 iteration 0136/0187: training loss 0.758; learning rate 0.000121
Epoch 23 iteration 0137/0187: training loss 0.758; learning rate 0.000121
Epoch 23 iteration 0138/0187: training loss 0.757; learning rate 0.000121
Epoch 23 iteration 0139/0187: training loss 0.756; learning rate 0.000121
Epoch 23 iteration 0140/0187: training loss 0.756; learning rate 0.000121
Epoch 23 iteration 0141/0187: training loss 0.755; learning rate 0.000121
Epoch 23 iteration 0142/0187: training loss 0.757; learning rate 0.000120
Epoch 23 iteration 0143/0187: training loss 0.755; learning rate 0.000120
Epoch 23 iteration 0144/0187: training loss 0.754; learning rate 0.000120
Epoch 23 iteration 0145/0187: training loss 0.753; learning rate 0.000120
Epoch 23 iteration 0146/0187: training loss 0.755; learning rate 0.000120
Epoch 23 iteration 0147/0187: training loss 0.754; learning rate 0.000120
Epoch 23 iteration 0148/0187: training loss 0.755; learning rate 0.000120
Epoch 23 iteration 0149/0187: training loss 0.755; learning rate 0.000120
Epoch 23 iteration 0150/0187: training loss 0.754; learning rate 0.000120
Epoch 23 iteration 0151/0187: training loss 0.753; learning rate 0.000120
Epoch 23 iteration 0152/0187: training loss 0.752; learning rate 0.000120
Epoch 23 iteration 0153/0187: training loss 0.752; learning rate 0.000119
Epoch 23 iteration 0154/0187: training loss 0.753; learning rate 0.000119
Epoch 23 iteration 0155/0187: training loss 0.753; learning rate 0.000119
Epoch 23 iteration 0156/0187: training loss 0.752; learning rate 0.000119
Epoch 23 iteration 0157/0187: training loss 0.754; learning rate 0.000119
Epoch 23 iteration 0158/0187: training loss 0.756; learning rate 0.000119
Epoch 23 iteration 0159/0187: training loss 0.755; learning rate 0.000119
Epoch 23 iteration 0160/0187: training loss 0.754; learning rate 0.000119
Epoch 23 iteration 0161/0187: training loss 0.755; learning rate 0.000119
Epoch 23 iteration 0162/0187: training loss 0.756; learning rate 0.000119
Epoch 23 iteration 0163/0187: training loss 0.756; learning rate 0.000119
Epoch 23 iteration 0164/0187: training loss 0.755; learning rate 0.000118
Epoch 23 iteration 0165/0187: training loss 0.756; learning rate 0.000118
Epoch 23 iteration 0166/0187: training loss 0.756; learning rate 0.000118
Epoch 23 iteration 0167/0187: training loss 0.757; learning rate 0.000118
Epoch 23 iteration 0168/0187: training loss 0.757; learning rate 0.000118
Epoch 23 iteration 0169/0187: training loss 0.757; learning rate 0.000118
Epoch 23 iteration 0170/0187: training loss 0.756; learning rate 0.000118
Epoch 23 iteration 0171/0187: training loss 0.755; learning rate 0.000118
Epoch 23 iteration 0172/0187: training loss 0.755; learning rate 0.000118
Epoch 23 iteration 0173/0187: training loss 0.756; learning rate 0.000118
Epoch 23 iteration 0174/0187: training loss 0.755; learning rate 0.000117
Epoch 23 iteration 0175/0187: training loss 0.756; learning rate 0.000117
Epoch 23 iteration 0176/0187: training loss 0.756; learning rate 0.000117
Epoch 23 iteration 0177/0187: training loss 0.756; learning rate 0.000117
Epoch 23 iteration 0178/0187: training loss 0.756; learning rate 0.000117
Epoch 23 iteration 0179/0187: training loss 0.756; learning rate 0.000117
Epoch 23 iteration 0180/0187: training loss 0.754; learning rate 0.000117
Epoch 23 iteration 0181/0187: training loss 0.753; learning rate 0.000117
Epoch 23 iteration 0182/0187: training loss 0.753; learning rate 0.000117
Epoch 23 iteration 0183/0187: training loss 0.753; learning rate 0.000117
Epoch 23 iteration 0184/0187: training loss 0.753; learning rate 0.000117
Epoch 23 iteration 0185/0187: training loss 0.753; learning rate 0.000116
Epoch 23 iteration 0186/0187: training loss 0.752; learning rate 0.000116
Epoch 23 iteration 0187/0187: training loss 0.752; learning rate 0.000116
Epoch 23 validation pixAcc: 0.332, mIoU: 0.159
Epoch 24 iteration 0001/0187: training loss 0.831; learning rate 0.000116
Epoch 24 iteration 0002/0187: training loss 0.797; learning rate 0.000116
Epoch 24 iteration 0003/0187: training loss 0.796; learning rate 0.000116
Epoch 24 iteration 0004/0187: training loss 0.795; learning rate 0.000116
Epoch 24 iteration 0005/0187: training loss 0.786; learning rate 0.000116
Epoch 24 iteration 0006/0187: training loss 0.758; learning rate 0.000116
Epoch 24 iteration 0007/0187: training loss 0.749; learning rate 0.000115
Epoch 24 iteration 0008/0187: training loss 0.735; learning rate 0.000115
Epoch 24 iteration 0009/0187: training loss 0.725; learning rate 0.000115
Epoch 24 iteration 0010/0187: training loss 0.724; learning rate 0.000115
Epoch 24 iteration 0011/0187: training loss 0.711; learning rate 0.000115
Epoch 24 iteration 0012/0187: training loss 0.705; learning rate 0.000115
Epoch 24 iteration 0013/0187: training loss 0.724; learning rate 0.000115
Epoch 24 iteration 0014/0187: training loss 0.730; learning rate 0.000115
Epoch 24 iteration 0015/0187: training loss 0.731; learning rate 0.000115
Epoch 24 iteration 0016/0187: training loss 0.748; learning rate 0.000115
Epoch 24 iteration 0017/0187: training loss 0.747; learning rate 0.000115
Epoch 24 iteration 0018/0187: training loss 0.747; learning rate 0.000114
Epoch 24 iteration 0019/0187: training loss 0.749; learning rate 0.000114
Epoch 24 iteration 0020/0187: training loss 0.744; learning rate 0.000114
Epoch 24 iteration 0021/0187: training loss 0.743; learning rate 0.000114
Epoch 24 iteration 0022/0187: training loss 0.746; learning rate 0.000114
Epoch 24 iteration 0023/0187: training loss 0.734; learning rate 0.000114
Epoch 24 iteration 0024/0187: training loss 0.747; learning rate 0.000114
Epoch 24 iteration 0025/0187: training loss 0.743; learning rate 0.000114
Epoch 24 iteration 0026/0187: training loss 0.736; learning rate 0.000114
Epoch 24 iteration 0027/0187: training loss 0.734; learning rate 0.000114
Epoch 24 iteration 0028/0187: training loss 0.740; learning rate 0.000114
Epoch 24 iteration 0029/0187: training loss 0.736; learning rate 0.000113
Epoch 24 iteration 0030/0187: training loss 0.740; learning rate 0.000113
Epoch 24 iteration 0031/0187: training loss 0.744; learning rate 0.000113
Epoch 24 iteration 0032/0187: training loss 0.737; learning rate 0.000113
Epoch 24 iteration 0033/0187: training loss 0.732; learning rate 0.000113
Epoch 24 iteration 0034/0187: training loss 0.731; learning rate 0.000113
Epoch 24 iteration 0035/0187: training loss 0.728; learning rate 0.000113
Epoch 24 iteration 0036/0187: training loss 0.732; learning rate 0.000113
Epoch 24 iteration 0037/0187: training loss 0.728; learning rate 0.000113
Epoch 24 iteration 0038/0187: training loss 0.722; learning rate 0.000113
Epoch 24 iteration 0039/0187: training loss 0.731; learning rate 0.000112
Epoch 24 iteration 0040/0187: training loss 0.733; learning rate 0.000112
Epoch 24 iteration 0041/0187: training loss 0.731; learning rate 0.000112
Epoch 24 iteration 0042/0187: training loss 0.732; learning rate 0.000112
Epoch 24 iteration 0043/0187: training loss 0.734; learning rate 0.000112
Epoch 24 iteration 0044/0187: training loss 0.732; learning rate 0.000112
Epoch 24 iteration 0045/0187: training loss 0.732; learning rate 0.000112
Epoch 24 iteration 0046/0187: training loss 0.732; learning rate 0.000112
Epoch 24 iteration 0047/0187: training loss 0.729; learning rate 0.000112
Epoch 24 iteration 0048/0187: training loss 0.729; learning rate 0.000112
Epoch 24 iteration 0049/0187: training loss 0.733; learning rate 0.000112
Epoch 24 iteration 0050/0187: training loss 0.734; learning rate 0.000111
Epoch 24 iteration 0051/0187: training loss 0.735; learning rate 0.000111
Epoch 24 iteration 0052/0187: training loss 0.735; learning rate 0.000111
Epoch 24 iteration 0053/0187: training loss 0.736; learning rate 0.000111
Epoch 24 iteration 0054/0187: training loss 0.736; learning rate 0.000111
Epoch 24 iteration 0055/0187: training loss 0.734; learning rate 0.000111
Epoch 24 iteration 0056/0187: training loss 0.732; learning rate 0.000111
Epoch 24 iteration 0057/0187: training loss 0.731; learning rate 0.000111
Epoch 24 iteration 0058/0187: training loss 0.735; learning rate 0.000111
Epoch 24 iteration 0059/0187: training loss 0.733; learning rate 0.000111
Epoch 24 iteration 0060/0187: training loss 0.733; learning rate 0.000110
Epoch 24 iteration 0061/0187: training loss 0.736; learning rate 0.000110
Epoch 24 iteration 0062/0187: training loss 0.735; learning rate 0.000110
Epoch 24 iteration 0063/0187: training loss 0.736; learning rate 0.000110
Epoch 24 iteration 0064/0187: training loss 0.736; learning rate 0.000110
Epoch 24 iteration 0065/0187: training loss 0.734; learning rate 0.000110
Epoch 24 iteration 0066/0187: training loss 0.733; learning rate 0.000110
Epoch 24 iteration 0067/0187: training loss 0.734; learning rate 0.000110
Epoch 24 iteration 0068/0187: training loss 0.734; learning rate 0.000110
Epoch 24 iteration 0069/0187: training loss 0.734; learning rate 0.000110
Epoch 24 iteration 0070/0187: training loss 0.735; learning rate 0.000110
Epoch 24 iteration 0071/0187: training loss 0.736; learning rate 0.000109
Epoch 24 iteration 0072/0187: training loss 0.734; learning rate 0.000109
Epoch 24 iteration 0073/0187: training loss 0.733; learning rate 0.000109
Epoch 24 iteration 0074/0187: training loss 0.733; learning rate 0.000109
Epoch 24 iteration 0075/0187: training loss 0.731; learning rate 0.000109
Epoch 24 iteration 0076/0187: training loss 0.731; learning rate 0.000109
Epoch 24 iteration 0077/0187: training loss 0.732; learning rate 0.000109
Epoch 24 iteration 0078/0187: training loss 0.732; learning rate 0.000109
Epoch 24 iteration 0079/0187: training loss 0.731; learning rate 0.000109
Epoch 24 iteration 0080/0187: training loss 0.733; learning rate 0.000109
Epoch 24 iteration 0081/0187: training loss 0.734; learning rate 0.000108
Epoch 24 iteration 0082/0187: training loss 0.733; learning rate 0.000108
Epoch 24 iteration 0083/0187: training loss 0.733; learning rate 0.000108
Epoch 24 iteration 0084/0187: training loss 0.734; learning rate 0.000108
Epoch 24 iteration 0085/0187: training loss 0.733; learning rate 0.000108
Epoch 24 iteration 0086/0187: training loss 0.734; learning rate 0.000108
Epoch 24 iteration 0087/0187: training loss 0.733; learning rate 0.000108
Epoch 24 iteration 0088/0187: training loss 0.733; learning rate 0.000108
Epoch 24 iteration 0089/0187: training loss 0.734; learning rate 0.000108
Epoch 24 iteration 0090/0187: training loss 0.734; learning rate 0.000108
Epoch 24 iteration 0091/0188: training loss 0.732; learning rate 0.000108
Epoch 24 iteration 0092/0188: training loss 0.733; learning rate 0.000107
Epoch 24 iteration 0093/0188: training loss 0.734; learning rate 0.000107
Epoch 24 iteration 0094/0188: training loss 0.733; learning rate 0.000107
Epoch 24 iteration 0095/0188: training loss 0.735; learning rate 0.000107
Epoch 24 iteration 0096/0188: training loss 0.738; learning rate 0.000107
Epoch 24 iteration 0097/0188: training loss 0.738; learning rate 0.000107
Epoch 24 iteration 0098/0188: training loss 0.739; learning rate 0.000107
Epoch 24 iteration 0099/0188: training loss 0.740; learning rate 0.000107
Epoch 24 iteration 0100/0188: training loss 0.741; learning rate 0.000107
Epoch 24 iteration 0101/0188: training loss 0.740; learning rate 0.000107
Epoch 24 iteration 0102/0188: training loss 0.739; learning rate 0.000106
Epoch 24 iteration 0103/0188: training loss 0.737; learning rate 0.000106
Epoch 24 iteration 0104/0188: training loss 0.738; learning rate 0.000106
Epoch 24 iteration 0105/0188: training loss 0.741; learning rate 0.000106
Epoch 24 iteration 0106/0188: training loss 0.743; learning rate 0.000106
Epoch 24 iteration 0107/0188: training loss 0.745; learning rate 0.000106
Epoch 24 iteration 0108/0188: training loss 0.745; learning rate 0.000106
Epoch 24 iteration 0109/0188: training loss 0.744; learning rate 0.000106
Epoch 24 iteration 0110/0188: training loss 0.743; learning rate 0.000106
Epoch 24 iteration 0111/0188: training loss 0.744; learning rate 0.000106
Epoch 24 iteration 0112/0188: training loss 0.745; learning rate 0.000106
Epoch 24 iteration 0113/0188: training loss 0.745; learning rate 0.000105
Epoch 24 iteration 0114/0188: training loss 0.747; learning rate 0.000105
Epoch 24 iteration 0115/0188: training loss 0.748; learning rate 0.000105
Epoch 24 iteration 0116/0188: training loss 0.746; learning rate 0.000105
Epoch 24 iteration 0117/0188: training loss 0.746; learning rate 0.000105
Epoch 24 iteration 0118/0188: training loss 0.748; learning rate 0.000105
Epoch 24 iteration 0119/0188: training loss 0.748; learning rate 0.000105
Epoch 24 iteration 0120/0188: training loss 0.749; learning rate 0.000105
Epoch 24 iteration 0121/0188: training loss 0.749; learning rate 0.000105
Epoch 24 iteration 0122/0188: training loss 0.750; learning rate 0.000105
Epoch 24 iteration 0123/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0124/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0125/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0126/0188: training loss 0.752; learning rate 0.000104
Epoch 24 iteration 0127/0188: training loss 0.751; learning rate 0.000104
Epoch 24 iteration 0128/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0129/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0130/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0131/0188: training loss 0.749; learning rate 0.000104
Epoch 24 iteration 0132/0188: training loss 0.748; learning rate 0.000104
Epoch 24 iteration 0133/0188: training loss 0.750; learning rate 0.000104
Epoch 24 iteration 0134/0188: training loss 0.750; learning rate 0.000103
Epoch 24 iteration 0135/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0136/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0137/0188: training loss 0.748; learning rate 0.000103
Epoch 24 iteration 0138/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0139/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0140/0188: training loss 0.750; learning rate 0.000103
Epoch 24 iteration 0141/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0142/0188: training loss 0.749; learning rate 0.000103
Epoch 24 iteration 0143/0188: training loss 0.748; learning rate 0.000103
Epoch 24 iteration 0144/0188: training loss 0.750; learning rate 0.000102
Epoch 24 iteration 0145/0188: training loss 0.749; learning rate 0.000102
Epoch 24 iteration 0146/0188: training loss 0.750; learning rate 0.000102
Epoch 24 iteration 0147/0188: training loss 0.751; learning rate 0.000102
Epoch 24 iteration 0148/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0149/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0150/0188: training loss 0.753; learning rate 0.000102
Epoch 24 iteration 0151/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0152/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0153/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0154/0188: training loss 0.754; learning rate 0.000102
Epoch 24 iteration 0155/0188: training loss 0.753; learning rate 0.000101
Epoch 24 iteration 0156/0188: training loss 0.753; learning rate 0.000101
Epoch 24 iteration 0157/0188: training loss 0.754; learning rate 0.000101
Epoch 24 iteration 0158/0188: training loss 0.754; learning rate 0.000101
Epoch 24 iteration 0159/0188: training loss 0.753; learning rate 0.000101
Epoch 24 iteration 0160/0188: training loss 0.755; learning rate 0.000101
Epoch 24 iteration 0161/0188: training loss 0.755; learning rate 0.000101
Epoch 24 iteration 0162/0188: training loss 0.756; learning rate 0.000101
Epoch 24 iteration 0163/0188: training loss 0.756; learning rate 0.000101
Epoch 24 iteration 0164/0188: training loss 0.757; learning rate 0.000101
Epoch 24 iteration 0165/0188: training loss 0.758; learning rate 0.000100
Epoch 24 iteration 0166/0188: training loss 0.759; learning rate 0.000100
Epoch 24 iteration 0167/0188: training loss 0.758; learning rate 0.000100
Epoch 24 iteration 0168/0188: training loss 0.760; learning rate 0.000100
Epoch 24 iteration 0169/0188: training loss 0.759; learning rate 0.000100
Epoch 24 iteration 0170/0188: training loss 0.760; learning rate 0.000100
Epoch 24 iteration 0171/0188: training loss 0.760; learning rate 0.000100
Epoch 24 iteration 0172/0188: training loss 0.762; learning rate 0.000100
Epoch 24 iteration 0173/0188: training loss 0.764; learning rate 0.000100
Epoch 24 iteration 0174/0188: training loss 0.764; learning rate 0.000100
Epoch 24 iteration 0175/0188: training loss 0.765; learning rate 0.000100
Epoch 24 iteration 0176/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0177/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0178/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0179/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0180/0188: training loss 0.764; learning rate 0.000099
Epoch 24 iteration 0181/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0182/0188: training loss 0.765; learning rate 0.000099
Epoch 24 iteration 0183/0188: training loss 0.764; learning rate 0.000099
Epoch 24 iteration 0184/0188: training loss 0.763; learning rate 0.000099
Epoch 24 iteration 0185/0188: training loss 0.763; learning rate 0.000099
Epoch 24 iteration 0186/0188: training loss 0.763; learning rate 0.000098
Epoch 24 validation pixAcc: 0.325, mIoU: 0.163
Epoch 25 iteration 0001/0187: training loss 0.652; learning rate 0.000098
Epoch 25 iteration 0002/0187: training loss 0.730; learning rate 0.000098
Epoch 25 iteration 0003/0187: training loss 0.790; learning rate 0.000098
Epoch 25 iteration 0004/0187: training loss 0.773; learning rate 0.000098
Epoch 25 iteration 0005/0187: training loss 0.775; learning rate 0.000098
Epoch 25 iteration 0006/0187: training loss 0.760; learning rate 0.000098
Epoch 25 iteration 0007/0187: training loss 0.760; learning rate 0.000098
Epoch 25 iteration 0008/0187: training loss 0.744; learning rate 0.000098
Epoch 25 iteration 0009/0187: training loss 0.759; learning rate 0.000097
Epoch 25 iteration 0010/0187: training loss 0.766; learning rate 0.000097
Epoch 25 iteration 0011/0187: training loss 0.763; learning rate 0.000097
Epoch 25 iteration 0012/0187: training loss 0.761; learning rate 0.000097
Epoch 25 iteration 0013/0187: training loss 0.759; learning rate 0.000097
Epoch 25 iteration 0014/0187: training loss 0.758; learning rate 0.000097
Epoch 25 iteration 0015/0187: training loss 0.757; learning rate 0.000097
Epoch 25 iteration 0016/0187: training loss 0.761; learning rate 0.000097
Epoch 25 iteration 0017/0187: training loss 0.768; learning rate 0.000097
Epoch 25 iteration 0018/0187: training loss 0.774; learning rate 0.000097
Epoch 25 iteration 0019/0187: training loss 0.793; learning rate 0.000097
Epoch 25 iteration 0020/0187: training loss 0.790; learning rate 0.000096
Epoch 25 iteration 0021/0187: training loss 0.784; learning rate 0.000096
Epoch 25 iteration 0022/0187: training loss 0.782; learning rate 0.000096
Epoch 25 iteration 0023/0187: training loss 0.783; learning rate 0.000096
Epoch 25 iteration 0024/0187: training loss 0.789; learning rate 0.000096
Epoch 25 iteration 0025/0187: training loss 0.779; learning rate 0.000096
Epoch 25 iteration 0026/0187: training loss 0.779; learning rate 0.000096
Epoch 25 iteration 0027/0187: training loss 0.778; learning rate 0.000096
Epoch 25 iteration 0028/0187: training loss 0.780; learning rate 0.000096
Epoch 25 iteration 0029/0187: training loss 0.775; learning rate 0.000096
Epoch 25 iteration 0030/0187: training loss 0.774; learning rate 0.000095
Epoch 25 iteration 0031/0187: training loss 0.768; learning rate 0.000095
Epoch 25 iteration 0032/0187: training loss 0.767; learning rate 0.000095
Epoch 25 iteration 0033/0187: training loss 0.770; learning rate 0.000095
Epoch 25 iteration 0034/0187: training loss 0.765; learning rate 0.000095
Epoch 25 iteration 0035/0187: training loss 0.761; learning rate 0.000095
Epoch 25 iteration 0036/0187: training loss 0.760; learning rate 0.000095
Epoch 25 iteration 0037/0187: training loss 0.756; learning rate 0.000095
Epoch 25 iteration 0038/0187: training loss 0.750; learning rate 0.000095
Epoch 25 iteration 0039/0187: training loss 0.748; learning rate 0.000095
Epoch 25 iteration 0040/0187: training loss 0.748; learning rate 0.000095
Epoch 25 iteration 0041/0187: training loss 0.761; learning rate 0.000094
Epoch 25 iteration 0042/0187: training loss 0.767; learning rate 0.000094
Epoch 25 iteration 0043/0187: training loss 0.763; learning rate 0.000094
Epoch 25 iteration 0044/0187: training loss 0.760; learning rate 0.000094
Epoch 25 iteration 0045/0187: training loss 0.755; learning rate 0.000094
Epoch 25 iteration 0046/0187: training loss 0.754; learning rate 0.000094
Epoch 25 iteration 0047/0187: training loss 0.756; learning rate 0.000094
Epoch 25 iteration 0048/0187: training loss 0.754; learning rate 0.000094
Epoch 25 iteration 0049/0187: training loss 0.748; learning rate 0.000094
Epoch 25 iteration 0050/0187: training loss 0.751; learning rate 0.000094
Epoch 25 iteration 0051/0187: training loss 0.751; learning rate 0.000093
Epoch 25 iteration 0052/0187: training loss 0.753; learning rate 0.000093
Epoch 25 iteration 0053/0187: training loss 0.752; learning rate 0.000093
Epoch 25 iteration 0054/0187: training loss 0.754; learning rate 0.000093
Epoch 25 iteration 0055/0187: training loss 0.754; learning rate 0.000093
Epoch 25 iteration 0056/0187: training loss 0.753; learning rate 0.000093
Epoch 25 iteration 0057/0187: training loss 0.754; learning rate 0.000093
Epoch 25 iteration 0058/0187: training loss 0.753; learning rate 0.000093
Epoch 25 iteration 0059/0187: training loss 0.752; learning rate 0.000093
Epoch 25 iteration 0060/0187: training loss 0.748; learning rate 0.000093
Epoch 25 iteration 0061/0187: training loss 0.749; learning rate 0.000092
Epoch 25 iteration 0062/0187: training loss 0.748; learning rate 0.000092
Epoch 25 iteration 0063/0187: training loss 0.746; learning rate 0.000092
Epoch 25 iteration 0064/0187: training loss 0.749; learning rate 0.000092
Epoch 25 iteration 0065/0187: training loss 0.747; learning rate 0.000092
Epoch 25 iteration 0066/0187: training loss 0.744; learning rate 0.000092
Epoch 25 iteration 0067/0187: training loss 0.746; learning rate 0.000092
Epoch 25 iteration 0068/0187: training loss 0.744; learning rate 0.000092
Epoch 25 iteration 0069/0187: training loss 0.740; learning rate 0.000092
Epoch 25 iteration 0070/0187: training loss 0.737; learning rate 0.000092
Epoch 25 iteration 0071/0187: training loss 0.740; learning rate 0.000092
Epoch 25 iteration 0072/0187: training loss 0.741; learning rate 0.000091
Epoch 25 iteration 0073/0187: training loss 0.743; learning rate 0.000091
Epoch 25 iteration 0074/0187: training loss 0.744; learning rate 0.000091
Epoch 25 iteration 0075/0187: training loss 0.742; learning rate 0.000091
Epoch 25 iteration 0076/0187: training loss 0.744; learning rate 0.000091
Epoch 25 iteration 0077/0187: training loss 0.744; learning rate 0.000091
Epoch 25 iteration 0078/0187: training loss 0.749; learning rate 0.000091
Epoch 25 iteration 0079/0187: training loss 0.746; learning rate 0.000091
Epoch 25 iteration 0080/0187: training loss 0.745; learning rate 0.000091
Epoch 25 iteration 0081/0187: training loss 0.749; learning rate 0.000091
Epoch 25 iteration 0082/0187: training loss 0.751; learning rate 0.000090
Epoch 25 iteration 0083/0187: training loss 0.752; learning rate 0.000090
Epoch 25 iteration 0084/0187: training loss 0.752; learning rate 0.000090
Epoch 25 iteration 0085/0187: training loss 0.753; learning rate 0.000090
Epoch 25 iteration 0086/0187: training loss 0.750; learning rate 0.000090
Epoch 25 iteration 0087/0187: training loss 0.750; learning rate 0.000090
Epoch 25 iteration 0088/0187: training loss 0.748; learning rate 0.000090
Epoch 25 iteration 0089/0187: training loss 0.746; learning rate 0.000090
Epoch 25 iteration 0090/0187: training loss 0.747; learning rate 0.000090
Epoch 25 iteration 0091/0187: training loss 0.748; learning rate 0.000090
Epoch 25 iteration 0092/0187: training loss 0.748; learning rate 0.000089
Epoch 25 iteration 0093/0187: training loss 0.746; learning rate 0.000089
Epoch 25 iteration 0094/0187: training loss 0.746; learning rate 0.000089
Epoch 25 iteration 0095/0187: training loss 0.746; learning rate 0.000089
Epoch 25 iteration 0096/0187: training loss 0.745; learning rate 0.000089
Epoch 25 iteration 0097/0187: training loss 0.744; learning rate 0.000089
Epoch 25 iteration 0098/0187: training loss 0.745; learning rate 0.000089
Epoch 25 iteration 0099/0187: training loss 0.745; learning rate 0.000089
Epoch 25 iteration 0100/0187: training loss 0.745; learning rate 0.000089
Epoch 25 iteration 0101/0187: training loss 0.744; learning rate 0.000089
Epoch 25 iteration 0102/0187: training loss 0.747; learning rate 0.000088
Epoch 25 iteration 0103/0187: training loss 0.746; learning rate 0.000088
Epoch 25 iteration 0104/0187: training loss 0.743; learning rate 0.000088
Epoch 25 iteration 0105/0187: training loss 0.741; learning rate 0.000088
Epoch 25 iteration 0106/0187: training loss 0.742; learning rate 0.000088
Epoch 25 iteration 0107/0187: training loss 0.742; learning rate 0.000088
Epoch 25 iteration 0108/0187: training loss 0.743; learning rate 0.000088
Epoch 25 iteration 0109/0187: training loss 0.743; learning rate 0.000088
Epoch 25 iteration 0110/0187: training loss 0.743; learning rate 0.000088
Epoch 25 iteration 0111/0187: training loss 0.745; learning rate 0.000088
Epoch 25 iteration 0112/0187: training loss 0.747; learning rate 0.000088
Epoch 25 iteration 0113/0187: training loss 0.748; learning rate 0.000087
Epoch 25 iteration 0114/0187: training loss 0.750; learning rate 0.000087
Epoch 25 iteration 0115/0187: training loss 0.749; learning rate 0.000087
Epoch 25 iteration 0116/0187: training loss 0.748; learning rate 0.000087
Epoch 25 iteration 0117/0187: training loss 0.752; learning rate 0.000087
Epoch 25 iteration 0118/0187: training loss 0.750; learning rate 0.000087
Epoch 25 iteration 0119/0187: training loss 0.748; learning rate 0.000087
Epoch 25 iteration 0120/0187: training loss 0.750; learning rate 0.000087
Epoch 25 iteration 0121/0187: training loss 0.749; learning rate 0.000087
Epoch 25 iteration 0122/0187: training loss 0.749; learning rate 0.000087
Epoch 25 iteration 0123/0187: training loss 0.752; learning rate 0.000086
Epoch 25 iteration 0124/0187: training loss 0.751; learning rate 0.000086
Epoch 25 iteration 0125/0187: training loss 0.750; learning rate 0.000086
Epoch 25 iteration 0126/0187: training loss 0.750; learning rate 0.000086
Epoch 25 iteration 0127/0187: training loss 0.752; learning rate 0.000086
Epoch 25 iteration 0128/0187: training loss 0.753; learning rate 0.000086
Epoch 25 iteration 0129/0187: training loss 0.752; learning rate 0.000086
Epoch 25 iteration 0130/0187: training loss 0.753; learning rate 0.000086
Epoch 25 iteration 0131/0187: training loss 0.753; learning rate 0.000086
Epoch 25 iteration 0132/0187: training loss 0.751; learning rate 0.000086
Epoch 25 iteration 0133/0187: training loss 0.751; learning rate 0.000085
Epoch 25 iteration 0134/0187: training loss 0.750; learning rate 0.000085
Epoch 25 iteration 0135/0187: training loss 0.749; learning rate 0.000085
Epoch 25 iteration 0136/0187: training loss 0.750; learning rate 0.000085
Epoch 25 iteration 0137/0187: training loss 0.751; learning rate 0.000085
Epoch 25 iteration 0138/0187: training loss 0.750; learning rate 0.000085
Epoch 25 iteration 0139/0187: training loss 0.750; learning rate 0.000085
Epoch 25 iteration 0140/0187: training loss 0.752; learning rate 0.000085
Epoch 25 iteration 0141/0187: training loss 0.753; learning rate 0.000085
Epoch 25 iteration 0142/0187: training loss 0.752; learning rate 0.000085
Epoch 25 iteration 0143/0187: training loss 0.754; learning rate 0.000084
Epoch 25 iteration 0144/0187: training loss 0.754; learning rate 0.000084
Epoch 25 iteration 0145/0187: training loss 0.753; learning rate 0.000084
Epoch 25 iteration 0146/0187: training loss 0.753; learning rate 0.000084
Epoch 25 iteration 0147/0187: training loss 0.752; learning rate 0.000084
Epoch 25 iteration 0148/0187: training loss 0.752; learning rate 0.000084
Epoch 25 iteration 0149/0187: training loss 0.751; learning rate 0.000084
Epoch 25 iteration 0150/0187: training loss 0.750; learning rate 0.000084
Epoch 25 iteration 0151/0187: training loss 0.751; learning rate 0.000084
Epoch 25 iteration 0152/0187: training loss 0.751; learning rate 0.000084
Epoch 25 iteration 0153/0187: training loss 0.751; learning rate 0.000084
Epoch 25 iteration 0154/0187: training loss 0.751; learning rate 0.000083
Epoch 25 iteration 0155/0187: training loss 0.752; learning rate 0.000083
Epoch 25 iteration 0156/0187: training loss 0.750; learning rate 0.000083
Epoch 25 iteration 0157/0187: training loss 0.751; learning rate 0.000083
Epoch 25 iteration 0158/0187: training loss 0.752; learning rate 0.000083
Epoch 25 iteration 0159/0187: training loss 0.753; learning rate 0.000083
Epoch 25 iteration 0160/0187: training loss 0.753; learning rate 0.000083
Epoch 25 iteration 0161/0187: training loss 0.754; learning rate 0.000083
Epoch 25 iteration 0162/0187: training loss 0.753; learning rate 0.000083
Epoch 25 iteration 0163/0187: training loss 0.752; learning rate 0.000083
Epoch 25 iteration 0164/0187: training loss 0.752; learning rate 0.000082
Epoch 25 iteration 0165/0187: training loss 0.752; learning rate 0.000082
Epoch 25 iteration 0166/0187: training loss 0.751; learning rate 0.000082
Epoch 25 iteration 0167/0187: training loss 0.751; learning rate 0.000082
Epoch 25 iteration 0168/0187: training loss 0.752; learning rate 0.000082
Epoch 25 iteration 0169/0187: training loss 0.750; learning rate 0.000082
Epoch 25 iteration 0170/0187: training loss 0.749; learning rate 0.000082
Epoch 25 iteration 0171/0187: training loss 0.751; learning rate 0.000082
Epoch 25 iteration 0172/0187: training loss 0.750; learning rate 0.000082
Epoch 25 iteration 0173/0187: training loss 0.750; learning rate 0.000082
Epoch 25 iteration 0174/0187: training loss 0.749; learning rate 0.000081
Epoch 25 iteration 0175/0187: training loss 0.749; learning rate 0.000081
Epoch 25 iteration 0176/0187: training loss 0.751; learning rate 0.000081
Epoch 25 iteration 0177/0187: training loss 0.752; learning rate 0.000081
Epoch 25 iteration 0178/0187: training loss 0.751; learning rate 0.000081
Epoch 25 iteration 0179/0187: training loss 0.750; learning rate 0.000081
Epoch 25 iteration 0180/0187: training loss 0.751; learning rate 0.000081
Epoch 25 iteration 0181/0187: training loss 0.750; learning rate 0.000081
Epoch 25 iteration 0182/0187: training loss 0.750; learning rate 0.000081
Epoch 25 iteration 0183/0187: training loss 0.751; learning rate 0.000081
Epoch 25 iteration 0184/0187: training loss 0.751; learning rate 0.000080
Epoch 25 iteration 0185/0187: training loss 0.751; learning rate 0.000080
Epoch 25 iteration 0186/0187: training loss 0.752; learning rate 0.000080
Epoch 25 iteration 0187/0187: training loss 0.752; learning rate 0.000080
Epoch 25 validation pixAcc: 0.330, mIoU: 0.163
Epoch 26 iteration 0001/0187: training loss 0.716; learning rate 0.000080
Epoch 26 iteration 0002/0187: training loss 0.758; learning rate 0.000080
Epoch 26 iteration 0003/0187: training loss 0.780; learning rate 0.000080
Epoch 26 iteration 0004/0187: training loss 0.779; learning rate 0.000080
Epoch 26 iteration 0005/0187: training loss 0.824; learning rate 0.000080
Epoch 26 iteration 0006/0187: training loss 0.803; learning rate 0.000079
Epoch 26 iteration 0007/0187: training loss 0.821; learning rate 0.000079
Epoch 26 iteration 0008/0187: training loss 0.818; learning rate 0.000079
Epoch 26 iteration 0009/0187: training loss 0.797; learning rate 0.000079
Epoch 26 iteration 0010/0187: training loss 0.792; learning rate 0.000079
Epoch 26 iteration 0011/0187: training loss 0.772; learning rate 0.000079
Epoch 26 iteration 0012/0187: training loss 0.783; learning rate 0.000079
Epoch 26 iteration 0013/0187: training loss 0.784; learning rate 0.000079
Epoch 26 iteration 0014/0187: training loss 0.778; learning rate 0.000079
Epoch 26 iteration 0015/0187: training loss 0.784; learning rate 0.000079
Epoch 26 iteration 0016/0187: training loss 0.775; learning rate 0.000079
Epoch 26 iteration 0017/0187: training loss 0.780; learning rate 0.000078
Epoch 26 iteration 0018/0187: training loss 0.782; learning rate 0.000078
Epoch 26 iteration 0019/0187: training loss 0.795; learning rate 0.000078
Epoch 26 iteration 0020/0187: training loss 0.801; learning rate 0.000078
Epoch 26 iteration 0021/0187: training loss 0.801; learning rate 0.000078
Epoch 26 iteration 0022/0187: training loss 0.811; learning rate 0.000078
Epoch 26 iteration 0023/0187: training loss 0.816; learning rate 0.000078
Epoch 26 iteration 0024/0187: training loss 0.811; learning rate 0.000078
Epoch 26 iteration 0025/0187: training loss 0.801; learning rate 0.000078
Epoch 26 iteration 0026/0187: training loss 0.799; learning rate 0.000078
Epoch 26 iteration 0027/0187: training loss 0.801; learning rate 0.000077
Epoch 26 iteration 0028/0187: training loss 0.802; learning rate 0.000077
Epoch 26 iteration 0029/0187: training loss 0.798; learning rate 0.000077
Epoch 26 iteration 0030/0187: training loss 0.804; learning rate 0.000077
Epoch 26 iteration 0031/0187: training loss 0.806; learning rate 0.000077
Epoch 26 iteration 0032/0187: training loss 0.805; learning rate 0.000077
Epoch 26 iteration 0033/0187: training loss 0.803; learning rate 0.000077
Epoch 26 iteration 0034/0187: training loss 0.797; learning rate 0.000077
Epoch 26 iteration 0035/0187: training loss 0.791; learning rate 0.000077
Epoch 26 iteration 0036/0187: training loss 0.794; learning rate 0.000077
Epoch 26 iteration 0037/0187: training loss 0.793; learning rate 0.000076
Epoch 26 iteration 0038/0187: training loss 0.796; learning rate 0.000076
Epoch 26 iteration 0039/0187: training loss 0.795; learning rate 0.000076
Epoch 26 iteration 0040/0187: training loss 0.789; learning rate 0.000076
Epoch 26 iteration 0041/0187: training loss 0.784; learning rate 0.000076
Epoch 26 iteration 0042/0187: training loss 0.786; learning rate 0.000076
Epoch 26 iteration 0043/0187: training loss 0.779; learning rate 0.000076
Epoch 26 iteration 0044/0187: training loss 0.777; learning rate 0.000076
Epoch 26 iteration 0045/0187: training loss 0.773; learning rate 0.000076
Epoch 26 iteration 0046/0187: training loss 0.772; learning rate 0.000076
Epoch 26 iteration 0047/0187: training loss 0.767; learning rate 0.000075
Epoch 26 iteration 0048/0187: training loss 0.764; learning rate 0.000075
Epoch 26 iteration 0049/0187: training loss 0.764; learning rate 0.000075
Epoch 26 iteration 0050/0187: training loss 0.762; learning rate 0.000075
Epoch 26 iteration 0051/0187: training loss 0.758; learning rate 0.000075
Epoch 26 iteration 0052/0187: training loss 0.764; learning rate 0.000075
Epoch 26 iteration 0053/0187: training loss 0.766; learning rate 0.000075
Epoch 26 iteration 0054/0187: training loss 0.771; learning rate 0.000075
Epoch 26 iteration 0055/0187: training loss 0.771; learning rate 0.000075
Epoch 26 iteration 0056/0187: training loss 0.769; learning rate 0.000075
Epoch 26 iteration 0057/0187: training loss 0.771; learning rate 0.000074
Epoch 26 iteration 0058/0187: training loss 0.774; learning rate 0.000074
Epoch 26 iteration 0059/0187: training loss 0.773; learning rate 0.000074
Epoch 26 iteration 0060/0187: training loss 0.774; learning rate 0.000074
Epoch 26 iteration 0061/0187: training loss 0.772; learning rate 0.000074
Epoch 26 iteration 0062/0187: training loss 0.769; learning rate 0.000074
Epoch 26 iteration 0063/0187: training loss 0.767; learning rate 0.000074
Epoch 26 iteration 0064/0187: training loss 0.772; learning rate 0.000074
Epoch 26 iteration 0065/0187: training loss 0.773; learning rate 0.000074
Epoch 26 iteration 0066/0187: training loss 0.773; learning rate 0.000074
Epoch 26 iteration 0067/0187: training loss 0.772; learning rate 0.000073
Epoch 26 iteration 0068/0187: training loss 0.769; learning rate 0.000073
Epoch 26 iteration 0069/0187: training loss 0.769; learning rate 0.000073
Epoch 26 iteration 0070/0187: training loss 0.768; learning rate 0.000073
Epoch 26 iteration 0071/0187: training loss 0.769; learning rate 0.000073
Epoch 26 iteration 0072/0187: training loss 0.769; learning rate 0.000073
Epoch 26 iteration 0073/0187: training loss 0.767; learning rate 0.000073
Epoch 26 iteration 0074/0187: training loss 0.773; learning rate 0.000073
Epoch 26 iteration 0075/0187: training loss 0.774; learning rate 0.000073
Epoch 26 iteration 0076/0187: training loss 0.777; learning rate 0.000073
Epoch 26 iteration 0077/0187: training loss 0.774; learning rate 0.000072
Epoch 26 iteration 0078/0187: training loss 0.771; learning rate 0.000072
Epoch 26 iteration 0079/0187: training loss 0.771; learning rate 0.000072
Epoch 26 iteration 0080/0187: training loss 0.770; learning rate 0.000072
Epoch 26 iteration 0081/0187: training loss 0.769; learning rate 0.000072
Epoch 26 iteration 0082/0187: training loss 0.769; learning rate 0.000072
Epoch 26 iteration 0083/0187: training loss 0.770; learning rate 0.000072
Epoch 26 iteration 0084/0187: training loss 0.769; learning rate 0.000072
Epoch 26 iteration 0085/0187: training loss 0.771; learning rate 0.000072
Epoch 26 iteration 0086/0187: training loss 0.771; learning rate 0.000072
Epoch 26 iteration 0087/0187: training loss 0.772; learning rate 0.000071
Epoch 26 iteration 0088/0187: training loss 0.773; learning rate 0.000071
Epoch 26 iteration 0089/0187: training loss 0.770; learning rate 0.000071
Epoch 26 iteration 0090/0187: training loss 0.771; learning rate 0.000071
Epoch 26 iteration 0091/0188: training loss 0.771; learning rate 0.000071
Epoch 26 iteration 0092/0188: training loss 0.775; learning rate 0.000071
Epoch 26 iteration 0093/0188: training loss 0.776; learning rate 0.000071
Epoch 26 iteration 0094/0188: training loss 0.774; learning rate 0.000071
Epoch 26 iteration 0095/0188: training loss 0.775; learning rate 0.000071
Epoch 26 iteration 0096/0188: training loss 0.775; learning rate 0.000071
Epoch 26 iteration 0097/0188: training loss 0.776; learning rate 0.000070
Epoch 26 iteration 0098/0188: training loss 0.775; learning rate 0.000070
Epoch 26 iteration 0099/0188: training loss 0.775; learning rate 0.000070
Epoch 26 iteration 0100/0188: training loss 0.773; learning rate 0.000070
Epoch 26 iteration 0101/0188: training loss 0.773; learning rate 0.000070
Epoch 26 iteration 0102/0188: training loss 0.773; learning rate 0.000070
Epoch 26 iteration 0103/0188: training loss 0.772; learning rate 0.000070
Epoch 26 iteration 0104/0188: training loss 0.772; learning rate 0.000070
Epoch 26 iteration 0105/0188: training loss 0.771; learning rate 0.000070
Epoch 26 iteration 0106/0188: training loss 0.770; learning rate 0.000070
Epoch 26 iteration 0107/0188: training loss 0.770; learning rate 0.000069
Epoch 26 iteration 0108/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0109/0188: training loss 0.772; learning rate 0.000069
Epoch 26 iteration 0110/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0111/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0112/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0113/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0114/0188: training loss 0.772; learning rate 0.000069
Epoch 26 iteration 0115/0188: training loss 0.771; learning rate 0.000069
Epoch 26 iteration 0116/0188: training loss 0.774; learning rate 0.000069
Epoch 26 iteration 0117/0188: training loss 0.773; learning rate 0.000068
Epoch 26 iteration 0118/0188: training loss 0.771; learning rate 0.000068
Epoch 26 iteration 0119/0188: training loss 0.773; learning rate 0.000068
Epoch 26 iteration 0120/0188: training loss 0.771; learning rate 0.000068
Epoch 26 iteration 0121/0188: training loss 0.770; learning rate 0.000068
Epoch 26 iteration 0122/0188: training loss 0.770; learning rate 0.000068
Epoch 26 iteration 0123/0188: training loss 0.767; learning rate 0.000068
Epoch 26 iteration 0124/0188: training loss 0.767; learning rate 0.000068
Epoch 26 iteration 0125/0188: training loss 0.765; learning rate 0.000068
Epoch 26 iteration 0126/0188: training loss 0.765; learning rate 0.000068
Epoch 26 iteration 0127/0188: training loss 0.763; learning rate 0.000067
Epoch 26 iteration 0128/0188: training loss 0.762; learning rate 0.000067
Epoch 26 iteration 0129/0188: training loss 0.761; learning rate 0.000067
Epoch 26 iteration 0130/0188: training loss 0.761; learning rate 0.000067
Epoch 26 iteration 0131/0188: training loss 0.764; learning rate 0.000067
Epoch 26 iteration 0132/0188: training loss 0.765; learning rate 0.000067
Epoch 26 iteration 0133/0188: training loss 0.764; learning rate 0.000067
Epoch 26 iteration 0134/0188: training loss 0.766; learning rate 0.000067
Epoch 26 iteration 0135/0188: training loss 0.767; learning rate 0.000067
Epoch 26 iteration 0136/0188: training loss 0.767; learning rate 0.000067
Epoch 26 iteration 0137/0188: training loss 0.767; learning rate 0.000066
Epoch 26 iteration 0138/0188: training loss 0.766; learning rate 0.000066
Epoch 26 iteration 0139/0188: training loss 0.767; learning rate 0.000066
Epoch 26 iteration 0140/0188: training loss 0.766; learning rate 0.000066
Epoch 26 iteration 0141/0188: training loss 0.764; learning rate 0.000066
Epoch 26 iteration 0142/0188: training loss 0.765; learning rate 0.000066
Epoch 26 iteration 0143/0188: training loss 0.766; learning rate 0.000066
Epoch 26 iteration 0144/0188: training loss 0.766; learning rate 0.000066
Epoch 26 iteration 0145/0188: training loss 0.764; learning rate 0.000066
Epoch 26 iteration 0146/0188: training loss 0.764; learning rate 0.000066
Epoch 26 iteration 0147/0188: training loss 0.762; learning rate 0.000065
Epoch 26 iteration 0148/0188: training loss 0.761; learning rate 0.000065
Epoch 26 iteration 0149/0188: training loss 0.761; learning rate 0.000065
Epoch 26 iteration 0150/0188: training loss 0.760; learning rate 0.000065
Epoch 26 iteration 0151/0188: training loss 0.761; learning rate 0.000065
Epoch 26 iteration 0152/0188: training loss 0.759; learning rate 0.000065
Epoch 26 iteration 0153/0188: training loss 0.760; learning rate 0.000065
Epoch 26 iteration 0154/0188: training loss 0.761; learning rate 0.000065
Epoch 26 iteration 0155/0188: training loss 0.762; learning rate 0.000065
Epoch 26 iteration 0156/0188: training loss 0.761; learning rate 0.000065
Epoch 26 iteration 0157/0188: training loss 0.761; learning rate 0.000064
Epoch 26 iteration 0158/0188: training loss 0.759; learning rate 0.000064
Epoch 26 iteration 0159/0188: training loss 0.761; learning rate 0.000064
Epoch 26 iteration 0160/0188: training loss 0.761; learning rate 0.000064
Epoch 26 iteration 0161/0188: training loss 0.761; learning rate 0.000064
Epoch 26 iteration 0162/0188: training loss 0.760; learning rate 0.000064
Epoch 26 iteration 0163/0188: training loss 0.759; learning rate 0.000064
Epoch 26 iteration 0164/0188: training loss 0.759; learning rate 0.000064
Epoch 26 iteration 0165/0188: training loss 0.758; learning rate 0.000064
Epoch 26 iteration 0166/0188: training loss 0.757; learning rate 0.000064
Epoch 26 iteration 0167/0188: training loss 0.758; learning rate 0.000063
Epoch 26 iteration 0168/0188: training loss 0.758; learning rate 0.000063
Epoch 26 iteration 0169/0188: training loss 0.758; learning rate 0.000063
Epoch 26 iteration 0170/0188: training loss 0.758; learning rate 0.000063
Epoch 26 iteration 0171/0188: training loss 0.759; learning rate 0.000063
Epoch 26 iteration 0172/0188: training loss 0.760; learning rate 0.000063
Epoch 26 iteration 0173/0188: training loss 0.761; learning rate 0.000063
Epoch 26 iteration 0174/0188: training loss 0.761; learning rate 0.000063
Epoch 26 iteration 0175/0188: training loss 0.761; learning rate 0.000063
Epoch 26 iteration 0176/0188: training loss 0.761; learning rate 0.000063
Epoch 26 iteration 0177/0188: training loss 0.760; learning rate 0.000062
Epoch 26 iteration 0178/0188: training loss 0.758; learning rate 0.000062
Epoch 26 iteration 0179/0188: training loss 0.759; learning rate 0.000062
Epoch 26 iteration 0180/0188: training loss 0.759; learning rate 0.000062
Epoch 26 iteration 0181/0188: training loss 0.758; learning rate 0.000062
Epoch 26 iteration 0182/0188: training loss 0.758; learning rate 0.000062
Epoch 26 iteration 0183/0188: training loss 0.758; learning rate 0.000062
Epoch 26 iteration 0184/0188: training loss 0.757; learning rate 0.000062
Epoch 26 iteration 0185/0188: training loss 0.758; learning rate 0.000062
Epoch 26 iteration 0186/0188: training loss 0.757; learning rate 0.000062
Epoch 26 validation pixAcc: 0.332, mIoU: 0.167
Epoch 27 iteration 0001/0187: training loss 0.799; learning rate 0.000061
Epoch 27 iteration 0002/0187: training loss 0.910; learning rate 0.000061
Epoch 27 iteration 0003/0187: training loss 0.881; learning rate 0.000061
Epoch 27 iteration 0004/0187: training loss 0.863; learning rate 0.000061
Epoch 27 iteration 0005/0187: training loss 0.813; learning rate 0.000061
Epoch 27 iteration 0006/0187: training loss 0.786; learning rate 0.000061
Epoch 27 iteration 0007/0187: training loss 0.785; learning rate 0.000061
Epoch 27 iteration 0008/0187: training loss 0.775; learning rate 0.000061
Epoch 27 iteration 0009/0187: training loss 0.785; learning rate 0.000061
Epoch 27 iteration 0010/0187: training loss 0.787; learning rate 0.000060
Epoch 27 iteration 0011/0187: training loss 0.787; learning rate 0.000060
Epoch 27 iteration 0012/0187: training loss 0.779; learning rate 0.000060
Epoch 27 iteration 0013/0187: training loss 0.780; learning rate 0.000060
Epoch 27 iteration 0014/0187: training loss 0.771; learning rate 0.000060
Epoch 27 iteration 0015/0187: training loss 0.760; learning rate 0.000060
Epoch 27 iteration 0016/0187: training loss 0.750; learning rate 0.000060
Epoch 27 iteration 0017/0187: training loss 0.735; learning rate 0.000060
Epoch 27 iteration 0018/0187: training loss 0.725; learning rate 0.000060
Epoch 27 iteration 0019/0187: training loss 0.719; learning rate 0.000060
Epoch 27 iteration 0020/0187: training loss 0.716; learning rate 0.000059
Epoch 27 iteration 0021/0187: training loss 0.719; learning rate 0.000059
Epoch 27 iteration 0022/0187: training loss 0.717; learning rate 0.000059
Epoch 27 iteration 0023/0187: training loss 0.715; learning rate 0.000059
Epoch 27 iteration 0024/0187: training loss 0.727; learning rate 0.000059
Epoch 27 iteration 0025/0187: training loss 0.729; learning rate 0.000059
Epoch 27 iteration 0026/0187: training loss 0.726; learning rate 0.000059
Epoch 27 iteration 0027/0187: training loss 0.717; learning rate 0.000059
Epoch 27 iteration 0028/0187: training loss 0.715; learning rate 0.000059
Epoch 27 iteration 0029/0187: training loss 0.714; learning rate 0.000058
Epoch 27 iteration 0030/0187: training loss 0.727; learning rate 0.000058
Epoch 27 iteration 0031/0187: training loss 0.729; learning rate 0.000058
Epoch 27 iteration 0032/0187: training loss 0.723; learning rate 0.000058
Epoch 27 iteration 0033/0187: training loss 0.727; learning rate 0.000058
Epoch 27 iteration 0034/0187: training loss 0.729; learning rate 0.000058
Epoch 27 iteration 0035/0187: training loss 0.734; learning rate 0.000058
Epoch 27 iteration 0036/0187: training loss 0.736; learning rate 0.000058
Epoch 27 iteration 0037/0187: training loss 0.738; learning rate 0.000058
Epoch 27 iteration 0038/0187: training loss 0.734; learning rate 0.000058
Epoch 27 iteration 0039/0187: training loss 0.738; learning rate 0.000057
Epoch 27 iteration 0040/0187: training loss 0.739; learning rate 0.000057
Epoch 27 iteration 0041/0187: training loss 0.738; learning rate 0.000057
Epoch 27 iteration 0042/0187: training loss 0.732; learning rate 0.000057
Epoch 27 iteration 0043/0187: training loss 0.732; learning rate 0.000057
Epoch 27 iteration 0044/0187: training loss 0.732; learning rate 0.000057
Epoch 27 iteration 0045/0187: training loss 0.731; learning rate 0.000057
Epoch 27 iteration 0046/0187: training loss 0.733; learning rate 0.000057
Epoch 27 iteration 0047/0187: training loss 0.731; learning rate 0.000057
Epoch 27 iteration 0048/0187: training loss 0.737; learning rate 0.000057
Epoch 27 iteration 0049/0187: training loss 0.738; learning rate 0.000056
Epoch 27 iteration 0050/0187: training loss 0.736; learning rate 0.000056
Epoch 27 iteration 0051/0187: training loss 0.733; learning rate 0.000056
Epoch 27 iteration 0052/0187: training loss 0.732; learning rate 0.000056
Epoch 27 iteration 0053/0187: training loss 0.728; learning rate 0.000056
Epoch 27 iteration 0054/0187: training loss 0.729; learning rate 0.000056
Epoch 27 iteration 0055/0187: training loss 0.730; learning rate 0.000056
Epoch 27 iteration 0056/0187: training loss 0.733; learning rate 0.000056
Epoch 27 iteration 0057/0187: training loss 0.735; learning rate 0.000056
Epoch 27 iteration 0058/0187: training loss 0.734; learning rate 0.000056
Epoch 27 iteration 0059/0187: training loss 0.734; learning rate 0.000055
Epoch 27 iteration 0060/0187: training loss 0.735; learning rate 0.000055
Epoch 27 iteration 0061/0187: training loss 0.741; learning rate 0.000055
Epoch 27 iteration 0062/0187: training loss 0.742; learning rate 0.000055
Epoch 27 iteration 0063/0187: training loss 0.745; learning rate 0.000055
Epoch 27 iteration 0064/0187: training loss 0.743; learning rate 0.000055
Epoch 27 iteration 0065/0187: training loss 0.744; learning rate 0.000055
Epoch 27 iteration 0066/0187: training loss 0.741; learning rate 0.000055
Epoch 27 iteration 0067/0187: training loss 0.739; learning rate 0.000055
Epoch 27 iteration 0068/0187: training loss 0.736; learning rate 0.000055
Epoch 27 iteration 0069/0187: training loss 0.736; learning rate 0.000054
Epoch 27 iteration 0070/0187: training loss 0.737; learning rate 0.000054
Epoch 27 iteration 0071/0187: training loss 0.737; learning rate 0.000054
Epoch 27 iteration 0072/0187: training loss 0.739; learning rate 0.000054
Epoch 27 iteration 0073/0187: training loss 0.738; learning rate 0.000054
Epoch 27 iteration 0074/0187: training loss 0.738; learning rate 0.000054
Epoch 27 iteration 0075/0187: training loss 0.737; learning rate 0.000054
Epoch 27 iteration 0076/0187: training loss 0.736; learning rate 0.000054
Epoch 27 iteration 0077/0187: training loss 0.737; learning rate 0.000054
Epoch 27 iteration 0078/0187: training loss 0.736; learning rate 0.000053
Epoch 27 iteration 0079/0187: training loss 0.737; learning rate 0.000053
Epoch 27 iteration 0080/0187: training loss 0.740; learning rate 0.000053
Epoch 27 iteration 0081/0187: training loss 0.739; learning rate 0.000053
Epoch 27 iteration 0082/0187: training loss 0.742; learning rate 0.000053
Epoch 27 iteration 0083/0187: training loss 0.743; learning rate 0.000053
Epoch 27 iteration 0084/0187: training loss 0.743; learning rate 0.000053
Epoch 27 iteration 0085/0187: training loss 0.741; learning rate 0.000053
Epoch 27 iteration 0086/0187: training loss 0.742; learning rate 0.000053
Epoch 27 iteration 0087/0187: training loss 0.741; learning rate 0.000053
Epoch 27 iteration 0088/0187: training loss 0.741; learning rate 0.000052
Epoch 27 iteration 0089/0187: training loss 0.740; learning rate 0.000052
Epoch 27 iteration 0090/0187: training loss 0.737; learning rate 0.000052
Epoch 27 iteration 0091/0187: training loss 0.740; learning rate 0.000052
Epoch 27 iteration 0092/0187: training loss 0.739; learning rate 0.000052
Epoch 27 iteration 0093/0187: training loss 0.737; learning rate 0.000052
Epoch 27 iteration 0094/0187: training loss 0.739; learning rate 0.000052
Epoch 27 iteration 0095/0187: training loss 0.739; learning rate 0.000052
Epoch 27 iteration 0096/0187: training loss 0.741; learning rate 0.000052
Epoch 27 iteration 0097/0187: training loss 0.742; learning rate 0.000052
Epoch 27 iteration 0098/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0099/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0100/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0101/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0102/0187: training loss 0.739; learning rate 0.000051
Epoch 27 iteration 0103/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0104/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0105/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0106/0187: training loss 0.740; learning rate 0.000051
Epoch 27 iteration 0107/0187: training loss 0.740; learning rate 0.000050
Epoch 27 iteration 0108/0187: training loss 0.740; learning rate 0.000050
Epoch 27 iteration 0109/0187: training loss 0.739; learning rate 0.000050
Epoch 27 iteration 0110/0187: training loss 0.738; learning rate 0.000050
Epoch 27 iteration 0111/0187: training loss 0.738; learning rate 0.000050
Epoch 27 iteration 0112/0187: training loss 0.737; learning rate 0.000050
Epoch 27 iteration 0113/0187: training loss 0.737; learning rate 0.000050
Epoch 27 iteration 0114/0187: training loss 0.736; learning rate 0.000050
Epoch 27 iteration 0115/0187: training loss 0.736; learning rate 0.000050
Epoch 27 iteration 0116/0187: training loss 0.734; learning rate 0.000050
Epoch 27 iteration 0117/0187: training loss 0.734; learning rate 0.000049
Epoch 27 iteration 0118/0187: training loss 0.734; learning rate 0.000049
Epoch 27 iteration 0119/0187: training loss 0.735; learning rate 0.000049
Epoch 27 iteration 0120/0187: training loss 0.735; learning rate 0.000049
Epoch 27 iteration 0121/0187: training loss 0.735; learning rate 0.000049
Epoch 27 iteration 0122/0187: training loss 0.735; learning rate 0.000049
Epoch 27 iteration 0123/0187: training loss 0.736; learning rate 0.000049
Epoch 27 iteration 0124/0187: training loss 0.736; learning rate 0.000049
Epoch 27 iteration 0125/0187: training loss 0.735; learning rate 0.000049
Epoch 27 iteration 0126/0187: training loss 0.734; learning rate 0.000049
Epoch 27 iteration 0127/0187: training loss 0.736; learning rate 0.000048
Epoch 27 iteration 0128/0187: training loss 0.737; learning rate 0.000048
Epoch 27 iteration 0129/0187: training loss 0.736; learning rate 0.000048
Epoch 27 iteration 0130/0187: training loss 0.736; learning rate 0.000048
Epoch 27 iteration 0131/0187: training loss 0.738; learning rate 0.000048
Epoch 27 iteration 0132/0187: training loss 0.738; learning rate 0.000048
Epoch 27 iteration 0133/0187: training loss 0.737; learning rate 0.000048
Epoch 27 iteration 0134/0187: training loss 0.738; learning rate 0.000048
Epoch 27 iteration 0135/0187: training loss 0.739; learning rate 0.000048
Epoch 27 iteration 0136/0187: training loss 0.739; learning rate 0.000047
Epoch 27 iteration 0137/0187: training loss 0.739; learning rate 0.000047
Epoch 27 iteration 0138/0187: training loss 0.738; learning rate 0.000047
Epoch 27 iteration 0139/0187: training loss 0.738; learning rate 0.000047
Epoch 27 iteration 0140/0187: training loss 0.738; learning rate 0.000047
Epoch 27 iteration 0141/0187: training loss 0.738; learning rate 0.000047
Epoch 27 iteration 0142/0187: training loss 0.737; learning rate 0.000047
Epoch 27 iteration 0143/0187: training loss 0.737; learning rate 0.000047
Epoch 27 iteration 0144/0187: training loss 0.736; learning rate 0.000047
Epoch 27 iteration 0145/0187: training loss 0.737; learning rate 0.000047
Epoch 27 iteration 0146/0187: training loss 0.736; learning rate 0.000046
Epoch 27 iteration 0147/0187: training loss 0.737; learning rate 0.000046
Epoch 27 iteration 0148/0187: training loss 0.735; learning rate 0.000046
Epoch 27 iteration 0149/0187: training loss 0.735; learning rate 0.000046
Epoch 27 iteration 0150/0187: training loss 0.736; learning rate 0.000046
Epoch 27 iteration 0151/0187: training loss 0.736; learning rate 0.000046
Epoch 27 iteration 0152/0187: training loss 0.738; learning rate 0.000046
Epoch 27 iteration 0153/0187: training loss 0.738; learning rate 0.000046
Epoch 27 iteration 0154/0187: training loss 0.739; learning rate 0.000046
Epoch 27 iteration 0155/0187: training loss 0.738; learning rate 0.000045
Epoch 27 iteration 0156/0187: training loss 0.738; learning rate 0.000045
Epoch 27 iteration 0157/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0158/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0159/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0160/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0161/0187: training loss 0.736; learning rate 0.000045
Epoch 27 iteration 0162/0187: training loss 0.736; learning rate 0.000045
Epoch 27 iteration 0163/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0164/0187: training loss 0.737; learning rate 0.000045
Epoch 27 iteration 0165/0187: training loss 0.735; learning rate 0.000044
Epoch 27 iteration 0166/0187: training loss 0.735; learning rate 0.000044
Epoch 27 iteration 0167/0187: training loss 0.734; learning rate 0.000044
Epoch 27 iteration 0168/0187: training loss 0.736; learning rate 0.000044
Epoch 27 iteration 0169/0187: training loss 0.736; learning rate 0.000044
Epoch 27 iteration 0170/0187: training loss 0.737; learning rate 0.000044
Epoch 27 iteration 0171/0187: training loss 0.738; learning rate 0.000044
Epoch 27 iteration 0172/0187: training loss 0.738; learning rate 0.000044
Epoch 27 iteration 0173/0187: training loss 0.739; learning rate 0.000044
Epoch 27 iteration 0174/0187: training loss 0.739; learning rate 0.000043
Epoch 27 iteration 0175/0187: training loss 0.738; learning rate 0.000043
Epoch 27 iteration 0176/0187: training loss 0.737; learning rate 0.000043
Epoch 27 iteration 0177/0187: training loss 0.737; learning rate 0.000043
Epoch 27 iteration 0178/0187: training loss 0.738; learning rate 0.000043
Epoch 27 iteration 0179/0187: training loss 0.738; learning rate 0.000043
Epoch 27 iteration 0180/0187: training loss 0.739; learning rate 0.000043
Epoch 27 iteration 0181/0187: training loss 0.738; learning rate 0.000043
Epoch 27 iteration 0182/0187: training loss 0.739; learning rate 0.000043
Epoch 27 iteration 0183/0187: training loss 0.740; learning rate 0.000043
Epoch 27 iteration 0184/0187: training loss 0.740; learning rate 0.000042
Epoch 27 iteration 0185/0187: training loss 0.741; learning rate 0.000042
Epoch 27 iteration 0186/0187: training loss 0.742; learning rate 0.000042
Epoch 27 iteration 0187/0187: training loss 0.742; learning rate 0.000042
Epoch 27 validation pixAcc: 0.333, mIoU: 0.174
Epoch 28 iteration 0001/0187: training loss 0.682; learning rate 0.000042
Epoch 28 iteration 0002/0187: training loss 0.718; learning rate 0.000042
Epoch 28 iteration 0003/0187: training loss 0.688; learning rate 0.000042
Epoch 28 iteration 0004/0187: training loss 0.692; learning rate 0.000042
Epoch 28 iteration 0005/0187: training loss 0.674; learning rate 0.000041
Epoch 28 iteration 0006/0187: training loss 0.674; learning rate 0.000041
Epoch 28 iteration 0007/0187: training loss 0.702; learning rate 0.000041
Epoch 28 iteration 0008/0187: training loss 0.698; learning rate 0.000041
Epoch 28 iteration 0009/0187: training loss 0.682; learning rate 0.000041
Epoch 28 iteration 0010/0187: training loss 0.677; learning rate 0.000041
Epoch 28 iteration 0011/0187: training loss 0.681; learning rate 0.000041
Epoch 28 iteration 0012/0187: training loss 0.684; learning rate 0.000041
Epoch 28 iteration 0013/0187: training loss 0.683; learning rate 0.000041
Epoch 28 iteration 0014/0187: training loss 0.687; learning rate 0.000041
Epoch 28 iteration 0015/0187: training loss 0.685; learning rate 0.000040
Epoch 28 iteration 0016/0187: training loss 0.698; learning rate 0.000040
Epoch 28 iteration 0017/0187: training loss 0.689; learning rate 0.000040
Epoch 28 iteration 0018/0187: training loss 0.679; learning rate 0.000040
Epoch 28 iteration 0019/0187: training loss 0.677; learning rate 0.000040
Epoch 28 iteration 0020/0187: training loss 0.679; learning rate 0.000040
Epoch 28 iteration 0021/0187: training loss 0.691; learning rate 0.000040
Epoch 28 iteration 0022/0187: training loss 0.689; learning rate 0.000040
Epoch 28 iteration 0023/0187: training loss 0.695; learning rate 0.000040
Epoch 28 iteration 0024/0187: training loss 0.700; learning rate 0.000039
Epoch 28 iteration 0025/0187: training loss 0.708; learning rate 0.000039
Epoch 28 iteration 0026/0187: training loss 0.717; learning rate 0.000039
Epoch 28 iteration 0027/0187: training loss 0.717; learning rate 0.000039
Epoch 28 iteration 0028/0187: training loss 0.715; learning rate 0.000039
Epoch 28 iteration 0029/0187: training loss 0.717; learning rate 0.000039
Epoch 28 iteration 0030/0187: training loss 0.724; learning rate 0.000039
Epoch 28 iteration 0031/0187: training loss 0.722; learning rate 0.000039
Epoch 28 iteration 0032/0187: training loss 0.722; learning rate 0.000039
Epoch 28 iteration 0033/0187: training loss 0.722; learning rate 0.000039
Epoch 28 iteration 0034/0187: training loss 0.721; learning rate 0.000038
Epoch 28 iteration 0035/0187: training loss 0.721; learning rate 0.000038
Epoch 28 iteration 0036/0187: training loss 0.724; learning rate 0.000038
Epoch 28 iteration 0037/0187: training loss 0.719; learning rate 0.000038
Epoch 28 iteration 0038/0187: training loss 0.713; learning rate 0.000038
Epoch 28 iteration 0039/0187: training loss 0.718; learning rate 0.000038
Epoch 28 iteration 0040/0187: training loss 0.728; learning rate 0.000038
Epoch 28 iteration 0041/0187: training loss 0.730; learning rate 0.000038
Epoch 28 iteration 0042/0187: training loss 0.724; learning rate 0.000038
Epoch 28 iteration 0043/0187: training loss 0.729; learning rate 0.000037
Epoch 28 iteration 0044/0187: training loss 0.724; learning rate 0.000037
Epoch 28 iteration 0045/0187: training loss 0.722; learning rate 0.000037
Epoch 28 iteration 0046/0187: training loss 0.717; learning rate 0.000037
Epoch 28 iteration 0047/0187: training loss 0.715; learning rate 0.000037
Epoch 28 iteration 0048/0187: training loss 0.713; learning rate 0.000037
Epoch 28 iteration 0049/0187: training loss 0.713; learning rate 0.000037
Epoch 28 iteration 0050/0187: training loss 0.712; learning rate 0.000037
Epoch 28 iteration 0051/0187: training loss 0.712; learning rate 0.000037
Epoch 28 iteration 0052/0187: training loss 0.713; learning rate 0.000036
Epoch 28 iteration 0053/0187: training loss 0.713; learning rate 0.000036
Epoch 28 iteration 0054/0187: training loss 0.712; learning rate 0.000036
Epoch 28 iteration 0055/0187: training loss 0.710; learning rate 0.000036
Epoch 28 iteration 0056/0187: training loss 0.707; learning rate 0.000036
Epoch 28 iteration 0057/0187: training loss 0.711; learning rate 0.000036
Epoch 28 iteration 0058/0187: training loss 0.712; learning rate 0.000036
Epoch 28 iteration 0059/0187: training loss 0.710; learning rate 0.000036
Epoch 28 iteration 0060/0187: training loss 0.711; learning rate 0.000036
Epoch 28 iteration 0061/0187: training loss 0.711; learning rate 0.000036
Epoch 28 iteration 0062/0187: training loss 0.709; learning rate 0.000035
Epoch 28 iteration 0063/0187: training loss 0.708; learning rate 0.000035
Epoch 28 iteration 0064/0187: training loss 0.710; learning rate 0.000035
Epoch 28 iteration 0065/0187: training loss 0.708; learning rate 0.000035
Epoch 28 iteration 0066/0187: training loss 0.708; learning rate 0.000035
Epoch 28 iteration 0067/0187: training loss 0.708; learning rate 0.000035
Epoch 28 iteration 0068/0187: training loss 0.708; learning rate 0.000035
Epoch 28 iteration 0069/0187: training loss 0.705; learning rate 0.000035
Epoch 28 iteration 0070/0187: training loss 0.706; learning rate 0.000035
Epoch 28 iteration 0071/0187: training loss 0.708; learning rate 0.000034
Epoch 28 iteration 0072/0187: training loss 0.712; learning rate 0.000034
Epoch 28 iteration 0073/0187: training loss 0.712; learning rate 0.000034
Epoch 28 iteration 0074/0187: training loss 0.714; learning rate 0.000034
Epoch 28 iteration 0075/0187: training loss 0.713; learning rate 0.000034
Epoch 28 iteration 0076/0187: training loss 0.712; learning rate 0.000034
Epoch 28 iteration 0077/0187: training loss 0.713; learning rate 0.000034
Epoch 28 iteration 0078/0187: training loss 0.714; learning rate 0.000034
Epoch 28 iteration 0079/0187: training loss 0.715; learning rate 0.000034
Epoch 28 iteration 0080/0187: training loss 0.715; learning rate 0.000033
Epoch 28 iteration 0081/0187: training loss 0.713; learning rate 0.000033
Epoch 28 iteration 0082/0187: training loss 0.711; learning rate 0.000033
Epoch 28 iteration 0083/0187: training loss 0.710; learning rate 0.000033
Epoch 28 iteration 0084/0187: training loss 0.712; learning rate 0.000033
Epoch 28 iteration 0085/0187: training loss 0.709; learning rate 0.000033
Epoch 28 iteration 0086/0187: training loss 0.706; learning rate 0.000033
Epoch 28 iteration 0087/0187: training loss 0.704; learning rate 0.000033
Epoch 28 iteration 0088/0187: training loss 0.706; learning rate 0.000033
Epoch 28 iteration 0089/0187: training loss 0.706; learning rate 0.000032
Epoch 28 iteration 0090/0187: training loss 0.707; learning rate 0.000032
Epoch 28 iteration 0091/0188: training loss 0.704; learning rate 0.000032
Epoch 28 iteration 0092/0188: training loss 0.704; learning rate 0.000032
Epoch 28 iteration 0093/0188: training loss 0.710; learning rate 0.000032
Epoch 28 iteration 0094/0188: training loss 0.709; learning rate 0.000032
Epoch 28 iteration 0095/0188: training loss 0.709; learning rate 0.000032
Epoch 28 iteration 0096/0188: training loss 0.709; learning rate 0.000032
Epoch 28 iteration 0097/0188: training loss 0.707; learning rate 0.000032
Epoch 28 iteration 0098/0188: training loss 0.708; learning rate 0.000032
Epoch 28 iteration 0099/0188: training loss 0.708; learning rate 0.000031
Epoch 28 iteration 0100/0188: training loss 0.709; learning rate 0.000031
Epoch 28 iteration 0101/0188: training loss 0.709; learning rate 0.000031
Epoch 28 iteration 0102/0188: training loss 0.709; learning rate 0.000031
Epoch 28 iteration 0103/0188: training loss 0.710; learning rate 0.000031
Epoch 28 iteration 0104/0188: training loss 0.709; learning rate 0.000031
Epoch 28 iteration 0105/0188: training loss 0.710; learning rate 0.000031
Epoch 28 iteration 0106/0188: training loss 0.711; learning rate 0.000031
Epoch 28 iteration 0107/0188: training loss 0.710; learning rate 0.000031
Epoch 28 iteration 0108/0188: training loss 0.711; learning rate 0.000030
Epoch 28 iteration 0109/0188: training loss 0.710; learning rate 0.000030
Epoch 28 iteration 0110/0188: training loss 0.711; learning rate 0.000030
Epoch 28 iteration 0111/0188: training loss 0.713; learning rate 0.000030
Epoch 28 iteration 0112/0188: training loss 0.711; learning rate 0.000030
Epoch 28 iteration 0113/0188: training loss 0.711; learning rate 0.000030
Epoch 28 iteration 0114/0188: training loss 0.710; learning rate 0.000030
Epoch 28 iteration 0115/0188: training loss 0.713; learning rate 0.000030
Epoch 28 iteration 0116/0188: training loss 0.713; learning rate 0.000030
Epoch 28 iteration 0117/0188: training loss 0.713; learning rate 0.000029
Epoch 28 iteration 0118/0188: training loss 0.712; learning rate 0.000029
Epoch 28 iteration 0119/0188: training loss 0.711; learning rate 0.000029
Epoch 28 iteration 0120/0188: training loss 0.713; learning rate 0.000029
Epoch 28 iteration 0121/0188: training loss 0.713; learning rate 0.000029
Epoch 28 iteration 0122/0188: training loss 0.713; learning rate 0.000029
Epoch 28 iteration 0123/0188: training loss 0.712; learning rate 0.000029
Epoch 28 iteration 0124/0188: training loss 0.711; learning rate 0.000029
Epoch 28 iteration 0125/0188: training loss 0.710; learning rate 0.000029
Epoch 28 iteration 0126/0188: training loss 0.709; learning rate 0.000028
Epoch 28 iteration 0127/0188: training loss 0.710; learning rate 0.000028
Epoch 28 iteration 0128/0188: training loss 0.710; learning rate 0.000028
Epoch 28 iteration 0129/0188: training loss 0.709; learning rate 0.000028
Epoch 28 iteration 0130/0188: training loss 0.710; learning rate 0.000028
Epoch 28 iteration 0131/0188: training loss 0.710; learning rate 0.000028
Epoch 28 iteration 0132/0188: training loss 0.710; learning rate 0.000028
Epoch 28 iteration 0133/0188: training loss 0.709; learning rate 0.000028
Epoch 28 iteration 0134/0188: training loss 0.711; learning rate 0.000028
Epoch 28 iteration 0135/0188: training loss 0.710; learning rate 0.000027
Epoch 28 iteration 0136/0188: training loss 0.710; learning rate 0.000027
Epoch 28 iteration 0137/0188: training loss 0.709; learning rate 0.000027
Epoch 28 iteration 0138/0188: training loss 0.710; learning rate 0.000027
Epoch 28 iteration 0139/0188: training loss 0.712; learning rate 0.000027
Epoch 28 iteration 0140/0188: training loss 0.711; learning rate 0.000027
Epoch 28 iteration 0141/0188: training loss 0.711; learning rate 0.000027
Epoch 28 iteration 0142/0188: training loss 0.711; learning rate 0.000027
Epoch 28 iteration 0143/0188: training loss 0.710; learning rate 0.000027
Epoch 28 iteration 0144/0188: training loss 0.711; learning rate 0.000026
Epoch 28 iteration 0145/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0146/0188: training loss 0.709; learning rate 0.000026
Epoch 28 iteration 0147/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0148/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0149/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0150/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0151/0188: training loss 0.710; learning rate 0.000026
Epoch 28 iteration 0152/0188: training loss 0.709; learning rate 0.000026
Epoch 28 iteration 0153/0188: training loss 0.710; learning rate 0.000025
Epoch 28 iteration 0154/0188: training loss 0.710; learning rate 0.000025
Epoch 28 iteration 0155/0188: training loss 0.710; learning rate 0.000025
Epoch 28 iteration 0156/0188: training loss 0.709; learning rate 0.000025
Epoch 28 iteration 0157/0188: training loss 0.709; learning rate 0.000025
Epoch 28 iteration 0158/0188: training loss 0.712; learning rate 0.000025
Epoch 28 iteration 0159/0188: training loss 0.714; learning rate 0.000025
Epoch 28 iteration 0160/0188: training loss 0.714; learning rate 0.000025
Epoch 28 iteration 0161/0188: training loss 0.715; learning rate 0.000025
Epoch 28 iteration 0162/0188: training loss 0.718; learning rate 0.000024
Epoch 28 iteration 0163/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0164/0188: training loss 0.716; learning rate 0.000024
Epoch 28 iteration 0165/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0166/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0167/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0168/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0169/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0170/0188: training loss 0.717; learning rate 0.000024
Epoch 28 iteration 0171/0188: training loss 0.716; learning rate 0.000023
Epoch 28 iteration 0172/0188: training loss 0.716; learning rate 0.000023
Epoch 28 iteration 0173/0188: training loss 0.717; learning rate 0.000023
Epoch 28 iteration 0174/0188: training loss 0.717; learning rate 0.000023
Epoch 28 iteration 0175/0188: training loss 0.717; learning rate 0.000023
Epoch 28 iteration 0176/0188: training loss 0.717; learning rate 0.000023
Epoch 28 iteration 0177/0188: training loss 0.719; learning rate 0.000023
Epoch 28 iteration 0178/0188: training loss 0.719; learning rate 0.000023
Epoch 28 iteration 0179/0188: training loss 0.720; learning rate 0.000023
Epoch 28 iteration 0180/0188: training loss 0.720; learning rate 0.000022
Epoch 28 iteration 0181/0188: training loss 0.719; learning rate 0.000022
Epoch 28 iteration 0182/0188: training loss 0.720; learning rate 0.000022
Epoch 28 iteration 0183/0188: training loss 0.720; learning rate 0.000022
Epoch 28 iteration 0184/0188: training loss 0.720; learning rate 0.000022
Epoch 28 iteration 0185/0188: training loss 0.720; learning rate 0.000022
Epoch 28 iteration 0186/0188: training loss 0.720; learning rate 0.000022
Epoch 28 validation pixAcc: 0.334, mIoU: 0.168
Epoch 29 iteration 0001/0187: training loss 0.640; learning rate 0.000021
Epoch 29 iteration 0002/0187: training loss 0.600; learning rate 0.000021
Epoch 29 iteration 0003/0187: training loss 0.726; learning rate 0.000021
Epoch 29 iteration 0004/0187: training loss 0.716; learning rate 0.000021
Epoch 29 iteration 0005/0187: training loss 0.723; learning rate 0.000021
Epoch 29 iteration 0006/0187: training loss 0.719; learning rate 0.000021
Epoch 29 iteration 0007/0187: training loss 0.719; learning rate 0.000021
Epoch 29 iteration 0008/0187: training loss 0.725; learning rate 0.000021
Epoch 29 iteration 0009/0187: training loss 0.718; learning rate 0.000021
Epoch 29 iteration 0010/0187: training loss 0.705; learning rate 0.000020
Epoch 29 iteration 0011/0187: training loss 0.694; learning rate 0.000020
Epoch 29 iteration 0012/0187: training loss 0.679; learning rate 0.000020
Epoch 29 iteration 0013/0187: training loss 0.670; learning rate 0.000020
Epoch 29 iteration 0014/0187: training loss 0.667; learning rate 0.000020
Epoch 29 iteration 0015/0187: training loss 0.650; learning rate 0.000020
Epoch 29 iteration 0016/0187: training loss 0.639; learning rate 0.000020
Epoch 29 iteration 0017/0187: training loss 0.641; learning rate 0.000020
Epoch 29 iteration 0018/0187: training loss 0.651; learning rate 0.000020
Epoch 29 iteration 0019/0187: training loss 0.642; learning rate 0.000019
Epoch 29 iteration 0020/0187: training loss 0.641; learning rate 0.000019
Epoch 29 iteration 0021/0187: training loss 0.638; learning rate 0.000019
Epoch 29 iteration 0022/0187: training loss 0.636; learning rate 0.000019
Epoch 29 iteration 0023/0187: training loss 0.634; learning rate 0.000019
Epoch 29 iteration 0024/0187: training loss 0.646; learning rate 0.000019
Epoch 29 iteration 0025/0187: training loss 0.634; learning rate 0.000019
Epoch 29 iteration 0026/0187: training loss 0.628; learning rate 0.000019
Epoch 29 iteration 0027/0187: training loss 0.627; learning rate 0.000019
Epoch 29 iteration 0028/0187: training loss 0.626; learning rate 0.000018
Epoch 29 iteration 0029/0187: training loss 0.636; learning rate 0.000018
Epoch 29 iteration 0030/0187: training loss 0.634; learning rate 0.000018
Epoch 29 iteration 0031/0187: training loss 0.630; learning rate 0.000018
Epoch 29 iteration 0032/0187: training loss 0.635; learning rate 0.000018
Epoch 29 iteration 0033/0187: training loss 0.635; learning rate 0.000018
Epoch 29 iteration 0034/0187: training loss 0.632; learning rate 0.000018
Epoch 29 iteration 0035/0187: training loss 0.634; learning rate 0.000018
Epoch 29 iteration 0036/0187: training loss 0.634; learning rate 0.000017
Epoch 29 iteration 0037/0187: training loss 0.637; learning rate 0.000017
Epoch 29 iteration 0038/0187: training loss 0.639; learning rate 0.000017
Epoch 29 iteration 0039/0187: training loss 0.649; learning rate 0.000017
Epoch 29 iteration 0040/0187: training loss 0.646; learning rate 0.000017
Epoch 29 iteration 0041/0187: training loss 0.652; learning rate 0.000017
Epoch 29 iteration 0042/0187: training loss 0.661; learning rate 0.000017
Epoch 29 iteration 0043/0187: training loss 0.662; learning rate 0.000017
Epoch 29 iteration 0044/0187: training loss 0.666; learning rate 0.000017
Epoch 29 iteration 0045/0187: training loss 0.671; learning rate 0.000016
Epoch 29 iteration 0046/0187: training loss 0.670; learning rate 0.000016
Epoch 29 iteration 0047/0187: training loss 0.671; learning rate 0.000016
Epoch 29 iteration 0048/0187: training loss 0.672; learning rate 0.000016
Epoch 29 iteration 0049/0187: training loss 0.674; learning rate 0.000016
Epoch 29 iteration 0050/0187: training loss 0.676; learning rate 0.000016
Epoch 29 iteration 0051/0187: training loss 0.673; learning rate 0.000016
Epoch 29 iteration 0052/0187: training loss 0.676; learning rate 0.000016
Epoch 29 iteration 0053/0187: training loss 0.675; learning rate 0.000015
Epoch 29 iteration 0054/0187: training loss 0.673; learning rate 0.000015
Epoch 29 iteration 0055/0187: training loss 0.676; learning rate 0.000015
Epoch 29 iteration 0056/0187: training loss 0.681; learning rate 0.000015
Epoch 29 iteration 0057/0187: training loss 0.683; learning rate 0.000015
Epoch 29 iteration 0058/0187: training loss 0.686; learning rate 0.000015
Epoch 29 iteration 0059/0187: training loss 0.692; learning rate 0.000015
Epoch 29 iteration 0060/0187: training loss 0.695; learning rate 0.000015
Epoch 29 iteration 0061/0187: training loss 0.699; learning rate 0.000015
Epoch 29 iteration 0062/0187: training loss 0.701; learning rate 0.000014
Epoch 29 iteration 0063/0187: training loss 0.700; learning rate 0.000014
Epoch 29 iteration 0064/0187: training loss 0.700; learning rate 0.000014
Epoch 29 iteration 0065/0187: training loss 0.701; learning rate 0.000014
Epoch 29 iteration 0066/0187: training loss 0.701; learning rate 0.000014
Epoch 29 iteration 0067/0187: training loss 0.702; learning rate 0.000014
Epoch 29 iteration 0068/0187: training loss 0.701; learning rate 0.000014
Epoch 29 iteration 0069/0187: training loss 0.699; learning rate 0.000014
Epoch 29 iteration 0070/0187: training loss 0.703; learning rate 0.000013
Epoch 29 iteration 0071/0187: training loss 0.703; learning rate 0.000013
Epoch 29 iteration 0072/0187: training loss 0.702; learning rate 0.000013
Epoch 29 iteration 0073/0187: training loss 0.706; learning rate 0.000013
Epoch 29 iteration 0074/0187: training loss 0.707; learning rate 0.000013
Epoch 29 iteration 0075/0187: training loss 0.704; learning rate 0.000013
Epoch 29 iteration 0076/0187: training loss 0.703; learning rate 0.000013
Epoch 29 iteration 0077/0187: training loss 0.702; learning rate 0.000013
Epoch 29 iteration 0078/0187: training loss 0.700; learning rate 0.000012
Epoch 29 iteration 0079/0187: training loss 0.700; learning rate 0.000012
Epoch 29 iteration 0080/0187: training loss 0.699; learning rate 0.000012
Epoch 29 iteration 0081/0187: training loss 0.700; learning rate 0.000012
Epoch 29 iteration 0082/0187: training loss 0.700; learning rate 0.000012
Epoch 29 iteration 0083/0187: training loss 0.702; learning rate 0.000012
Epoch 29 iteration 0084/0187: training loss 0.701; learning rate 0.000012
Epoch 29 iteration 0085/0187: training loss 0.701; learning rate 0.000012
Epoch 29 iteration 0086/0187: training loss 0.701; learning rate 0.000012
Epoch 29 iteration 0087/0187: training loss 0.702; learning rate 0.000011
Epoch 29 iteration 0088/0187: training loss 0.704; learning rate 0.000011
Epoch 29 iteration 0089/0187: training loss 0.702; learning rate 0.000011
Epoch 29 iteration 0090/0187: training loss 0.704; learning rate 0.000011
Epoch 29 iteration 0091/0187: training loss 0.706; learning rate 0.000011
Epoch 29 iteration 0092/0187: training loss 0.707; learning rate 0.000011
Epoch 29 iteration 0093/0187: training loss 0.708; learning rate 0.000011
Epoch 29 iteration 0094/0187: training loss 0.709; learning rate 0.000011
Epoch 29 iteration 0095/0187: training loss 0.708; learning rate 0.000010
Epoch 29 iteration 0096/0187: training loss 0.708; learning rate 0.000010
Epoch 29 iteration 0097/0187: training loss 0.711; learning rate 0.000010
Epoch 29 iteration 0098/0187: training loss 0.712; learning rate 0.000010
Epoch 29 iteration 0099/0187: training loss 0.715; learning rate 0.000010
Epoch 29 iteration 0100/0187: training loss 0.717; learning rate 0.000010
Epoch 29 iteration 0101/0187: training loss 0.716; learning rate 0.000010
Epoch 29 iteration 0102/0187: training loss 0.718; learning rate 0.000010
Epoch 29 iteration 0103/0187: training loss 0.719; learning rate 0.000009
Epoch 29 iteration 0104/0187: training loss 0.720; learning rate 0.000009
Epoch 29 iteration 0105/0187: training loss 0.720; learning rate 0.000009
Epoch 29 iteration 0106/0187: training loss 0.718; learning rate 0.000009
Epoch 29 iteration 0107/0187: training loss 0.716; learning rate 0.000009
Epoch 29 iteration 0108/0187: training loss 0.715; learning rate 0.000009
Epoch 29 iteration 0109/0187: training loss 0.716; learning rate 0.000009
Epoch 29 iteration 0110/0187: training loss 0.716; learning rate 0.000009
Epoch 29 iteration 0111/0187: training loss 0.715; learning rate 0.000008
Epoch 29 iteration 0112/0187: training loss 0.715; learning rate 0.000008
Epoch 29 iteration 0113/0187: training loss 0.714; learning rate 0.000008
Epoch 29 iteration 0114/0187: training loss 0.716; learning rate 0.000008
Epoch 29 iteration 0115/0187: training loss 0.716; learning rate 0.000008
Epoch 29 iteration 0116/0187: training loss 0.719; learning rate 0.000008
Epoch 29 iteration 0117/0187: training loss 0.718; learning rate 0.000008
Epoch 29 iteration 0118/0187: training loss 0.717; learning rate 0.000008
Epoch 29 iteration 0119/0187: training loss 0.718; learning rate 0.000007
Epoch 29 iteration 0120/0187: training loss 0.719; learning rate 0.000007
Epoch 29 iteration 0121/0187: training loss 0.719; learning rate 0.000007
Epoch 29 iteration 0122/0187: training loss 0.717; learning rate 0.000007
Epoch 29 iteration 0123/0187: training loss 0.720; learning rate 0.000007
Epoch 29 iteration 0124/0187: training loss 0.719; learning rate 0.000007
Epoch 29 iteration 0125/0187: training loss 0.722; learning rate 0.000007
Epoch 29 iteration 0126/0187: training loss 0.720; learning rate 0.000006
Epoch 29 iteration 0127/0187: training loss 0.721; learning rate 0.000006
Epoch 29 iteration 0128/0187: training loss 0.721; learning rate 0.000006
Epoch 29 iteration 0129/0187: training loss 0.720; learning rate 0.000006
Epoch 29 iteration 0130/0187: training loss 0.720; learning rate 0.000006
Epoch 29 iteration 0131/0187: training loss 0.722; learning rate 0.000006
Epoch 29 iteration 0132/0187: training loss 0.722; learning rate 0.000006
Epoch 29 iteration 0133/0187: training loss 0.720; learning rate 0.000006
Epoch 29 iteration 0134/0187: training loss 0.721; learning rate 0.000005
Epoch 29 iteration 0135/0187: training loss 0.722; learning rate 0.000005
Epoch 29 iteration 0136/0187: training loss 0.722; learning rate 0.000005
Epoch 29 iteration 0137/0187: training loss 0.722; learning rate 0.000005
Epoch 29 iteration 0138/0187: training loss 0.721; learning rate 0.000005
Epoch 29 iteration 0139/0187: training loss 0.722; learning rate 0.000005
Epoch 29 iteration 0140/0187: training loss 0.721; learning rate 0.000005
Epoch 29 iteration 0141/0187: training loss 0.723; learning rate 0.000005
Epoch 29 iteration 0142/0187: training loss 0.723; learning rate 0.000004
Epoch 29 iteration 0143/0187: training loss 0.722; learning rate 0.000004
Epoch 29 iteration 0144/0187: training loss 0.722; learning rate 0.000004
Epoch 29 iteration 0145/0187: training loss 0.722; learning rate 0.000004
Epoch 29 iteration 0146/0187: training loss 0.721; learning rate 0.000004
Epoch 29 iteration 0147/0187: training loss 0.722; learning rate 0.000004
Epoch 29 iteration 0148/0187: training loss 0.721; learning rate 0.000004
Epoch 29 iteration 0149/0187: training loss 0.720; learning rate 0.000003
Epoch 29 iteration 0150/0187: training loss 0.721; learning rate 0.000003
Epoch 29 iteration 0151/0187: training loss 0.722; learning rate 0.000003
Epoch 29 iteration 0152/0187: training loss 0.722; learning rate 0.000003
Epoch 29 iteration 0153/0187: training loss 0.721; learning rate 0.000003
Epoch 29 iteration 0154/0187: training loss 0.722; learning rate 0.000003
Epoch 29 iteration 0155/0187: training loss 0.722; learning rate 0.000003
Epoch 29 iteration 0156/0187: training loss 0.724; learning rate 0.000002
Epoch 29 iteration 0157/0187: training loss 0.723; learning rate 0.000002
Epoch 29 iteration 0158/0187: training loss 0.723; learning rate 0.000002
Epoch 29 iteration 0159/0187: training loss 0.723; learning rate 0.000002
Epoch 29 iteration 0160/0187: training loss 0.724; learning rate 0.000002
Epoch 29 iteration 0161/0187: training loss 0.729; learning rate 0.000002
Epoch 29 iteration 0162/0187: training loss 0.729; learning rate 0.000002
Epoch 29 iteration 0163/0187: training loss 0.729; learning rate 0.000001
Epoch 29 iteration 0164/0187: training loss 0.730; learning rate 0.000001
Epoch 29 iteration 0165/0187: training loss 0.730; learning rate 0.000001
Epoch 29 iteration 0166/0187: training loss 0.730; learning rate 0.000001
Epoch 29 iteration 0167/0187: training loss 0.730; learning rate 0.000001
Epoch 29 iteration 0168/0187: training loss 0.730; learning rate 0.000001
Epoch 29 iteration 0169/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0170/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0171/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0172/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0173/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0174/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0175/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0176/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0177/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0178/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0179/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0180/0187: training loss 0.733; learning rate 0.000000
Epoch 29 iteration 0181/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0182/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0183/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0184/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0185/0187: training loss 0.731; learning rate 0.000000
Epoch 29 iteration 0186/0187: training loss 0.732; learning rate 0.000000
Epoch 29 iteration 0187/0187: training loss 0.731; learning rate 0.000000
Epoch 29 validation pixAcc: 0.335, mIoU: 0.171
